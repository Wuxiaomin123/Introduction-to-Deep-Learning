{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df281d68",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-02T10:21:55.797755Z",
     "iopub.status.busy": "2024-06-02T10:21:55.797365Z",
     "iopub.status.idle": "2024-06-02T10:21:56.502505Z",
     "shell.execute_reply": "2024-06-02T10:21:56.501601Z"
    },
    "papermill": {
     "duration": 0.729697,
     "end_time": "2024-06-02T10:21:56.504650",
     "exception": false,
     "start_time": "2024-06-02T10:21:55.774953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72da502",
   "metadata": {
    "papermill": {
     "duration": 0.020941,
     "end_time": "2024-06-02T10:21:56.546046",
     "exception": false,
     "start_time": "2024-06-02T10:21:56.525105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### In this Week 4's Mini Project, we will analyze data about Twitter texts. Our goal is to classify whether the Twitter texts indicating disasters or not. The data is from: https://www.kaggle.com/competitions/nlp-getting-started. Our mission is to train a Recurrent Neural Network model based on training dataset and classify the twitters in testing dataset. We will use BiLSTM model to do the classification mission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea7835",
   "metadata": {
    "papermill": {
     "duration": 0.02003,
     "end_time": "2024-06-02T10:21:56.586202",
     "exception": false,
     "start_time": "2024-06-02T10:21:56.566172",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f97c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:21:56.627916Z",
     "iopub.status.busy": "2024-06-02T10:21:56.627047Z",
     "iopub.status.idle": "2024-06-02T10:21:56.631305Z",
     "shell.execute_reply": "2024-06-02T10:21:56.630562Z"
    },
    "papermill": {
     "duration": 0.027026,
     "end_time": "2024-06-02T10:21:56.633168",
     "exception": false,
     "start_time": "2024-06-02T10:21:56.606142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"    #use Keras2 version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83489a",
   "metadata": {
    "papermill": {
     "duration": 0.019618,
     "end_time": "2024-06-02T10:21:56.672500",
     "exception": false,
     "start_time": "2024-06-02T10:21:56.652882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c87a093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:21:56.713489Z",
     "iopub.status.busy": "2024-06-02T10:21:56.713235Z",
     "iopub.status.idle": "2024-06-02T10:22:14.188323Z",
     "shell.execute_reply": "2024-06-02T10:22:14.187351Z"
    },
    "papermill": {
     "duration": 17.497896,
     "end_time": "2024-06-02T10:22:14.190613",
     "exception": false,
     "start_time": "2024-06-02T10:21:56.692717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 10:21:59.475013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-02 10:21:59.475102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-02 10:21:59.611329: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f04a5",
   "metadata": {
    "papermill": {
     "duration": 0.019847,
     "end_time": "2024-06-02T10:22:14.231162",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.211315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Set a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab6bf62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.272616Z",
     "iopub.status.busy": "2024-06-02T10:22:14.271989Z",
     "iopub.status.idle": "2024-06-02T10:22:14.277245Z",
     "shell.execute_reply": "2024-06-02T10:22:14.276436Z"
    },
    "papermill": {
     "duration": 0.027964,
     "end_time": "2024-06-02T10:22:14.279051",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.251087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed_value=3407):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f249b",
   "metadata": {
    "papermill": {
     "duration": 0.019577,
     "end_time": "2024-06-02T10:22:14.318569",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.298992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e37a5",
   "metadata": {
    "papermill": {
     "duration": 0.020378,
     "end_time": "2024-06-02T10:22:14.360201",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.339823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### See the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1be062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.401260Z",
     "iopub.status.busy": "2024-06-02T10:22:14.400908Z",
     "iopub.status.idle": "2024-06-02T10:22:14.458257Z",
     "shell.execute_reply": "2024-06-02T10:22:14.457438Z"
    },
    "papermill": {
     "duration": 0.080063,
     "end_time": "2024-06-02T10:22:14.460162",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.380099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv',index_col=None)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c065d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.502166Z",
     "iopub.status.busy": "2024-06-02T10:22:14.501904Z",
     "iopub.status.idle": "2024-06-02T10:22:14.511217Z",
     "shell.execute_reply": "2024-06-02T10:22:14.510286Z"
    },
    "papermill": {
     "duration": 0.032011,
     "end_time": "2024-06-02T10:22:14.512992",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.480981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71697fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.555087Z",
     "iopub.status.busy": "2024-06-02T10:22:14.554818Z",
     "iopub.status.idle": "2024-06-02T10:22:14.577034Z",
     "shell.execute_reply": "2024-06-02T10:22:14.575999Z"
    },
    "papermill": {
     "duration": 0.045526,
     "end_time": "2024-06-02T10:22:14.578919",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.533393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8aa1573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.622430Z",
     "iopub.status.busy": "2024-06-02T10:22:14.622164Z",
     "iopub.status.idle": "2024-06-02T10:22:14.639149Z",
     "shell.execute_reply": "2024-06-02T10:22:14.638236Z"
    },
    "papermill": {
     "duration": 0.041217,
     "end_time": "2024-06-02T10:22:14.641174",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.599957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd5d1f",
   "metadata": {
    "papermill": {
     "duration": 0.020742,
     "end_time": "2024-06-02T10:22:14.682730",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.661988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### See if there are duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7479c94e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.725722Z",
     "iopub.status.busy": "2024-06-02T10:22:14.725070Z",
     "iopub.status.idle": "2024-06-02T10:22:14.738784Z",
     "shell.execute_reply": "2024-06-02T10:22:14.737935Z"
    },
    "papermill": {
     "duration": 0.037272,
     "end_time": "2024-06-02T10:22:14.740798",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.703526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates0 = df_train.duplicated()\n",
    "df_train[duplicates0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0240c0",
   "metadata": {
    "papermill": {
     "duration": 0.060639,
     "end_time": "2024-06-02T10:22:14.823044",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.762405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see that there are no duplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b6aa9",
   "metadata": {
    "papermill": {
     "duration": 0.020833,
     "end_time": "2024-06-02T10:22:14.865044",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.844211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### From the above 5 cells, we can see that the training dataset contains 7613 texts. The target is 1 or 0 that stands for disaster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c752c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:14.908313Z",
     "iopub.status.busy": "2024-06-02T10:22:14.907996Z",
     "iopub.status.idle": "2024-06-02T10:22:15.152821Z",
     "shell.execute_reply": "2024-06-02T10:22:15.151901Z"
    },
    "papermill": {
     "duration": 0.268658,
     "end_time": "2024-06-02T10:22:15.154663",
     "exception": false,
     "start_time": "2024-06-02T10:22:14.886005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = [len(x0) for x0 in list(df_train['text'].str.split())]\n",
    "plt.hist(word_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf74819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.200037Z",
     "iopub.status.busy": "2024-06-02T10:22:15.199646Z",
     "iopub.status.idle": "2024-06-02T10:22:15.230318Z",
     "shell.execute_reply": "2024-06-02T10:22:15.228950Z"
    },
    "papermill": {
     "duration": 0.057049,
     "end_time": "2024-06-02T10:22:15.233538",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.176489",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 7,\n",
       " 22,\n",
       " 8,\n",
       " 16,\n",
       " 18,\n",
       " 14,\n",
       " 15,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 27,\n",
       " 12,\n",
       " 7,\n",
       " 11,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 13,\n",
       " 21,\n",
       " 8,\n",
       " 19,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 24,\n",
       " 6,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 27,\n",
       " 9,\n",
       " 11,\n",
       " 18,\n",
       " 10,\n",
       " 15,\n",
       " 19,\n",
       " 13,\n",
       " 22,\n",
       " 24,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 15,\n",
       " 23,\n",
       " 10,\n",
       " 8,\n",
       " 19,\n",
       " 27,\n",
       " 12,\n",
       " 16,\n",
       " 18,\n",
       " 14,\n",
       " 18,\n",
       " 4,\n",
       " 15,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 16,\n",
       " 18,\n",
       " 27,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 8,\n",
       " 24,\n",
       " 25,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 4,\n",
       " 26,\n",
       " 22,\n",
       " 21,\n",
       " 10,\n",
       " 22,\n",
       " 17,\n",
       " 20,\n",
       " 8,\n",
       " 20,\n",
       " 16,\n",
       " 11,\n",
       " 16,\n",
       " 8,\n",
       " 24,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 2,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 16,\n",
       " 21,\n",
       " 16,\n",
       " 23,\n",
       " 8,\n",
       " 16,\n",
       " 19,\n",
       " 26,\n",
       " 16,\n",
       " 27,\n",
       " 11,\n",
       " 5,\n",
       " 2,\n",
       " 21,\n",
       " 19,\n",
       " 14,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 20,\n",
       " 9,\n",
       " 10,\n",
       " 22,\n",
       " 10,\n",
       " 18,\n",
       " 20,\n",
       " 13,\n",
       " 13,\n",
       " 22,\n",
       " 18,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 18,\n",
       " 11,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 25,\n",
       " 10,\n",
       " 14,\n",
       " 20,\n",
       " 5,\n",
       " 13,\n",
       " 21,\n",
       " 12,\n",
       " 7,\n",
       " 21,\n",
       " 22,\n",
       " 11,\n",
       " 20,\n",
       " 13,\n",
       " 9,\n",
       " 19,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 21,\n",
       " 14,\n",
       " 11,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 22,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 11,\n",
       " 13,\n",
       " 17,\n",
       " 19,\n",
       " 6,\n",
       " 13,\n",
       " 19,\n",
       " 5,\n",
       " 16,\n",
       " 14,\n",
       " 17,\n",
       " 18,\n",
       " 4,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 17,\n",
       " 16,\n",
       " 22,\n",
       " 24,\n",
       " 17,\n",
       " 15,\n",
       " 8,\n",
       " 16,\n",
       " 9,\n",
       " 21,\n",
       " 14,\n",
       " 23,\n",
       " 13,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 15,\n",
       " 23,\n",
       " 22,\n",
       " 7,\n",
       " 17,\n",
       " 21,\n",
       " 6,\n",
       " 19,\n",
       " 4,\n",
       " 12,\n",
       " 8,\n",
       " 19,\n",
       " 19,\n",
       " 17,\n",
       " 19,\n",
       " 20,\n",
       " 6,\n",
       " 13,\n",
       " 21,\n",
       " 14,\n",
       " 19,\n",
       " 23,\n",
       " 22,\n",
       " 21,\n",
       " 18,\n",
       " 12,\n",
       " 23,\n",
       " 19,\n",
       " 9,\n",
       " 13,\n",
       " 12,\n",
       " 19,\n",
       " 19,\n",
       " 10,\n",
       " 16,\n",
       " 7,\n",
       " 16,\n",
       " 2,\n",
       " 17,\n",
       " 17,\n",
       " 20,\n",
       " 8,\n",
       " 19,\n",
       " 12,\n",
       " 5,\n",
       " 18,\n",
       " 10,\n",
       " 23,\n",
       " 18,\n",
       " 20,\n",
       " 13,\n",
       " 26,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 17,\n",
       " 21,\n",
       " 14,\n",
       " 11,\n",
       " 14,\n",
       " 4,\n",
       " 21,\n",
       " 23,\n",
       " 19,\n",
       " 16,\n",
       " 16,\n",
       " 11,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 15,\n",
       " 16,\n",
       " 21,\n",
       " 22,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 23,\n",
       " 14,\n",
       " 16,\n",
       " 20,\n",
       " 16,\n",
       " 16,\n",
       " 18,\n",
       " 15,\n",
       " 24,\n",
       " 9,\n",
       " 6,\n",
       " 20,\n",
       " 5,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 12,\n",
       " 18,\n",
       " 21,\n",
       " 24,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 19,\n",
       " 18,\n",
       " 20,\n",
       " 14,\n",
       " 22,\n",
       " 8,\n",
       " 12,\n",
       " 18,\n",
       " 12,\n",
       " 11,\n",
       " 28,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 24,\n",
       " 20,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 20,\n",
       " 22,\n",
       " 20,\n",
       " 20,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 19,\n",
       " 6,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 13,\n",
       " 4,\n",
       " 13,\n",
       " 4,\n",
       " 11,\n",
       " 10,\n",
       " 13,\n",
       " 9,\n",
       " 19,\n",
       " 8,\n",
       " 19,\n",
       " 15,\n",
       " 18,\n",
       " 13,\n",
       " 4,\n",
       " 19,\n",
       " 13,\n",
       " 15,\n",
       " 13,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 11,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 11,\n",
       " 7,\n",
       " 4,\n",
       " 11,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 17,\n",
       " 9,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 18,\n",
       " 6,\n",
       " 14,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 11,\n",
       " 18,\n",
       " 5,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 20,\n",
       " 18,\n",
       " 16,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 11,\n",
       " 20,\n",
       " 14,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 15,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 19,\n",
       " 13,\n",
       " 27,\n",
       " 12,\n",
       " 18,\n",
       " 14,\n",
       " 7,\n",
       " 11,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 11,\n",
       " 24,\n",
       " 24,\n",
       " 8,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 3,\n",
       " 8,\n",
       " 18,\n",
       " 22,\n",
       " 4,\n",
       " 5,\n",
       " 20,\n",
       " 24,\n",
       " 12,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 6,\n",
       " 18,\n",
       " 12,\n",
       " 4,\n",
       " 22,\n",
       " 18,\n",
       " 18,\n",
       " 21,\n",
       " 14,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 12,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 25,\n",
       " 18,\n",
       " 9,\n",
       " 18,\n",
       " 13,\n",
       " 12,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 21,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 19,\n",
       " 18,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 11,\n",
       " 6,\n",
       " 14,\n",
       " 11,\n",
       " 15,\n",
       " 8,\n",
       " 22,\n",
       " 8,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 20,\n",
       " 14,\n",
       " 15,\n",
       " 21,\n",
       " 14,\n",
       " 21,\n",
       " 19,\n",
       " 21,\n",
       " 8,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 4,\n",
       " 22,\n",
       " 9,\n",
       " 5,\n",
       " 18,\n",
       " 19,\n",
       " 13,\n",
       " 20,\n",
       " 15,\n",
       " 10,\n",
       " 16,\n",
       " 16,\n",
       " 9,\n",
       " 14,\n",
       " 16,\n",
       " 20,\n",
       " 17,\n",
       " 11,\n",
       " 16,\n",
       " 16,\n",
       " 19,\n",
       " 18,\n",
       " 15,\n",
       " 18,\n",
       " 20,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 20,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 10,\n",
       " 17,\n",
       " 14,\n",
       " 16,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 14,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 4,\n",
       " 20,\n",
       " 16,\n",
       " 18,\n",
       " 22,\n",
       " 20,\n",
       " 12,\n",
       " 14,\n",
       " 21,\n",
       " 15,\n",
       " 22,\n",
       " 21,\n",
       " 14,\n",
       " 20,\n",
       " 4,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 19,\n",
       " 20,\n",
       " 4,\n",
       " 17,\n",
       " 18,\n",
       " 22,\n",
       " 4,\n",
       " 22,\n",
       " 15,\n",
       " 20,\n",
       " 9,\n",
       " 15,\n",
       " 3,\n",
       " 8,\n",
       " 14,\n",
       " 26,\n",
       " 5,\n",
       " 5,\n",
       " 21,\n",
       " 14,\n",
       " 11,\n",
       " 15,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 12,\n",
       " 10,\n",
       " 3,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 10,\n",
       " 13,\n",
       " 24,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 20,\n",
       " 15,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 21,\n",
       " 14,\n",
       " 9,\n",
       " 7,\n",
       " 14,\n",
       " 8,\n",
       " 11,\n",
       " 15,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 13,\n",
       " 18,\n",
       " 11,\n",
       " 20,\n",
       " 7,\n",
       " 12,\n",
       " 15,\n",
       " 14,\n",
       " 19,\n",
       " 15,\n",
       " 5,\n",
       " 19,\n",
       " 21,\n",
       " 21,\n",
       " 12,\n",
       " 3,\n",
       " 18,\n",
       " 6,\n",
       " 24,\n",
       " 17,\n",
       " 17,\n",
       " 5,\n",
       " 12,\n",
       " 20,\n",
       " 22,\n",
       " 23,\n",
       " 12,\n",
       " 17,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 25,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 7,\n",
       " 10,\n",
       " 11,\n",
       " 24,\n",
       " 5,\n",
       " 12,\n",
       " 11,\n",
       " 7,\n",
       " 13,\n",
       " 18,\n",
       " 7,\n",
       " 28,\n",
       " 8,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 17,\n",
       " 18,\n",
       " 7,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 8,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 20,\n",
       " 18,\n",
       " 27,\n",
       " 25,\n",
       " 21,\n",
       " 26,\n",
       " 17,\n",
       " 10,\n",
       " 9,\n",
       " 15,\n",
       " 23,\n",
       " 6,\n",
       " 8,\n",
       " 21,\n",
       " 24,\n",
       " 9,\n",
       " 26,\n",
       " 4,\n",
       " 17,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 19,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 5,\n",
       " 6,\n",
       " 23,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 2,\n",
       " 3,\n",
       " 26,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 22,\n",
       " 10,\n",
       " 15,\n",
       " 10,\n",
       " 20,\n",
       " 13,\n",
       " 21,\n",
       " 11,\n",
       " 7,\n",
       " 5,\n",
       " 14,\n",
       " 12,\n",
       " 21,\n",
       " 9,\n",
       " 26,\n",
       " 19,\n",
       " 26,\n",
       " 24,\n",
       " 19,\n",
       " 20,\n",
       " 7,\n",
       " 4,\n",
       " 22,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 23,\n",
       " 21,\n",
       " 16,\n",
       " 24,\n",
       " 2,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 11,\n",
       " 22,\n",
       " 11,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 12,\n",
       " 14,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 28,\n",
       " 3,\n",
       " 4,\n",
       " 12,\n",
       " 13,\n",
       " 25,\n",
       " 18,\n",
       " 17,\n",
       " 19,\n",
       " 8,\n",
       " 19,\n",
       " 14,\n",
       " 25,\n",
       " 13,\n",
       " 14,\n",
       " 5,\n",
       " 17,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 18,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 7,\n",
       " 13,\n",
       " 19,\n",
       " 5,\n",
       " 13,\n",
       " 21,\n",
       " 6,\n",
       " 17,\n",
       " 20,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 18,\n",
       " 14,\n",
       " 9,\n",
       " 15,\n",
       " 11,\n",
       " 18,\n",
       " 11,\n",
       " 23,\n",
       " 5,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 26,\n",
       " 3,\n",
       " 5,\n",
       " 28,\n",
       " 18,\n",
       " 20,\n",
       " 26,\n",
       " 10,\n",
       " 23,\n",
       " 5,\n",
       " 8,\n",
       " 21,\n",
       " 5,\n",
       " 14,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 12,\n",
       " 17,\n",
       " 10,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 22,\n",
       " 18,\n",
       " 9,\n",
       " 10,\n",
       " 15,\n",
       " 26,\n",
       " 18,\n",
       " 22,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 22,\n",
       " 25,\n",
       " 16,\n",
       " 14,\n",
       " 19,\n",
       " 17,\n",
       " 22,\n",
       " 28,\n",
       " 6,\n",
       " 11,\n",
       " 7,\n",
       " 28,\n",
       " 11,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 31,\n",
       " 25,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 11,\n",
       " 16,\n",
       " 20,\n",
       " 18,\n",
       " 13,\n",
       " 18,\n",
       " 28,\n",
       " 18,\n",
       " 18,\n",
       " 15,\n",
       " 16,\n",
       " 13,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 18,\n",
       " 23,\n",
       " 20,\n",
       " 12,\n",
       " 6,\n",
       " 15,\n",
       " 18,\n",
       " 27,\n",
       " 4,\n",
       " 11,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 23,\n",
       " 14,\n",
       " 12,\n",
       " 19,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e3204a",
   "metadata": {
    "papermill": {
     "duration": 0.022084,
     "end_time": "2024-06-02T10:22:15.294430",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.272346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see that the range of number of words is from 1 to 31, most texts' number of words is from 5 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce6c50c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.341386Z",
     "iopub.status.busy": "2024-06-02T10:22:15.340767Z",
     "iopub.status.idle": "2024-06-02T10:22:15.353297Z",
     "shell.execute_reply": "2024-06-02T10:22:15.352343Z"
    },
    "papermill": {
     "duration": 0.038758,
     "end_time": "2024-06-02T10:22:15.355204",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.316446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword0 = [x0 for x0 in df_train['keyword'].tolist() if pd.isna(x0) == False]\n",
    "keyword0 = list(set(keyword0))\n",
    "len(keyword0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bdd76c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.400835Z",
     "iopub.status.busy": "2024-06-02T10:22:15.400528Z",
     "iopub.status.idle": "2024-06-02T10:22:15.412791Z",
     "shell.execute_reply": "2024-06-02T10:22:15.411970Z"
    },
    "papermill": {
     "duration": 0.037079,
     "end_time": "2024-06-02T10:22:15.414574",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.377495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location0 = [x0 for x0 in df_train['location'].tolist() if pd.isna(x0) == False]\n",
    "location0 = list(set(location0))\n",
    "len(location0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd71519",
   "metadata": {
    "papermill": {
     "duration": 0.02652,
     "end_time": "2024-06-02T10:22:15.463496",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.436976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Only 221 twitters have keywords and only 3341 twitters have locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdbbe736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.524060Z",
     "iopub.status.busy": "2024-06-02T10:22:15.523119Z",
     "iopub.status.idle": "2024-06-02T10:22:15.532831Z",
     "shell.execute_reply": "2024-06-02T10:22:15.531955Z"
    },
    "papermill": {
     "duration": 0.042394,
     "end_time": "2024-06-02T10:22:15.535418",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.493024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4342, 1: 3271}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict = {}\n",
    "for x0 in list(set(df_train['target'].tolist())):\n",
    "    category_dict[x0] = len(df_train[df_train['target']==x0])\n",
    "category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9cbaea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.586949Z",
     "iopub.status.busy": "2024-06-02T10:22:15.586173Z",
     "iopub.status.idle": "2024-06-02T10:22:15.726629Z",
     "shell.execute_reply": "2024-06-02T10:22:15.725377Z"
    },
    "papermill": {
     "duration": 0.168628,
     "end_time": "2024-06-02T10:22:15.730342",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.561714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('category dict proportion')\n",
    "plt.pie(category_dict.values(),labels=category_dict.keys(),autopct='%.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b027f0",
   "metadata": {
    "papermill": {
     "duration": 0.037476,
     "end_time": "2024-06-02T10:22:15.821138",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.783662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### From the pie chart above, we can see that roughly 57% of training samples, 4342, are not disaster twitters and roughly 43%, 3271, are disaster twitters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de5bbe",
   "metadata": {
    "papermill": {
     "duration": 0.022523,
     "end_time": "2024-06-02T10:22:15.872513",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.849990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173cfdd",
   "metadata": {
    "papermill": {
     "duration": 0.022535,
     "end_time": "2024-06-02T10:22:15.918545",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.896010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### In the data cleaning step, we will remove punctuation marks, numbers, stop words, URL, non-English characters, extra spaces and extremely short sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790b6bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:15.970956Z",
     "iopub.status.busy": "2024-06-02T10:22:15.970123Z",
     "iopub.status.idle": "2024-06-02T10:22:15.978895Z",
     "shell.execute_reply": "2024-06-02T10:22:15.977884Z"
    },
    "papermill": {
     "duration": 0.037778,
     "end_time": "2024-06-02T10:22:15.980789",
     "exception": false,
     "start_time": "2024-06-02T10:22:15.943011",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " ' -',\n",
       " '- ',\n",
       " \" '\",\n",
       " \"' \"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show punctuation marks\n",
    "punctuation_marks = [x0 for x0 in string.punctuation if x0 not in [\"'\",'-']]\n",
    "punctuation_marks = punctuation_marks + [' -','- ',\" '\",\"' \"]\n",
    "\n",
    "punctuation_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3591cc04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:16.028604Z",
     "iopub.status.busy": "2024-06-02T10:22:16.028020Z",
     "iopub.status.idle": "2024-06-02T10:22:16.041114Z",
     "shell.execute_reply": "2024-06-02T10:22:16.040111Z"
    },
    "papermill": {
     "duration": 0.03906,
     "end_time": "2024-06-02T10:22:16.043040",
     "exception": false,
     "start_time": "2024-06-02T10:22:16.003980",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beyond',\n",
       " 'also',\n",
       " 'always',\n",
       " 'ours',\n",
       " 'nobody',\n",
       " 'himself',\n",
       " 'about',\n",
       " 'mine',\n",
       " 'these',\n",
       " 'ltd',\n",
       " 'has',\n",
       " 'could',\n",
       " 'it',\n",
       " 'co',\n",
       " 'alone',\n",
       " 'so',\n",
       " 'everyone',\n",
       " 'thus',\n",
       " 'take',\n",
       " 'each',\n",
       " 'might',\n",
       " 'an',\n",
       " 'much',\n",
       " 'along',\n",
       " 'thence',\n",
       " 'go',\n",
       " 'only',\n",
       " 'myself',\n",
       " 'perhaps',\n",
       " 'per',\n",
       " 'are',\n",
       " 'i',\n",
       " 'found',\n",
       " 'noone',\n",
       " 'several',\n",
       " 'sincere',\n",
       " 'else',\n",
       " 'every',\n",
       " 'serious',\n",
       " 'ever',\n",
       " 'former',\n",
       " 'both',\n",
       " 'thereafter',\n",
       " 'nothing',\n",
       " 'hereupon',\n",
       " 'see',\n",
       " 'whoever',\n",
       " 'because',\n",
       " 'into',\n",
       " 'than',\n",
       " 'everywhere',\n",
       " 'became',\n",
       " 'very',\n",
       " 'cant',\n",
       " 'inc',\n",
       " 'ie',\n",
       " 'against',\n",
       " 'whose',\n",
       " 'hundred',\n",
       " 'fifty',\n",
       " 'was',\n",
       " 'us',\n",
       " 'themselves',\n",
       " 'throughout',\n",
       " 'get',\n",
       " 'some',\n",
       " 'all',\n",
       " 'third',\n",
       " 'amount',\n",
       " 'beforehand',\n",
       " 'con',\n",
       " 'fifteen',\n",
       " 'find',\n",
       " 'we',\n",
       " 'still',\n",
       " 'before',\n",
       " 'while',\n",
       " 'fill',\n",
       " 'its',\n",
       " 'such',\n",
       " 'below',\n",
       " 'wherever',\n",
       " 'thin',\n",
       " 'top',\n",
       " 'yourselves',\n",
       " 'down',\n",
       " 'again',\n",
       " 'here',\n",
       " 'nor',\n",
       " 'neither',\n",
       " 'whom',\n",
       " 'upon',\n",
       " 'least',\n",
       " 'first',\n",
       " 'moreover',\n",
       " 'within',\n",
       " 'seem',\n",
       " 'their',\n",
       " 'own',\n",
       " 'whenever',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'yours',\n",
       " 'call',\n",
       " 'indeed',\n",
       " 'whereupon',\n",
       " 'made',\n",
       " 'if',\n",
       " 'hereafter',\n",
       " 'were',\n",
       " 'and',\n",
       " 'once',\n",
       " 'often',\n",
       " 'eight',\n",
       " 'besides',\n",
       " 'as',\n",
       " 'although',\n",
       " 'whereby',\n",
       " 'become',\n",
       " 'ten',\n",
       " 'twelve',\n",
       " 'formerly',\n",
       " 'everything',\n",
       " 'hasnt',\n",
       " 'a',\n",
       " 'please',\n",
       " 'can',\n",
       " 'next',\n",
       " 'whereafter',\n",
       " 'give',\n",
       " 'anyhow',\n",
       " 'amoungst',\n",
       " 'show',\n",
       " 'full',\n",
       " 'same',\n",
       " 're',\n",
       " 'beside',\n",
       " 'herein',\n",
       " 'by',\n",
       " 'more',\n",
       " 'without',\n",
       " 'then',\n",
       " 'becoming',\n",
       " 'herself',\n",
       " 'part',\n",
       " 'back',\n",
       " 'thereupon',\n",
       " 'mostly',\n",
       " 'someone',\n",
       " 'since',\n",
       " 'whence',\n",
       " 'done',\n",
       " 'one',\n",
       " 'those',\n",
       " 'anyway',\n",
       " 'keep',\n",
       " 'eg',\n",
       " 'across',\n",
       " 'hers',\n",
       " 'almost',\n",
       " 'cannot',\n",
       " 'between',\n",
       " 'her',\n",
       " 'cry',\n",
       " 'put',\n",
       " 'would',\n",
       " 'four',\n",
       " 'me',\n",
       " 'he',\n",
       " 'hereby',\n",
       " 'above',\n",
       " 'seems',\n",
       " 'must',\n",
       " 'thereby',\n",
       " 'them',\n",
       " 'enough',\n",
       " 'should',\n",
       " 'latterly',\n",
       " 'already',\n",
       " 'under',\n",
       " 'now',\n",
       " 'side',\n",
       " 'twenty',\n",
       " 'when',\n",
       " 'or',\n",
       " 'sixty',\n",
       " 'amongst',\n",
       " 'his',\n",
       " 'at',\n",
       " 'less',\n",
       " 'whither',\n",
       " 'either',\n",
       " 'except',\n",
       " 'my',\n",
       " 'bottom',\n",
       " 'move',\n",
       " 'where',\n",
       " 'been',\n",
       " 'last',\n",
       " 'rather',\n",
       " 'another',\n",
       " 'who',\n",
       " 'two',\n",
       " 'nowhere',\n",
       " 'nevertheless',\n",
       " 'after',\n",
       " 'any',\n",
       " 'have',\n",
       " 'sometimes',\n",
       " 'whereas',\n",
       " 'off',\n",
       " 'with',\n",
       " 'eleven',\n",
       " 'until',\n",
       " 'from',\n",
       " 'being',\n",
       " 'hence',\n",
       " 'none',\n",
       " 'meanwhile',\n",
       " 'too',\n",
       " 'this',\n",
       " 'around',\n",
       " 'most',\n",
       " 'afterwards',\n",
       " 'be',\n",
       " 'how',\n",
       " 'interest',\n",
       " 'well',\n",
       " 'wherein',\n",
       " 'together',\n",
       " 'anyone',\n",
       " 'whether',\n",
       " 'thru',\n",
       " 'that',\n",
       " 'no',\n",
       " 'through',\n",
       " 'never',\n",
       " 'detail',\n",
       " 'among',\n",
       " 'system',\n",
       " 'thick',\n",
       " 'front',\n",
       " 'towards',\n",
       " 'do',\n",
       " 'seemed',\n",
       " 'but',\n",
       " 'others',\n",
       " 'even',\n",
       " 'few',\n",
       " 'somehow',\n",
       " 'five',\n",
       " 'six',\n",
       " 'in',\n",
       " 'something',\n",
       " 'other',\n",
       " 'de',\n",
       " 'will',\n",
       " 'couldnt',\n",
       " 'due',\n",
       " 'toward',\n",
       " 'what',\n",
       " 'to',\n",
       " 'you',\n",
       " 'sometime',\n",
       " 'un',\n",
       " 'onto',\n",
       " 'there',\n",
       " 'anything',\n",
       " 'for',\n",
       " 'namely',\n",
       " 'yourself',\n",
       " 'fire',\n",
       " 'further',\n",
       " 'which',\n",
       " 'therein',\n",
       " 'otherwise',\n",
       " 'of',\n",
       " 'three',\n",
       " 'though',\n",
       " 'out',\n",
       " 'she',\n",
       " 'becomes',\n",
       " 'empty',\n",
       " 'describe',\n",
       " 'our',\n",
       " 'whatever',\n",
       " 'etc',\n",
       " 'during',\n",
       " 'am',\n",
       " 'elsewhere',\n",
       " 'forty',\n",
       " 'up',\n",
       " 'they',\n",
       " 'itself',\n",
       " 'anywhere',\n",
       " 'whole',\n",
       " 'may',\n",
       " 'ourselves',\n",
       " 'bill',\n",
       " 'why',\n",
       " 'him',\n",
       " 'via',\n",
       " 'behind',\n",
       " 'therefore',\n",
       " 'latter',\n",
       " 'is',\n",
       " 'mill',\n",
       " 'seeming',\n",
       " 'however',\n",
       " 'had',\n",
       " 'your',\n",
       " 'nine',\n",
       " 'on',\n",
       " 'name',\n",
       " 'the',\n",
       " 'over',\n",
       " 'many',\n",
       " 'somewhere']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show stop words\n",
    "list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23c153e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:16.091065Z",
     "iopub.status.busy": "2024-06-02T10:22:16.090442Z",
     "iopub.status.idle": "2024-06-02T10:22:17.598656Z",
     "shell.execute_reply": "2024-06-02T10:22:17.597750Z"
    },
    "papermill": {
     "duration": 1.534716,
     "end_time": "2024-06-02T10:22:17.600969",
     "exception": false,
     "start_time": "2024-06-02T10:22:16.066253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610         M1.94 [01:04 UTC]?5km S of Volcano Hawaii.       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove URL\n",
    "for i in df_train.index:\n",
    "    temp0 = [elt0 for elt0 in df_train.loc[i,'text'].split() if 'http' not in elt0]\n",
    "    temp1 = ' '.join(temp0)\n",
    "    df_train.loc[i,'text'] = temp1\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770f0814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:17.651138Z",
     "iopub.status.busy": "2024-06-02T10:22:17.650810Z",
     "iopub.status.idle": "2024-06-02T10:22:19.789313Z",
     "shell.execute_reply": "2024-06-02T10:22:19.788421Z"
    },
    "papermill": {
     "duration": 2.165478,
     "end_time": "2024-06-02T10:22:19.791259",
     "exception": false,
     "start_time": "2024-06-02T10:22:17.625781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria ahrary TheTawniest control wild fires Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M UTC km S Volcano Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating e-bike collided car Littl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Latest Homes Razed Northern California Wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                 Deeds Reason earthquake ALLAH Forgive       1  \n",
       "1                      Forest near La Ronge Sask Canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     people receive wildfires evacuation orders Cal...       1  \n",
       "4     Just got sent photo Ruby Alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  giant cranes holding bridge collapse nearby homes       1  \n",
       "7609  aria ahrary TheTawniest control wild fires Cal...       1  \n",
       "7610                          M UTC km S Volcano Hawaii       1  \n",
       "7611  Police investigating e-bike collided car Littl...       1  \n",
       "7612  Latest Homes Razed Northern California Wildfir...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punctuation marks\n",
    "for x0 in punctuation_marks:\n",
    "    df_train['text'] = df_train['text'].str.replace(x0,' ')\n",
    "#remove numbers\n",
    "for x0 in string.digits:\n",
    "    df_train['text'] = df_train['text'].str.replace(x0,' ')\n",
    "#remove stop words\n",
    "for i in df_train.index:\n",
    "    temp0 = [elt0 for elt0 in df_train.loc[i,'text'].split() if elt0.lower() not in list(ENGLISH_STOP_WORDS)]\n",
    "    temp1 = ' '.join(temp0)\n",
    "    df_train.loc[i,'text'] = temp1\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fe347ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:19.841279Z",
     "iopub.status.busy": "2024-06-02T10:22:19.840987Z",
     "iopub.status.idle": "2024-06-02T10:22:20.298501Z",
     "shell.execute_reply": "2024-06-02T10:22:20.297614Z"
    },
    "papermill": {
     "duration": 0.484629,
     "end_time": "2024-06-02T10:22:20.300581",
     "exception": false,
     "start_time": "2024-06-02T10:22:19.815952",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ã¡',\n",
       " '\\x9d',\n",
       " 'Â©',\n",
       " 'Ã‚',\n",
       " 'ÃŠ',\n",
       " 'Â¼',\n",
       " 'Âª',\n",
       " 'Ã‡',\n",
       " 'Ã€',\n",
       " 'Ãˆ',\n",
       " 'Â¡',\n",
       " '\\n',\n",
       " 'Ã›',\n",
       " 'Â«',\n",
       " 'Ã’',\n",
       " '\\x90',\n",
       " 'Â¤',\n",
       " 'Ã‘',\n",
       " 'Ã¸',\n",
       " '\\x89',\n",
       " 'Â£',\n",
       " 'Ã',\n",
       " 'Ã£',\n",
       " 'Ã¥',\n",
       " 'Ã¢',\n",
       " '\\x81',\n",
       " 'Ã¼',\n",
       " 'Ã¤',\n",
       " 'Â¢',\n",
       " 'Â¨',\n",
       " 'Ã·',\n",
       " 'Â¬',\n",
       " 'Ã',\n",
       " 'ÃŒ',\n",
       " 'Â´',\n",
       " 'Â±',\n",
       " 'Ã“',\n",
       " 'Â¦']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect non-English characters\n",
    "char0 = []\n",
    "for i in df_train.index:\n",
    "    #\n",
    "    x0 = df_train.loc[i,'text']\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    x0 = df_train.loc[i,'keyword']\n",
    "    x0 = str(x0)\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    x0 = df_train.loc[i,'location']\n",
    "    x0 = str(x0)\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    char0 = list(set(char0))\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.ascii_letters]\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.punctuation]\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.digits]\n",
    "\n",
    "char0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e508825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:20.351833Z",
     "iopub.status.busy": "2024-06-02T10:22:20.351540Z",
     "iopub.status.idle": "2024-06-02T10:22:20.487677Z",
     "shell.execute_reply": "2024-06-02T10:22:20.486844Z"
    },
    "papermill": {
     "duration": 0.164483,
     "end_time": "2024-06-02T10:22:20.489591",
     "exception": false,
     "start_time": "2024-06-02T10:22:20.325108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria ahrary TheTawniest control wild fires Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M UTC km S Volcano Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating e-bike collided car Littl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Latest Homes Razed Northern California Wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                 Deeds Reason earthquake ALLAH Forgive       1  \n",
       "1                      Forest near La Ronge Sask Canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     people receive wildfires evacuation orders Cal...       1  \n",
       "4     Just got sent photo Ruby Alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  giant cranes holding bridge collapse nearby homes       1  \n",
       "7609  aria ahrary TheTawniest control wild fires Cal...       1  \n",
       "7610                          M UTC km S Volcano Hawaii       1  \n",
       "7611  Police investigating e-bike collided car Littl...       1  \n",
       "7612  Latest Homes Razed Northern California Wildfir...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove non-English characters\n",
    "for x0 in char0:\n",
    "    df_train['text'] = df_train['text'].str.replace(x0,' ')\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b7bfd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:20.540065Z",
     "iopub.status.busy": "2024-06-02T10:22:20.539410Z",
     "iopub.status.idle": "2024-06-02T10:22:21.457610Z",
     "shell.execute_reply": "2024-06-02T10:22:21.456652Z"
    },
    "papermill": {
     "duration": 0.945511,
     "end_time": "2024-06-02T10:22:21.459727",
     "exception": false,
     "start_time": "2024-06-02T10:22:20.514216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria ahrary TheTawniest control wild fires Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M UTC km S Volcano Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating e-bike collided car Littl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Latest Homes Razed Northern California Wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                 Deeds Reason earthquake ALLAH Forgive       1  \n",
       "1                      Forest near La Ronge Sask Canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     people receive wildfires evacuation orders Cal...       1  \n",
       "4     Just got sent photo Ruby Alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  giant cranes holding bridge collapse nearby homes       1  \n",
       "7609  aria ahrary TheTawniest control wild fires Cal...       1  \n",
       "7610                          M UTC km S Volcano Hawaii       1  \n",
       "7611  Police investigating e-bike collided car Littl...       1  \n",
       "7612  Latest Homes Razed Northern California Wildfir...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove extra spaces\n",
    "for i in df_train.index:\n",
    "    df_train.loc[i,'text'] = ' '.join(df_train.loc[i,'text'].split())\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "649705eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:21.511501Z",
     "iopub.status.busy": "2024-06-02T10:22:21.511195Z",
     "iopub.status.idle": "2024-06-02T10:22:21.530459Z",
     "shell.execute_reply": "2024-06-02T10:22:21.529552Z"
    },
    "papermill": {
     "duration": 0.046732,
     "end_time": "2024-06-02T10:22:21.532310",
     "exception": false,
     "start_time": "2024-06-02T10:22:21.485578",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria ahrary TheTawniest control wild fires Cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M UTC km S Volcano Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating e-bike collided car Littl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Latest Homes Razed Northern California Wildfir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7612 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0                 Deeds Reason earthquake ALLAH Forgive       1  \n",
       "1                      Forest near La Ronge Sask Canada       1  \n",
       "2     residents asked shelter place notified officer...       1  \n",
       "3     people receive wildfires evacuation orders Cal...       1  \n",
       "4     Just got sent photo Ruby Alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  giant cranes holding bridge collapse nearby homes       1  \n",
       "7609  aria ahrary TheTawniest control wild fires Cal...       1  \n",
       "7610                          M UTC km S Volcano Hawaii       1  \n",
       "7611  Police investigating e-bike collided car Littl...       1  \n",
       "7612  Latest Homes Razed Northern California Wildfir...       1  \n",
       "\n",
       "[7612 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete extremely short sentences\n",
    "df_train = df_train.drop(index=df_train[df_train['text'].str.len()==0].index.tolist())\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7550323",
   "metadata": {
    "papermill": {
     "duration": 0.025151,
     "end_time": "2024-06-02T10:22:21.582391",
     "exception": false,
     "start_time": "2024-06-02T10:22:21.557240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Then we use almost the same procedures to treat the testing texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2cbdf85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:21.634400Z",
     "iopub.status.busy": "2024-06-02T10:22:21.634099Z",
     "iopub.status.idle": "2024-06-02T10:22:23.755430Z",
     "shell.execute_reply": "2024-06-02T10:22:23.754503Z"
    },
    "papermill": {
     "duration": 2.149665,
     "end_time": "2024-06-02T10:22:23.757418",
     "exception": false,
     "start_time": "2024-06-02T10:22:21.607753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard earthquake different cities stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest spot pond geese fleeing street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting Spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills China Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES SAFETY FASTENERS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm RI worse hurricane city amp hardest hit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook HWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CityofCalgary activated Municipal Emergency Pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                      Just happened terrible car crash  \n",
       "1           Heard earthquake different cities stay safe  \n",
       "2            forest spot pond geese fleeing street save  \n",
       "3                 Apocalypse lighting Spokane wildfires  \n",
       "4                   Typhoon Soudelor kills China Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES SAFETY FASTENERS...  \n",
       "3259  Storm RI worse hurricane city amp hardest hit ...  \n",
       "3260                      Green Line derailment Chicago  \n",
       "3261           MEG issues Hazardous Weather Outlook HWO  \n",
       "3262  CityofCalgary activated Municipal Emergency Pl...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv',index_col=None)\n",
    "#remove URL\n",
    "for i in df_test.index:\n",
    "    temp0 = [elt0 for elt0 in df_test.loc[i,'text'].split() if 'http' not in elt0]\n",
    "    temp1 = ' '.join(temp0)\n",
    "    df_test.loc[i,'text'] = temp1\n",
    "#remove punctuation marks\n",
    "for x0 in punctuation_marks:\n",
    "    df_test['text'] = df_test['text'].str.replace(x0,' ')\n",
    "#remove numbers\n",
    "for x0 in string.digits:\n",
    "    df_test['text'] = df_test['text'].str.replace(x0,' ')\n",
    "#remove stop words\n",
    "for i in df_test.index:\n",
    "    temp0 = [elt0 for elt0 in df_test.loc[i,'text'].split() if elt0.lower() not in list(ENGLISH_STOP_WORDS)]\n",
    "    temp1 = ' '.join(temp0)\n",
    "    df_test.loc[i,'text'] = temp1\n",
    "#collect non-English characters\n",
    "char0 = []\n",
    "for i in df_test.index:\n",
    "    #\n",
    "    x0 = df_test.loc[i,'text']\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    x0 = df_test.loc[i,'keyword']\n",
    "    x0 = str(x0)\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    x0 = df_test.loc[i,'location']\n",
    "    x0 = str(x0)\n",
    "    x0 = x0.replace(' ','')\n",
    "    char0 = char0 + list(x0)\n",
    "    #\n",
    "    char0 = list(set(char0))\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.ascii_letters]\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.punctuation]\n",
    "char0 = [x0 for x0 in char0 if x0 not in string.digits]\n",
    "#remove non-English characters\n",
    "for x0 in char0:\n",
    "    df_test['text'] = df_test['text'].str.replace(x0,' ')\n",
    "#remove extra spaces\n",
    "for i in df_test.index:\n",
    "    df_test.loc[i,'text'] = ' '.join(df_test.loc[i,'text'].split())\n",
    "#delete extremely short sentences\n",
    "#df_test = df_test.drop(index=df_test[df_test['text'].str.len()==0].index.tolist())\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdc28a",
   "metadata": {
    "papermill": {
     "duration": 0.025209,
     "end_time": "2024-06-02T10:22:23.809600",
     "exception": false,
     "start_time": "2024-06-02T10:22:23.784391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Word Vectors and Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414664df",
   "metadata": {
    "papermill": {
     "duration": 0.025108,
     "end_time": "2024-06-02T10:22:23.859844",
     "exception": false,
     "start_time": "2024-06-02T10:22:23.834736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We firstly tokenize all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e077661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:23.911609Z",
     "iopub.status.busy": "2024-06-02T10:22:23.911324Z",
     "iopub.status.idle": "2024-06-02T10:22:23.934373Z",
     "shell.execute_reply": "2024-06-02T10:22:23.933535Z"
    },
    "papermill": {
     "duration": 0.053582,
     "end_time": "2024-06-02T10:22:23.938762",
     "exception": false,
     "start_time": "2024-06-02T10:22:23.885180",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deeds Reason earthquake ALLAH Forgive',\n",
       " 'Forest near La Ronge Sask Canada',\n",
       " 'residents asked shelter place notified officers evacuation shelter place orders expected',\n",
       " 'people receive wildfires evacuation orders California',\n",
       " 'Just got sent photo Ruby Alaska smoke wildfires pours school',\n",
       " 'RockyFire Update California Hwy closed directions Lake County CAfire wildfires',\n",
       " 'flood disaster Heavy rain causes flash flooding streets Manitou Colorado Springs areas',\n",
       " \"I'm hill woods\",\n",
       " \"There's emergency evacuation happening building street\",\n",
       " \"I'm afraid tornado coming area\",\n",
       " 'people died heat wave far',\n",
       " 'Haha South Tampa getting flooded hah WAIT SECOND LIVE SOUTH TAMPA GONNA GONNA FVCK flooding',\n",
       " \"raining flooding Florida TampaBay Tampa days I've lost count\",\n",
       " 'Flood Bago Myanmar arrived Bago',\n",
       " 'Damage school bus multi car crash BREAKING',\n",
       " \"What's man\",\n",
       " 'love fruits',\n",
       " 'Summer lovely',\n",
       " 'car fast',\n",
       " 'goooooooaaaaaal',\n",
       " 'ridiculous',\n",
       " 'London cool',\n",
       " 'Love skiing',\n",
       " 'wonderful day',\n",
       " 'LOOOOOOL',\n",
       " \"way can't eat shit\",\n",
       " 'NYC week',\n",
       " 'Love girlfriend',\n",
       " 'Cooool',\n",
       " 'like pasta',\n",
       " 'end',\n",
       " 'bbcmtd Wholesale Markets ablaze',\n",
       " 'try bring heavy metal RT',\n",
       " 'AFRICANBAZE Breaking news Nigeria flag set ablaze Aba',\n",
       " 'Crying Set ablaze',\n",
       " 'plus LOOK SKY NIGHT ABLAZE',\n",
       " \"PhDSquares mufc they've built hype new acquisitions doubt set EPL ablaze season\",\n",
       " 'INEC Office Abia Set Ablaze',\n",
       " 'Barbados Bridgetown JAMAICA cars set ablaze SANTA CRUZ Head St Elizabeth Police Superintende',\n",
       " 'Ablaze Lord D',\n",
       " 'Check nsfw',\n",
       " \"outside you're ablaze alive you're dead inside\",\n",
       " 'awesome time visiting CFC head office ancop site ablaze Thanks Tita Vida taking care',\n",
       " 'SOOOO PUMPED ABLAZE southridgelife',\n",
       " 'wanted set Chicago ablaze preaching hotel',\n",
       " 'gained followers week Know stats grow',\n",
       " 'West burned Thousands wildfires ablaze California',\n",
       " 'Building perfect tracklist life leave streets ablaze',\n",
       " 'Check nsfw',\n",
       " \"night retainers It's quite weird Better used wear single night year\",\n",
       " 'Deputies Man shot Brighton home set ablaze',\n",
       " 'Man wife years jail setting ablaze niece',\n",
       " 'SANTA CRUZ Head St Elizabeth Police Superintendent Lanford Salmon r',\n",
       " 'Police Arsonist Deliberately Set Black Church North Carolina Ablaze',\n",
       " 'Noches El-Bestia Alexis Sanchez happy teammates training hard goodnight gunners',\n",
       " 'Kurds trampling Turkmen flag later set ablaze vandalized offices Turkmen Diyala',\n",
       " 'TRUCK ABLAZE R VOORTREKKER AVE OUTSIDE TAMBO INTL CARGO SECTION',\n",
       " 'Set hearts ablaze city gift skyline like kiss lips',\n",
       " \"sky ablaze tonight Los Angeles I'm expecting IG FB filled sunset shots know peeps\",\n",
       " 'West burned Thousands wildfires ablaze California climate energy',\n",
       " 'Revel wmv videos means mac farewell ablaze wmv en route dvd GtxRWm',\n",
       " 'Progressive greetings month students set pens ablaze Torch Publications',\n",
       " 'Rene Ablaze amp Jacinta Secret k Fallen Skies Edit Mar',\n",
       " 'Navista Steve fires California tinderbox clown setting hood ablaze News',\n",
       " 'NowPlaying Rene Ablaze amp Ian Buff Magnitude EDM',\n",
       " 'nxwestmidlands huge Wholesale markets ablaze',\n",
       " \"ablaze time does talk don't know make work\",\n",
       " \"'I can't kids cuz got bicycle accident amp split testicles it's impossible kids MICHAEL FATHER\",\n",
       " 'Accident I- W NashvilleTraffic Traffic moving m slower usual',\n",
       " 'Accident center lane blocked SantaClara US- NB Great America Pkwy BayArea Traffic',\n",
       " 'personalinjury accident summer Read advice amp solicitor help OtleyHour',\n",
       " 'stlouis caraccidentlawyer Speeding Causes Teen Accidents Car Accident tee',\n",
       " 'Reported motor vehicle accident Curry Herman Rd near Stephenson involving overturned vehicle use',\n",
       " 'BigRigRadio Live Accident Awareness',\n",
       " 'I- Mile Marker South Mooresville Iredell Vehicle Accident Ramp Closed PM',\n",
       " 'RT SleepJunkies Sleeping pills double risk car accident',\n",
       " \"'By accident knew gon happen\",\n",
       " 'Traffic accident N CABRILLO HWY MAGELLAN AV MIR',\n",
       " 'I- Mile Marker South Mooresville Iredell Vehicle Accident Congestion PM',\n",
       " 'pastor scene accident owner range rover',\n",
       " \"mom didn't home fast wished mom accident truck spilt mayonnaise\",\n",
       " \"horrible car accident past Sunday I'm finally able Thank GOD\",\n",
       " 'wait pissed Donnie tell accident',\n",
       " \"TruckCrash Overturns FortWorth Interstate Click you've\",\n",
       " 'Accident Ashville SB SR traffic',\n",
       " 'Carolina accident Motorcyclist Dies I- Crash Car Crossed Median motorcycle rider traveling',\n",
       " 'FYI CAD FYI ACCIDENT PROPERTY DAMAGE NHS PINER RD HORNDALE DR',\n",
       " 'RT nAAYf accident years Turning Chandanee Magu near MMA Taxi rammed halfway turned conf',\n",
       " 'Accident left lane blocked Manchester Rt NB Eddy Rd stop traffic NH- delay mins traffic',\n",
       " 'ACCIDENT PROPERTY DAMAGE PINER RD HORNDALE DR',\n",
       " 'accident',\n",
       " 'FYI CAD FYI ACCIDENT PROPERTY DAMAGE WPD S TH ST',\n",
       " 'PM TRAFFIC ACCIDENT INJURY WILLIS FOREMAN RD',\n",
       " 'Aashiqui Actress Anu Aggarwal Near-Fatal Accident',\n",
       " 'Suffield Alberta Accident',\n",
       " 'Mile backup I- South accident blocking Right Lanes Exit Langtree Rd consider NC NC NC alternate',\n",
       " 'accident changed life help determine options financially support life care plans on-going treatment',\n",
       " \"BREAKING deadly motorcycle car accident happened Hagerstown today I'll details State WHAG\",\n",
       " 'flowri marinading accident',\n",
       " \"car week got fucking car accident Mfs can't fucking drive\",\n",
       " 'NorwayMFA Bahrain police previously died road accident killed explosion',\n",
       " 'heard Church Leaders Kenya coming forward comment accident issue disciplinary measures ArrestPastorNganga',\n",
       " 'afterShock DeLo scuf ps live game cya',\n",
       " \"'The man drive effort gets painful man win Roger Bannister\",\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " \"'There victory bargain basement prices Dwight David Eisenhower\",\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " \"'Nobody remembers came second Charles Schulz\",\n",
       " 'afterShock DeLo im speaking using scuf xb people end getting ps',\n",
       " \"'The harder conflict glorious triumph Thomas Paine\",\n",
       " \"GrowingUpSpoiled going clay pigeon shooting crying aftershock'\",\n",
       " 'guess actually wants free Aftershock TC',\n",
       " \"Aftershock terrifying best roller coaster I've DISCLAIMER I've\",\n",
       " 'Aftershock',\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " \"KJForDays I'm seeing Issues aftershock\",\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " 'IR ICEMOON AFTERSHOCK djicemoon Dubstep TrapMusic DnB EDM Dance Ices',\n",
       " 'WisdomWed BONUS Minute Daily Habits really improve life lifehacks',\n",
       " 'Aftershock Protect Profit Global Financial Meltdown David Wiedemer',\n",
       " 'moment scary roller coaster guy just screaming bloody murder silverwood aftershock',\n",
       " 'Aftershock Full Streaming YouTube',\n",
       " 'gt gt Aftershock Protect Profit Global Financial book esquireattire',\n",
       " \"face difficulties you're doing wrong you're doing right Joel Osteen\",\n",
       " \"'The thing stands dream try belief actually possible Joel Brown\",\n",
       " 'Praise God ministry tells like wdyouth biblestudy',\n",
       " \"'Remembering going die best way know avoid trap thinking lose Steve Jobs\",\n",
       " 'Tried orange aftershock today life',\n",
       " 'OnFireAnders love bb',\n",
       " 'Aftershock',\n",
       " 'Aftershock school kick great want thank making possible great night',\n",
       " 'People say interrupt doing George Bernard Shaw',\n",
       " \"'The man gets oyster second man gets shell Andrew Carnegie\",\n",
       " 'need P U tonight play Hybrid Slayer ps EU HMU Cod sandscrims EmpirikGaming CoDAWScrims TP KOTC TPFA afterShock Org',\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts o news',\n",
       " 'Strict liability context airplane accident Pilot error common component aviation cr',\n",
       " 'crobscarla lifetime odds dying airplane accident',\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts Wedn',\n",
       " \"AlexAllTimeLow awwww they're airplane accident they're gonna die cuties good job\",\n",
       " 'family members osama bin laden died airplane accident ironic mhmmm gov shit suspect',\n",
       " 'Man Goes Airplane Engine Accident YouTube',\n",
       " 'Horrible Accident Man Died Wings Airplane - -',\n",
       " 'Cessna airplane accident Ocampo Coahuila Mexico July killed men including State Coahuila government official',\n",
       " 'Horrible Accident Man Died Wings Airplane - - WatchTheVideo',\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts Wednesday',\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts Wednesday began examining t',\n",
       " \"KCA VoteJKT ID mbataweel RIP BINLADEN Family members killed airplane's accident\",\n",
       " 'sent coworker nudes accident thank god airplane mode',\n",
       " \"mickinyman TheAtlantic killed airplane accident night car wreck Politics it's best\",\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts MLB',\n",
       " 'unbelievably insane man airport airplane aircraft aeroplane runway accident freaky',\n",
       " 'Horrible Accident Man Died Wings Airplane - -',\n",
       " 'Horrible Accident Man Died Wings Airplane - -',\n",
       " 'Usama bin Ladins family dead airplane crash Naturally accident',\n",
       " 'Pilot Dies Plane Crash Car Festival YouTube Crash Aircraft Airplane Pilot Death Accident CarFest',\n",
       " 'Strict liability context airplane accident',\n",
       " 'DTN Brazil Experts France begin examining airplane debris Reunion Island French air accident exp',\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts Wedn',\n",
       " 'Horrible Accident Man Died Wings Airplane - - WTF Can t Believe EYES',\n",
       " \"Nicole Fletcher victim crashed airplane times ago accident left little bit trauma she's\",\n",
       " 'OMG Horrible Accident Man Died Wings Airplane',\n",
       " \"OMG don't believe RIP bro AirPlane Accident JetEngine TurboJet Boing G\",\n",
       " 'Experts France begin examining airplane debris Reunion Island French air accident experts Wednesday began examining t',\n",
       " 'airplane accident',\n",
       " 'phone looks like car ship airplane accident Terrible',\n",
       " \"Statistically I'm risk getting killed cop dying airplane accident\",\n",
       " 'airplane crashes house Colombia people die accident',\n",
       " 'shooting airplane accident',\n",
       " 'drone cause airplane accident Pilots worried use drones esp close vicinity airports',\n",
       " 'Early wake sister begging come amp ride w ambulance hospital RODKiai',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'air ambulances scene crash cars lorry EMSNe',\n",
       " 'feared killed Pakistani air ambulance helicopter crash Reuters yugvani',\n",
       " 'Leading emergency services boss welcomes new ambulance charity',\n",
       " \"travelling Aberystwyth-Shrewsbury right there's incident Services halt just outside Shrews Ambulance scene\",\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE CHOICE LEZ COMPLIANT eBay',\n",
       " 'New Nanotech Device Able Target Destroy Blood Clots',\n",
       " 'skyhawkmm traplord FREDOSANTANA LilReese hella crazy fights ambulance couple mosh pits',\n",
       " 'run ambulance lucky justsaying randomthought',\n",
       " 'news feared killed Pakistani air ambulance helicopter crash til DNA',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'TanSlash waiting ambulance',\n",
       " 'fouseyTUBE ok Need ambulance Hahahah good',\n",
       " 'AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE CHOICE LEZ COMPLIANT eBay',\n",
       " 'Pakistan air ambulance helicopter crash kills',\n",
       " 'TheNissonian RejectdCartoons nissan ok need medical assistance ambulance need',\n",
       " 'EMS NY EMTs petition hour minimum wage ems paramedics ambulance',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE CHOICE LEZ COMPLIANT eBay',\n",
       " 'AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE CHOICE LEZ COMPLIANT eBay',\n",
       " \"Kiwi Karyn Check what's parking lot said year ambulance St Johns\",\n",
       " \"don't know way ambulance coming lt lt\",\n",
       " 'reuters feared killed Pakistani air ambulance helicopter crash',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'ambulance right outside work',\n",
       " \"LeoBlakeCarter dog thinks he's ambulance\",\n",
       " 'HAPPENING HATZOLAH EMS AMBULANCE RESPONDING DUAL SIRENS AND',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'feared killed Pakistani air ambulance helicopter crash worldNews',\n",
       " 'feared killed Pakistani air ambulance helicopter crash worldnews',\n",
       " \"What's police ambulance number Lesotho body know\",\n",
       " 'medic AACE org surprised standardised clinical practice NHS ambulance trust',\n",
       " 'feared killed Pakistani air ambulance helicopter crash',\n",
       " 'People try j-walk ambulance passing hate',\n",
       " 'episode Trunks annihilated Freiza cleanest shit showed nigga mercy',\n",
       " 'SHALL ANNIHILATED PETEBESTS DESSICATED LAID BARE SHALL KNEEL',\n",
       " 'Uribe just annihilated baseball Mets',\n",
       " 'marksmaponyane Hey Sundowns annihilated previous meeting Celtic improvement',\n",
       " \"Volfan TNeazzy Mizzou annihilated florida past seasons ended muschamp's career just can't compete Bama\",\n",
       " 'Annihilated Abs',\n",
       " 'annihilated status education mba behalf easy street careen eOvm',\n",
       " 'Luka die annihilated Alois Trancy',\n",
       " \"ACarewornHeart good fella sorry won't annihilated\",\n",
       " 'Cop pulls drunk driver safety SECONDS car hit train ViralSpell',\n",
       " 'annihilated',\n",
       " 'Cop pulls drunk driver safety SECONDS car hit train ViralSpell',\n",
       " 'BOOM country just entirely annihilated h Britain',\n",
       " 'AmirKingKhan annihilated thank FloydMayweather',\n",
       " 'thing sure-God promised Israel annihilated horror Iran w nukes',\n",
       " \"violentfeminazi guess that's ok Armenians we've spent history getting annihilated\",\n",
       " 'years annihilated people instantly aware ability annihilate humanity',\n",
       " 'day tryouts went good minus fact stopped quickly short ball Annihilated toenail injury',\n",
       " 's oryx symbol Arabian Peninsula annihilated hunters',\n",
       " 'Luka die annihilated Alois Trancy',\n",
       " 'Ready annihilated BUCS game',\n",
       " 'PhilipDuncan breakfastone People annihilated nights weather Really Philip thought forecast',\n",
       " 'Domain sophistication annihilated closely up-to-the-minute feat ZrNf',\n",
       " 'stormbeard steel lord seen Judas Priest Rob came Scorpions support Fucking annihilated place Astonishing gig',\n",
       " \"Officially skipping FantasticFour Fant stic hashtag It's getting ANNIHILATED reviews Bummer\",\n",
       " 'TomcatArts explaining annihilated case survivor evolved godlike',\n",
       " 'just completely annihilated cech paul keegan time alive',\n",
       " 'TomcatArts annihilated legion survivors imperfect hybrid project quickly formed new secret cell',\n",
       " \"SirBrandonKnt exactly That's lesnar cena match summerslam year great Brock annihilated guy who's\",\n",
       " 'Cop pulls drunk driver safety SECONDS car hit train ViralSpell',\n",
       " 'ANNIHILATED DAMASCUS SYRIAN ARMY GRINDS ALLOOSH GANG MANURE PILE',\n",
       " \"thatdes ok wasn't completely forthright food coma bc kebab tahini pickles annihilated w fries\",\n",
       " 'fun filled happy-hour Simmons bar Camden handsome got annihilated apart game',\n",
       " 'Juanny Beisbol Sr Annihilated ball LGM',\n",
       " 'rvfriedmann Hell just fraction belief total annihilation destruction USA LodiSilverado ritzy jewels',\n",
       " \"POTUS Maybe Israel tell we're sorry Pres sold river annihilation\",\n",
       " 'Evildead Annihilation Civilization',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'annihilating quarterstaff annihilation',\n",
       " 'World Annihilation vs Self Transformation Aliens Attack Exterminate Humans',\n",
       " 'StarMade Stardate Planetary Annihilation YouTube',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'KimKardashian sign share petition save wild horses Arizona',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'TheEllenShow check Salt River horses help stop annihilation happen signatures change org Thx',\n",
       " 'souls punished with annihilation',\n",
       " 'CalFreedomMom steph mention major contributor annihilation Israel',\n",
       " 'willienelson need help Horses die RT amp sign petition stand amp voice gilbert',\n",
       " 'reject laws misguided false prophets imprison nations fueling self annihilation',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'annihilation Jeb Christie amp Kasich hours away God allow day',\n",
       " 'Barbi Twins need help-horses die RT amp sign petition stand amp voice gilbert',\n",
       " 'Whippenz need help Horses die RT amp sign petition stand amp voice gilbert',\n",
       " 'Hey AZ Sign petition save WildHorses TantoNationalForest RollingStones sing-a-long order',\n",
       " 'Stop Annihilation Salt River Wild Horses Change',\n",
       " \"SonofBaldwin he's current Nova bookslast checked tied books Rider died Annihilation\",\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'sign amp RT save SaltRiverWildHorses',\n",
       " 'THANKS COUNT DANTE JOIN FOLLOWING ANNIHILATION ZONE JOHNNY',\n",
       " 'World Annihilation vs Self Transformation Aliens Attack Exterminate Humans',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " 'U S National Park Services Tonto National Forest Stop Annihilation Salt River Wild Horse Change',\n",
       " \"I'm gonna fight Taylor soon\",\n",
       " \"ohH FUKURODANI DIDN'T SURVIVE APOCALYPSE BOKUTO FEELS HORRIBLE poor boy ppor child\",\n",
       " 'jocelyn birthday apocalypse',\n",
       " \"RT janenelson RT StephenSCIFI Adaptation Watch Charlie Human's APOCALYPSE Optioned Film sciencefiction\",\n",
       " 'Apocalypse',\n",
       " \"hour It's August PM Here's Red Rover Zombie Apocalypse internetradio collegeradi\",\n",
       " 'HoneyBunzGem primalkitchen feel like doing pull-up stages Apocalypse',\n",
       " \"She's kinda hot played radio today What's Disease apocalypse started careful\",\n",
       " \"it's apocalypse lol gf m\",\n",
       " \"know it's question interpretation sign apocalypse called\",\n",
       " 'Julie R apocalypse version Romeo Juliet warmbodies',\n",
       " 'apocalypse',\n",
       " 'RT fittscott Minecraft NIGHT LUCKY BLOCK MOD BOB APOCALYPSE WITHER amp Mod Showcase Popularmmos vi',\n",
       " 'begins day snow apocalypse',\n",
       " 'RT Mother Mary Short Reading Apocalypse spirit angel took enormous high mountain',\n",
       " 'candylit Imagine sarumi zombie apocalypse Fighting Heart heart conversations',\n",
       " 'RT ZONEWolf liked YouTube video Minecraft NIGHT LUCKY BLOCK MOD BOB APOCALYPSE WITHER amp Mo',\n",
       " \"that's planet it's lone audience apocalypse\",\n",
       " \"Dad bought DVD looks like science doc read it's actually impending biblical apocalypse\",\n",
       " \"alexandrapullin apocalypse comes week know I'll\",\n",
       " 'LOOK GRIZZLY PEAK RIGHT looks like beginning dystopian apocalypse movie',\n",
       " 'niece just asked scared apocalypse',\n",
       " \"There's Storm Cairo latest X-Men Apocalypse set photo YahooTV\",\n",
       " 'Minecraft NIGHT LUCKY BLOCK MOD BOB APOCALYPSE WITHER amp Mod Showcase Popularmmos YouTube',\n",
       " 'Shot Heart XV going totally love bad heart pierc',\n",
       " 'RT Geek Apocalypse pm GMT Hesse plays dark souls day etcPB',\n",
       " 'know zombies',\n",
       " 'latest BryanSinger reveals Storm queen Apocalypse RuPaul AlexShipppp',\n",
       " 'Shadow boxing apocalypse',\n",
       " 'Short Reading Apocalypse spirit angel took enormous high mountain',\n",
       " \"Enjoyed live-action Attack Titan time posters I'm reminded freshly clean coiffed apocalypse\",\n",
       " 'liked YouTube video Minecraft NIGHT LUCKY BLOCK MOD BOB APOCALYPSE WITHER amp Mod Showcase',\n",
       " 'PBBan Temporary avYsss aRmageddon KILL FLAGS Fast XP Reason',\n",
       " 'PBBan Temporary Russaky aRmageddon KILL FLAGS Fast XP Reason',\n",
       " 'OFFICIAL VID DoubleCups gt gt gt gt TrubGME Prod THISIZBWRIGHT gt gt ARMAGEDDON',\n",
       " 'ouvindo Peace Love amp Armageddon',\n",
       " \"Best movie you've seen Armageddon\",\n",
       " \"Bed time Don't wake unless revolution Armageddon start\",\n",
       " 'Red Faction Armageddon Microsoft Xbox read eBay',\n",
       " 'KatieKatCubs know shit goes World Series Armageddon',\n",
       " 'RT Ophiuchus Love TrueLove romance lith Voodoo seduction Astrology RTRRT LOTZ - apocalypse Armageddon pla',\n",
       " 'im gonna beat armageddon Hsu Hao just got flawless try',\n",
       " \"ENews Ben Affleck know there's wife kids girls can't help I've loved Armageddon eonlinechat\",\n",
       " \"'If I'd long coat hand I'd worn certainty armageddon bears sense occasion\",\n",
       " 'PHONE SPYING Hidden Door NSA Data Mining Software FINANCIAL ARMAGEDDON BLOG',\n",
       " 'PBBan Temporary hyider ghost aRmageddon KILL FLAGS Fast XP Reason',\n",
       " 'RT RTRRTcoach Love TrueLove romance lith Voodoo seduction Astrology RTRRT LOTZ - apocalypse Armageddon planet',\n",
       " 'PBBan Temporary fighterdena aRmageddon KILL FLAGS Fast XP Reason',\n",
       " 'Photo Sketch did based Taste Armageddon episode startrek tos',\n",
       " 'Armageddon',\n",
       " 'AberdeenFC AberdeenFanPage Good luck tomorrow night coefficient points plz Armageddon',\n",
       " 'paddytomlinson ARMAGEDDON',\n",
       " \"RohnertParkDPS You're history books Thank Justice Department way haven't paid income tax yrs\",\n",
       " 'Vladimir Putin Issues Major Warning Late Escape Armageddon',\n",
       " \"God's Kingdom Heavenly Gov't rule people earth Armageddon\",\n",
       " 'L B Entertainment lot BruceWillis MOVIES DVD DIE HARD MONKEYS ARMAGEDDON SIXTH eBay Auction',\n",
       " 'Let s talk goof guild Saunders Come right stage',\n",
       " \"Karnythia niece gaining ability stand I'm getting prepared toddler apocalypse Armageddon\",\n",
       " 'Check PREPPERS DOOMSDAY LIBRARY COLLECTION CD shtf preppertalk survival prepper eBay',\n",
       " 'Erker Eep Thought yesterday saw hella scary hail armageddon',\n",
       " 'Ahamedis think Messiah come years ago Armageddon Dajaal Gog amp Magog',\n",
       " \"Sadly Windows Reveals Microsoft's Ethics Armageddon\",\n",
       " 'Armageddon averted El Patron UltimaLucha',\n",
       " 'samihonkonen time hours latest series WW Blueprint Armageddon extremely impressive',\n",
       " 'European Fitba till Christmas ARMAGEDDON',\n",
       " 'Christians United Israel CUFI Jews convert soon die armageddon',\n",
       " 'OFFICIAL VID gt DoubleCups gt gt gt gt TrubGME Prod THISIZBWRIGHT gt gt ARMAGEDDON',\n",
       " 'Tomorrow day start armageddon preseasonworkouts',\n",
       " 'Lee does comedy LeeJasper Working class Tories prepare Armageddon InterestRateRise',\n",
       " 'Charts Prove Financial Crisis BEGUN Financial Armageddon Economic Collapse Blog tracks tren',\n",
       " 'Paul Craig Roberts Vladimir Putin Issues Major Warning Late Escape brics roberts russia',\n",
       " \"RohnertParkDPS You're stage Right lights Isn't funny goofballs staff PD\",\n",
       " 'OFFICIAL VID TheReal gt gt gt gt gt gt TrubGME gt gt gt ARMAGEDDON Comin Soon',\n",
       " 'Celtic Fingers crossed Aberdeen tomorrow night Armageddon eh',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " 'Seconds Summer pick Fan Army SOSFAM',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " 'Seeing army whitewalkers thing slightly intrigued GoT far',\n",
       " 'Build kingdom lead army victory Start g friend code LZKTJNOX',\n",
       " 'Salvation Army hosts rally reconnect fathers children Salvation Army hosting school rally',\n",
       " 'Vote Directioners vs Queens th round Billboard FanArmyFaceOff',\n",
       " 'build army dogs leader lion dogs fight like lion',\n",
       " \"'Show Hero TV Review\",\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " 'INFANTRY Mens Lume Dial Army Analog Quartz Wrist Watch Sport Blue Nylon Fabric rea',\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " \"VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH\",\n",
       " 'RT DrAyesha IndiaKoMunTorJawabDo Indian Army ki',\n",
       " 'Seconds Summer pick Fan Army SOSFAM',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " 'da MTVSummerStar VideoVeranoMTV MTVHottest Britney Spears Lana Del Rey',\n",
       " 'Direction pick Fan Army Directioners',\n",
       " \"Stony Jackson America's hope leads army felons army rejects army o Satan\",\n",
       " 'WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEATHER WATCH WAR MIDO WW read eBay',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'Beyonce pick Fan Army Beyhive',\n",
       " 'POTUS appoints Brig Gen Richard G Kaiser member Mississippi River Commission Learn MRC',\n",
       " 'AP violent country army involved help control killings bring peace poor people',\n",
       " 'WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEATHER WATCH WAR MIDO WW read eBay',\n",
       " 'WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEATHER WATCH WAR MIDO WW read eBay',\n",
       " 'Direction pick Fan Army Directioners x',\n",
       " 'Direction pick Fan Army Directioners',\n",
       " 'Jewish Terrorists Charged Historic-Church Arson Ugly Truth',\n",
       " \"Spokane authorities say they're struggling solve arson cases like today's Hamilton\",\n",
       " 'Thousands attend rally organized Peace protesting arson attack took life',\n",
       " 'Add Familia arson squad',\n",
       " 'fake hate crime Lesbians burn house new',\n",
       " 'Los Angeles Times Arson suspect linked fires caught Northern NewsInTweets',\n",
       " 'Mourning notices stabbing arson victims stir politics grief Israel',\n",
       " 'Mourning notices stabbing arson victims stir politics grief Israel',\n",
       " 'Sound Arson',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme LGBT',\n",
       " 'Wait',\n",
       " 'Arson suspect linked fires caught Northern California',\n",
       " 'Trial Date Set Man Charged Arson Burglary',\n",
       " 'death Palestinian toddler arson attack Israel cracks Jewish',\n",
       " 'Palestinian Teen Killed Amid Protests Arson Attack',\n",
       " 'Kisii Police Kisii hunt students failed arson plot Police Kisii hunt students CountyNews',\n",
       " 'Mariah getting shoulders poor girl',\n",
       " 'Mourning notices stabbing arson victims stir politics grief Israel Posters Shira Banki',\n",
       " 'RelaxInPR miprv RT latimes Arson suspect linked fires caught Northern California',\n",
       " 'Jewish leaders prayed hospital Palestinian family treated arson HuffPostRelig',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme theadvocatemag LGBT',\n",
       " 'sayn ae angel arson',\n",
       " 'Kisii Police Kisii hunt students failed arson plot Police Kisii hunt students CountyNews',\n",
       " 'Mourning notices stabbing arson victims stir politics grief Israel',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme theadvocatemag LGBT',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme LGBT',\n",
       " 'Mourning notices stabbing arson victims stir politics grief Israel',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme LGBT',\n",
       " 'Arson suspect linked fires caught Northern California latimes',\n",
       " 'Tennessee lesbian couple faked hate crime destroyed home arson Lesbian',\n",
       " 'Arson suspect linked fires caught Northern California Los Angeles Times',\n",
       " 'Arson suspect linked fires caught Northern California',\n",
       " 'DC Cloudy goldrush hate white people mo',\n",
       " 'NOWPLAYING Arsonist MC Impressed ARSONISTMUSIC',\n",
       " 'Alleged East Bay serial arsonist arrested',\n",
       " 'Safyuan just minor citation possesion decriminalized substance im facing time',\n",
       " 'Suspected serial arsonist arrested Calif',\n",
       " 'Arson suspect linked fires caught Northern California',\n",
       " 'local arsonist diamorfiend legal forgets',\n",
       " 'Casper rmg u dick',\n",
       " 'Bloor Ossington arsonist burned mattress Northumberland St cbcto',\n",
       " \"'wHeRE's aRsOnISt\",\n",
       " \"don't nice say come sit\",\n",
       " 'Vegetarian Vegan Video shows arsonist torching popular BK restaurant Strictly Vegetarian GoVegan UniteBlue',\n",
       " 'Arsonist arrested setting fires WATCH tonight s headlines Nightbeat VeronicaDLCruz MinuteMix',\n",
       " 'Video Captures Man Removing American Flag Long Beach CA Home Burning Arsonist Sought',\n",
       " 'hif trick think nasty things',\n",
       " 'Spotlight Paradise Arsonist MC WNIAGospel arsonistmusic',\n",
       " 'makes',\n",
       " 'town salem just melted ice cube bc im arsonist D',\n",
       " 'Arsonists blamed blaze plastics recycling business Adelaide pcaldicott reports NewsAdl',\n",
       " 'hotboy shit',\n",
       " 'Zodiac Girl feat Trey Dupree Produced Sparkz Beatz Chuck Da Arsonist',\n",
       " 'local arsonist LMFAO',\n",
       " 'Alleged East Bay serial arsonist arrested SanFrancisco',\n",
       " 'Doofus diamorfiend im jokin moves',\n",
       " \"local arsonist guess u say it's just shit thinking\",\n",
       " 'Arsonist Sets NYC Vegetarian Restaurant Police NewYork',\n",
       " 'liked YouTube video slimebeast Town Salem Win Arsonist',\n",
       " 'Casper rmg BestComedyVine whats cracking cuz',\n",
       " 'smoke good fuck eat drink drive nice car wear green mink',\n",
       " 'kill got court day earl',\n",
       " 'local arsonist lmao real live',\n",
       " 'Owner Chicago-Area Gay Bar Admits Arson Scheme Frank Elliott pleaded guilty hiring arsonist',\n",
       " 'Trusting Iran stop terrorism like inviting arsonist join brigade Telegraph',\n",
       " 'Big Burning True Story Arsonist Missing Girl',\n",
       " 'Stay vigilent Civil liberties constant attack nativehuman myreligion',\n",
       " 'Credit pfannebeckers inspiring rediscover fantabulous tbt',\n",
       " 'Nashville Theater Attack Gun Grabbers Demand Hatchet Control',\n",
       " 'BREAKING Terror Attack Police Post Udhampur',\n",
       " 'Demi stans really think Heart Attack sold million copies',\n",
       " \"scares there's new versions nuclear attack warnings like just knowing governments prepare\",\n",
       " 'ISIL claims suicide bombing Saudi mosque killed',\n",
       " 'DatTomm funniest twitter feminists try attack Head',\n",
       " 'Horrific attack wife muslim Italy LiveLeak News',\n",
       " 'blasts accused Yeda Yakub dies Karachi heart attack Mumbai',\n",
       " 'etribune Drone attack kills -suspected militants North Waziristan AceBreakingNews',\n",
       " 'Suspect latest theater attack psychological issues',\n",
       " 'Militants attack police post Udhampur SPOs injured LiveMint AllTheNews',\n",
       " 'BREAKING Obama Officials GAVE Muslim Terrorist Weapon Used Texas Attack',\n",
       " 'Delhi Government Provide Free Treatment Acid Attack Victims Private Hospitals',\n",
       " 'New post darkreading New SMB Relay Attack Steals User Credentials Internet',\n",
       " 'Israeli forces raid home alleged car attack suspect palestine',\n",
       " \"Just heart attack thought goat dead don't worry Rocket okay\",\n",
       " \"I'm gonna lie I'm kinda ready attack Senior year\",\n",
       " \"'Left hand diamond graveyard shift attack defend right handside fucking idiot\",\n",
       " 'volleyball Attack II Volleyball Training Machine Sets Simulation',\n",
       " \"Notley's tactful direct response Harper's attack Alberta's gov't Hell YEAH Premier ableg cdnpoli\",\n",
       " 'Police Assailant latest movie theatre attack homeless psychological issues',\n",
       " 'CaIxxum SOS thanks damn heart attack',\n",
       " 'Suspect latest theatre attack psychological issues',\n",
       " 'India shud evidence pak share terrorists amp use attack Share oth contries',\n",
       " 'illegal alien released Obama DHS times Charged Rape amp Murder Santa Maria CA Woman Prior Offenses',\n",
       " 'Strongly condemn attack ARY news team Karachi cowardly act simply trying job',\n",
       " 'Nashville Theater Attack Gun Grabbers Demand Hatchet Control',\n",
       " 'fact atomic bombs called Little Boy Fat man says lot mentality went attack',\n",
       " 'blazerfan ignoranceshe Latinoand benothing morebut attack dog hate group GOP',\n",
       " 'Heart disease prevention secondhand smoke',\n",
       " 'Dayton-area org tells hit cyber attack',\n",
       " \"Attack Titan game PS Vita yay Can't wait\",\n",
       " 'infowars Nashville Theater Attack Gun Grabbers Demand Hatchet Control nwo',\n",
       " 'anxiety attack',\n",
       " 'dog attacked food pugprobs',\n",
       " 'Cop injured gunfight militants attack Udhampur police post Suspected militants attacked police post',\n",
       " 'envw NickCoCoFree JulieDiCaro jdabe asked did feel attacked julie asked frail',\n",
       " 'messeymetoo feel attacked',\n",
       " \"I'm feeling attacked\",\n",
       " \"black men didn't make way White men did black men getting attacked\",\n",
       " \"believe fucking cis female going claim offended transgendered female who's attacked media\",\n",
       " 'Israeli helicopters attacked civilians Gaza just completed exercises Greece',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'PT unit attacked responsible targeting Muslim Scholars imprisoning youth',\n",
       " 'Telnet attacked STREAMYX-HOME-SOUTHERN',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'TBT Remember time Patrick Kane attacked cab driver',\n",
       " 'im feeling attacked',\n",
       " 'IK Troll Pol Rivals Literally Abused Attacked Families Literally Abuse IK Loosers',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'envw NickCoCoFree JulieDiCaro jdabe worst person Questioning julie attacked guys empathy',\n",
       " 'Kelly Osbourne attacked racist Donald Trump remark Latinos View',\n",
       " \"eunice njoki aiii needs chill answer calmly like she's attacked\",\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " \"attacked Robot-lvl I've earned total free satoshis robotcoingame Bitcoin FreeBitcoin\",\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'christinalavv lindsay wynn just saw tweets feel really attacked',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'TV program saw said air plane flew uranium Fukushima attacked machine gun student army digging',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'MageAvexis lt things attacked',\n",
       " 'Christian Attacked Muslims Temple Mount Waving Israeli Flag Pamela Geller',\n",
       " 'WeLoveLA NHLDucks Avalanche Defense Match vs St Louis Blues SportsRoadhouse',\n",
       " 'liked YouTube video Kalle Mattson Avalanche Official Video',\n",
       " \"we'll crash like avalanche\",\n",
       " \"Colorado Avalanche Men's Official Colorado Avalanche Reebok T-Shirt XL Blue Cotton NHL Hockey\",\n",
       " 'TIX Frozen Fury XVII Los Angeles Kings v Avalanche Row AA MGM Grand',\n",
       " 'BET DIDNT KNOW KICK BOX',\n",
       " \"little piece wrote Avalanche Designs blog I'd appreciate greatly checked\",\n",
       " 'PATRICK ROY - UPPER DECK SPX FINITE COLORADO AVALANCHE MINT',\n",
       " \"Musician Kalle Mattson Recreates Classic Album Covers Clever Music Video Avalanche'\",\n",
       " 'Beautiful Sweet Avalanche Faith Akito roses lots frothy gyp weddinghour',\n",
       " '- TIX Calgary Flames vs COL Avalanche Preseason Scotiabank Saddledome',\n",
       " 'Secrets avalanche catechize inner self confidential communication respects creating worth len',\n",
       " 'fall leaves poplar fully ordained tumbling avalanche Spurgeon',\n",
       " 'saw great punk bands making original music week Check em GHOSTOFTHEAV MontroseBand',\n",
       " 'GREAT PERFORMANCE CHIP FUEL GAS SAVER CHEVY TAHOE BLAZER AVALANCHE S-',\n",
       " 'Musician Kalle Mattson Recreates Classic Album Covers Clever Music Video Avalanche',\n",
       " 'driving avalanche having car week like driving tank',\n",
       " 'Free Ebay Sniping RT Chevrolet Avalanche Ltz Lifted x Truck Favorite amp Share',\n",
       " \"Chiasson Sens can't come deal ColoradoAvalanche Avalanche\",\n",
       " 'Paul Rudd Emile Hirsch David Gordon Green Prince Avalanche Q amp Filmmakers Google entretenimento Video',\n",
       " 'Great time deal Avalanche music purchase Neal Rigga shirt',\n",
       " 'bigperm drafted Avalanche rd overall Played season UtahGrizz',\n",
       " 'GOT VIDEOS RAPPERS GOT SONGS',\n",
       " 'funkflex yo flex im',\n",
       " \"avalanche world away make believing I'm wide awake\",\n",
       " 'possible new jerseys Avalanche year',\n",
       " \"feat Watch BTS kallemattson's incredible music video Avalanche\",\n",
       " 'Avalanche City Sunset nowplay listen radio',\n",
       " 'Chevrolet Avalanche LT lt used l v v automatic wd pickup truck premium b',\n",
       " 'snowflake avalanche feels responsible',\n",
       " 'STAR WARS POWER JEDI COLLECTION BATTLE DROID HASBRO read eBay',\n",
       " 'CIVIL WAR GENERAL BATTLE BULL RUN HERO COLONEL nd NEW HAMPSHIRE LETTER SIGNED',\n",
       " 'Dragon Ball Z Battle Gods Rotten Tomatoes RottenTomatoes',\n",
       " 'added video YouTube playlist World Tanks Battle Assistant Mod Bat Chat Arti kaboom',\n",
       " 'YA BOY CLIP VS KUS BATTLE MofeRadio Heavybag battle dom QOTRING BattleRapChris Hughes',\n",
       " 'fully aware battle support fight',\n",
       " \"It's baaaack Petersen's Bowhunting Battle Bows Make sure head cast vote\",\n",
       " \"Tb throwback want battle Here's War\",\n",
       " 'Kelby Tomlinson mild-mannered nd baseman great metropolitan team fights never-ending battle hits RBI SFGiants way',\n",
       " 'Black Eye space battle occurred Star M involving fleets totaling ships destroyed',\n",
       " 'really happened Taken King Story Trailer space battle ripped hole Saturn EyTay neur sis',\n",
       " 'BATTLE ANIMATIONS FUCKING',\n",
       " 'happens Battle Block CBSBigBrother finally',\n",
       " 'DU gon rap battle',\n",
       " 'Thing Battle Internal vs External Motivation',\n",
       " 'Check item just got Phantasmal Cummerbund Warcraft',\n",
       " 'young German stormtrooper engaged Battle Somme',\n",
       " 'liked YouTube video Marvel VS DC Avengers Battle',\n",
       " 'UtahCanary sigh daily battle',\n",
       " \"SexyDragonMagic I've come realization just don't attention span mass battle games painting playing\",\n",
       " 'DetroitPls interested win battle',\n",
       " 'Battle GOATS',\n",
       " 'STAR WARS POWER JEDI COLLECTION BATTLE DROID HASBRO read eBay',\n",
       " 'Black Eye space battle occurred Star O involving fleets totaling ships destroyed',\n",
       " 'LonePine remembered Australia descendants grow canberra Gallipoli WW',\n",
       " \"did miss Gary Busey's son plays DIXIE electronic green fiddle post-battle celebration sequence\",\n",
       " 'News FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer transport bioterror germs USATODAY',\n",
       " 'USA TODAY FedEx longer transport bioterror pathogens NewsInTweets',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps usatoday',\n",
       " 'Jacksonville Busines FedEx stops shipping potential bioterror pathogens',\n",
       " 'FedEx longer transport bioterror pathogens wake anthrax lab mishaps',\n",
       " 'APHL responds FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'News FedEx longer transport bioterror germs wake anthrax lab mishaps say FedEx TCOT',\n",
       " 'FedEx stop transporting bioterror germs lab mishaps FedEx stopped transporting certain research',\n",
       " 'FedEx longer ship potential bioterror pathogens AtlBizChron',\n",
       " \"frontpage Bioterror lab faced secret sanctions RickPerry doesn't make cut FoxNews GOPDebate USATODAY\",\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps usatoday',\n",
       " 'House Energy amp amp Commerce subcommittee hold hearing CDC oversight bioterror labs Army anthrax mishaps htt',\n",
       " 'FedEx willing transport research specimens potential bioterror pathogens wake anthrax lab mishaps',\n",
       " 'world FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps news phone apple mobile',\n",
       " 'RT alisonannyoung EXCLUSIVE FedEx longer transport research specimens bioterror pathogens wake anthrax lab mishaps',\n",
       " 'Hmm problem researchers FedEx longer transport select agents usatoday',\n",
       " 'world FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer ship potential bioterror pathogens FedEx Corp NYSE FDX longer deliver packages',\n",
       " 'FedEx longer ship potential bioterror pathogens Atlanta Business Chronicle',\n",
       " 'Thank FedEx longer shipping live microbes Department Defense',\n",
       " 'NowPlaying orchardalley LES nyc bioterror manufactured fear state repression abcnorio gardens',\n",
       " 'FedEx longer transport bioterror germs usatoday',\n",
       " 'JAX Biz Journal FedEx stops shipping potential bioterror pathogens',\n",
       " \"USATODAY today's frontpage Bioterror lab faced secret sanctions RickPerry doesn't make cut FoxNew\",\n",
       " 'BreakingNews FedEx longer willing transport research specimens potential bioter',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps usatoday',\n",
       " 'FedEx stops shipping potential bioterror pathogens trucking',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " 'FedEx longer shipping bioterror germs WXIA-TV scoopit',\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps',\n",
       " \"USATODAY today's frontpage Bioterror lab faced secret sanctions RickPerry doesn't make cut FoxNew\",\n",
       " 'FedEx longer transport bioterror germs wake anthrax lab mishaps USATODAY',\n",
       " 'Biolab safety concerns grow FedEx stops transporting certain specimens Research facilities dumbfounded action',\n",
       " 'fight bioterrorism sir',\n",
       " 'liked YouTube video FEMA REGION III TARGETED BIOTERRORISM NASA JAPAN ROCKET LAUNCH LITHIUM',\n",
       " 'anthrax bioterrorism CDC Carry Extensive Review Lab Safety Pathogen Handling Procedures',\n",
       " 'Firepower lab electronic resource automation fight infectious diseases bioterrorism',\n",
       " 'CAgov BLKs amp WHTs colluded WHT F USAgov AUTH Hostage amp make look BLK w Bioterrorism amp use lgl org IDis ID VP',\n",
       " 'DarrellIssa Does great Iran deal cover bioterrorism got cut terrible good work',\n",
       " 'Tale Pox Body Horrors virus infectiousdiseases bioterrorism',\n",
       " 'Bioterrorism public health superbug biolabs epidemics biosurveillance outbreaks Homeland Security News Wire',\n",
       " \"Creation AI Climate change Bioterrorism Mass automation workforce Contact life Wealth inequality Yea we've got easy\",\n",
       " 'Lies Trust dvd CIA Hollywood Bioterrorism Len Horowitz Vaccines Nwo',\n",
       " \"O Magazine satan's daughter shadow warrior ft women aka transgender mode ps nyc fold extra extra center bioterrorism\",\n",
       " \"DrRichardBesser Yes think college - it's difficult think bioterrorism esp bc dispersed\",\n",
       " 'Volunteers needed participate Emergency Preparedness drill simulating bioterrorism disaster HVnewsnetwork',\n",
       " 'MeyerBjoern thelonevirologi MackayIM major American newspaper running series alleged bioterrorism research going n',\n",
       " 'fight bioterrorism sir',\n",
       " 'CDC pretty cool list bioterrorism agents',\n",
       " 'Government Experts Concerned Possible Bioterrorism Using GM Organisms Scientists concerned',\n",
       " 'Does Prepare HHS Selects Regional Special Pathogen Treatment Centers Bioterrorism Infectious Ebola',\n",
       " 'StationCDRKelly Support Sys USAgov AUTH taken Hostage BLK clergyforced exist younger amp grossly disfigured BIOTERRORISM AP',\n",
       " \"won amp think possibility transformation impossible don't quite like medical mysteries BIOTERRORISM sucks\",\n",
       " 'fight bioterrorism sir',\n",
       " 'book describing future therapies technologies sport sexuality bioterrorism diagnosis digitalhealth hcsm',\n",
       " \"APhiABeta w ugliness ugly AMESocialAction Frat's BIOTERRORISMI'm who's FBI ID U tolewant'g Home ABC\",\n",
       " 'HowardU BLKs amp WHTs colluded WHT F USAgov AUTH Hostage amp make look BLK w Bioterrorism amp use lgl org IDis ID',\n",
       " 'fight bioterrorism sir',\n",
       " 'cspanwj BLKs amp WHTs colluded WHT F USAgov AUTH Hostage amp make look BLK w Bioterrorism amp use lgl org IDis ID',\n",
       " \"ONU France Bioterrorism Rockefeller Chi RockefellerUniv'Heiress evade lgl efforts prosecute BLKs HarvardU kidnap'g AFP\",\n",
       " 'IranDeal covers nuclear activity doing Bioterrorism Iran broken agreements',\n",
       " 'Threat Anthrax CDC CDCgov',\n",
       " 'bioterrorism Authorities allay glanders fears ahead Rio Olympic equestrian test event HorsetalkNZ',\n",
       " 'Kaotix Blaze craving u',\n",
       " \"I've pool day raisinfingers\",\n",
       " 'know looking Hammond Share listing',\n",
       " 'Life amazin time crazy niggas dey wanna blaze hate took dedication n motivation',\n",
       " 'Blaze bro',\n",
       " 'Shayoly yes love',\n",
       " 'virtual tour listings Fir St Cannon Beach listed Dorrie Caruana',\n",
       " 'Pendleton media office said base right Horno blaze',\n",
       " 'Welcome djryanwolf djcoreygrand djknyce djoneplustwo OfficialCoreDJs Family Cleveland StandUp IAMTONYNEAL',\n",
       " 'Love living blaze inside apt balcony',\n",
       " \"bellalinn alrighty Hit we'll blaze\",\n",
       " 'blaze jays fuck dutch slave trade',\n",
       " 'hair poverty moment need fade weekend gets',\n",
       " 'Property losses California wildfire nearly double week-old blaze rages',\n",
       " 'breaking Firefighters battling blaze east Cary condo building',\n",
       " 'niggas love hating',\n",
       " \"audacityjamesta Don't like babes lt you'll ball fun LAN\",\n",
       " 'liked YouTube video iamrrsb Minecraft Skywars O BLAZE QUE USA HACK E FLECHADAS SINISTRAS',\n",
       " 'Dems Blaze covered months ago Chicago police detained thousands black Americans interrogation facility',\n",
       " \"Yo got bars I'm rapper\",\n",
       " 'UGH Y DID BLAZE CALORIES PIZZAS OK COOL thisispublichealth',\n",
       " 'pic blaze fort kids look like jackass stuffin face like',\n",
       " 'looks like year writing computers ahead',\n",
       " 'Beautiful Juic just letting know',\n",
       " \"BabySweet I'm mad amp don't blaze\",\n",
       " 'GuiltyGearXXACP yeah know blaze blue dont twitter lol drew weeks ago',\n",
       " 'socialmedia news New Facebook Page Features Seek Help Personalize Customer Experience',\n",
       " 'DJJOHNBLazE shout blaze hottest DJ Sothwest',\n",
       " 'liked YouTube video Minecraft Episode Blaze Farm Beginnings',\n",
       " 'Property losses California wildfire nearly double week-old blaze rages fire',\n",
       " 'ChristyCroley works Did new Vela Short Blaze',\n",
       " 'UABStephenLong courtlizcamp Total tweet fail beautiful inside Blaze',\n",
       " 'mixtape coming promise goin right',\n",
       " 'itzSteven xdojjjj whopper jr huh leo started year people blaze',\n",
       " 'Bedrooms Baths sale Palms CA YouTube Video',\n",
       " 'Yes guns',\n",
       " 'cee DAEM GIRL SMOOTH ASF c',\n",
       " 'ArtisteOfTheWeekFact say Conversations coast coastdjs agree Crystal Blaz s Jiwonle HipHop ClubBanger',\n",
       " 'Bright amp BLAZING Fireman Birthday Party Weddings',\n",
       " 'REAL ViBEZ RADIO BLAZING BEST VIBEZ nowplaying listenlive',\n",
       " 'Montgomery come blazing hot weather stay STDs rejected city slogan',\n",
       " 'Come join Tomorrow August Transcend Blazing Trail Diversified World Marketing',\n",
       " 'Morgan Silver Dollar S Gem BU DMPL Cameo Rev Blazing MS High grade read',\n",
       " 'Morgan Silver Dollar S Gem BU DMPL Cameo Rev Blazing MS High grade read',\n",
       " \"bowl got thinking Damn I've blazing damn long\",\n",
       " \"DanRyckert drewscanlon He's blazing game best stealth skills beats silenced M\",\n",
       " 'Bit pacquiao vs marquez unfilled blazing swarm online DuRvOd',\n",
       " 'Turn radios stoponesounds live airwaves amp fm StickyNYC roots blazing hits',\n",
       " 'BaseballQuotes inch dynasty',\n",
       " \"I'm weapons master Let's guns blazing Hinatobot\",\n",
       " 'Blazing Ben PattyDs gwfrazee JoshuaAssaraf really Sadly come expect Obama',\n",
       " 'SHOUOUT kasad lla CAUSE VOCALS BLAZING HOT LIKE WEATHER SHES',\n",
       " 'S XLEAK Ph tos yrs old Ash wo lady Festac town Delta exp sed BBM leaked pictures',\n",
       " 'Oh heart racing temperature blazing roof VideoVeranoMTV Fifth Harmony',\n",
       " 'omgbethersss BethanyMota haha love',\n",
       " 'dmac Colorado Spanish word Latin origin meaning reddish colored dummies pronouncing wrong',\n",
       " 'Traffic Freezing Cold Blazing Hot Heat Traffic',\n",
       " \"I'm crazy run degree mid-day heat blazing sun place I'm not\",\n",
       " \"I'm blazing rn there's stop\",\n",
       " 'blazing',\n",
       " 'bekah w thanks sweat bullets time blazing sun beating',\n",
       " 'ACOUSTICMALOLEY blazing',\n",
       " 'OfficialTJonez Lost Words new fan fam Crazy skills blessed blazing dude love respect',\n",
       " 'Let Hot Blazing Fantasy escorts gfe DUBAI',\n",
       " 'srajapakse thank missy thought suited blazing hot summertime yee-haw',\n",
       " 'asukager magical bag blazing',\n",
       " \"Blazing Elwoods BlazingElwoods Don't Bother Doug's Song Tune\",\n",
       " 'Morgan Silver Dollar P CH Gem Bu PL Blazing MS Satin Rare Proof Like re',\n",
       " 'Blazing Hot Etisalat Free MB Complete Months Etisalat Giving MB TECNO Q Ime',\n",
       " 'FunkyLilShack mariaf want bitch slapping guns blazing cake throwing Charles showdown worth wait',\n",
       " 'Follow EdWelchMusic check Hit Single Unpacked Man BLAZING',\n",
       " \"don't know hours physical activity blazing sun isn't sport\",\n",
       " 'Ways archetype bleeding well-grounded readiness FpOJ',\n",
       " \"Threw chicken nugget sisters lip it's bleeding\",\n",
       " \"slit throat I'd apologize bleeding\",\n",
       " 'Joe Landolina gel make stop bleeding instantly arizona realestate',\n",
       " 'nose bleeding like years ago',\n",
       " \"bleeding typewriter day far I've written bunch gunk\",\n",
       " \"JaydenNotJared can't help Hope you're ok Text need talk Sending hugs way PS bleeding death allowed\",\n",
       " \"stab promise you'll bleeding\",\n",
       " \"Apparently you're bleeding people look weird lol it's fine walking\",\n",
       " 'Eating takis rubbing eyes hands eyes bleeding tears',\n",
       " 'DarrylB yea heard coming Vampiro bleeding',\n",
       " 'CoreyAshe Did look broken bleeding',\n",
       " 'hit foot toe bleeding',\n",
       " \"King Naruto long Madara bleeding I'm good\",\n",
       " \"thinking stepped broken glass pun tak sedar don't feel pain it's bleeding Shit\",\n",
       " 'ears bleeding',\n",
       " 'waited hours cab feet bleeding',\n",
       " \"It's cute dinner date Til cams nose starts bleeding\",\n",
       " 'Uptown Jorge head like yo nose bleeding',\n",
       " \"I've bleeding silence feel safer violence\",\n",
       " \"Jannet fell someone's hit head concrete bleeding n shit\",\n",
       " \"KatRamsland Yes I'm bleeding heart liberal\",\n",
       " \"Deadpool favourite marvel characters know wears red suit bad guys can't tell he's bleeding\",\n",
       " 'beckyfeigin defs stops bleeding',\n",
       " \"SoDamnTrue know u you're bleeding heart wannabe pickup artist\",\n",
       " 'chaosmagician awesome saw bleeding pretty bad',\n",
       " \"burberryant bleeding brain don't know cause\",\n",
       " \"lets good soccer you're bleeding yo face\",\n",
       " 'tammy w ElijahMallari bleeding wild things running apartment hes work bar',\n",
       " 'ColoicCarnality bleeding instincts kicked looks away scratches head',\n",
       " 'ears bleeding hate stefano',\n",
       " 'ear started bleeding',\n",
       " 'Benjm TourofUtah B Grego saw pileup TV racing bleeding',\n",
       " 'rave wedding seeing eyes bleeding',\n",
       " 'say bad things happen reason wise words gonna stop te bleeding',\n",
       " 'DamnAarielle yo timeline blew damn fast',\n",
       " 'mfalcon look Just blew w atomic bomb',\n",
       " \"blew oomf instagrams cause she's cute she's active follower\",\n",
       " 'b fowler Crazy line blew',\n",
       " 'Instagram just blew apparently featured jazz tonight cool love',\n",
       " \"i'd team usagi blew entire solar airhead misstep\",\n",
       " \"BenKin Mili remember u like - blew game U probs don't kings won cup\",\n",
       " \"hate people tweet receipts KNOW wrong wont bc blew literally gtfo you're desperate\",\n",
       " \"think just blew HopeInHearts notifications check she's encouraging love\",\n",
       " 'Hw going - Dude keeping Took exit Pulled told blew motor Lolol fast furious',\n",
       " 'universe actually exist scientists say SUN blew Earth began',\n",
       " 'Max blew tf shots fired CatfishMTV',\n",
       " 'Rick Morty Blew',\n",
       " 'CodyThompson ty just blew motor went flames got ok',\n",
       " 'Just realized dude OnlyFTF way blew tusky game robsimss CantMissKid',\n",
       " 'night just blew rq',\n",
       " 'blew snapchat reason',\n",
       " \"Queens Gambit went Anakin blew droid control ship Oh fun Can't Stop WBC\",\n",
       " 'iphooey TIME Ironically Michele Bachmann brought w Ron Paul amp blew called hoax finally right',\n",
       " 'Catfish retweeted amp notifications blew',\n",
       " 'guy whistled parking lot amp did help wind blew skirt getting car',\n",
       " 'Blew mentions',\n",
       " 'st time blew phone times blocked Believe Catfish',\n",
       " 'ImAwesome like literally blew',\n",
       " 'FREYAS VIDEO BLEW',\n",
       " 'YahooSchwab easy way look good Ray Rice fiasco blew',\n",
       " 'Lmao light skin guy blew Twitter talking ugly kid',\n",
       " 'watched Honey Blew Economy Recommend Conservative Shoppe Horrors HarperANetflixShow elxn stopharper',\n",
       " 'Did fireball falling earth Look like plane blew',\n",
       " 'Ye did thing Big Sean blew',\n",
       " 'Zayn just blew twitter',\n",
       " 'Bitch blew shit',\n",
       " \"WeLoveRobDyrdek adrian peel damn clue Tried clicks blew That's played\",\n",
       " 'Daorcey nsit great pair Like couple Graywardens fighting blight',\n",
       " 'DaMidnighter theres actually theory magisters arent reason blight dwarves ones',\n",
       " 'cycling fan feel sorry world athletics doping blight exacerbated monetary reward lot soul searching required',\n",
       " 'kynespeace blight',\n",
       " 'Poor Jack',\n",
       " \"realhotcullen agree knew we'd going deep roads Blight red lyrium ain't gt gt\",\n",
       " 'load welfare loving sponge blight society',\n",
       " 'New post Prysmian secures contract Blight Bank wind farm',\n",
       " 'Cleveland Heights Shaker Heights fight blight House Door',\n",
       " 'todd calfee mattburgener wanted info blight u got',\n",
       " 'Carl Everest Rob Cobes Whitt Blight Frost Leo Snuff Godly drink beer Someday',\n",
       " 'THDA Kicks Anti-Blight Loan Effort Memphis',\n",
       " 'City program help turn blight greenspace Tennessee Housing Development news',\n",
       " 'Demetae yes want new blight leader',\n",
       " \"Releases planing level Constellation Blight Gaia iClown's Drumstep Remix iClown Infinity\",\n",
       " 'Cleveland Heights Shaker Heights fight blight House Door',\n",
       " \"'If going achieve excellence big things develop habit little matters dont know author\",\n",
       " 'Hendy sure purdies alive blight',\n",
       " 'Tracy Blight Thank following',\n",
       " 'Apperception bridgework blight XxhJeSC',\n",
       " \"dotish blight car right ahead it's\",\n",
       " 'Sexual Revolution Blight Women Stories ACeBabes HealthWeekly AmateurNester',\n",
       " \"LIKE SWEAR SECRET WE'LL UNCOVER OLD GODS SLUMBER THINK THERES GONNA BLIGHT\",\n",
       " 'WillHillBet double result live app',\n",
       " 'parksboardfacts ZippoLine wants use community asked blight park moveit',\n",
       " 'Look Policy Matters Ohio report CLE Cuyahoga County blight greening vacant lands soon',\n",
       " 'anellatulip taint magisters did open gates let blight away',\n",
       " 'anellatulip theory makes way sense says dwarves actual origin blight',\n",
       " 'Palestinian refugee tragedy blight humanity amp shame Israeli living',\n",
       " 'Article Michael Jackman Metro Times Detroit group later downgraded estimate square miles',\n",
       " 'Locksmithing-art respecting elaboration blight locks lPDkl',\n",
       " 'jake blight WeAlIlKnowA cunt',\n",
       " 'wins ranked play gave special card cool long te',\n",
       " 'Amazon Deal wait buy',\n",
       " 'New print available Waiting Long Pamela Blizzard',\n",
       " 'Blizzard draco FREE ART KAMON',\n",
       " 'StevenOnTwatter PussyxDestroyer just order blizzard pay nuts say ball flavored Boom free ice cream',\n",
       " 'Blizzard Auz pm CST RadioRiffRocks hrs Rock make hump day complete',\n",
       " 'really wants rolo blizzard mom said guess DQ tonight',\n",
       " 'Ashayo MsMiggi Hi Ashayo believe VODs YouTube presentation like seeing live',\n",
       " 'Stats',\n",
       " 'Tweet Taiji dolphin worship group based superstitions Just look tweets',\n",
       " \"blizzard fans Lucio Let's overwatch hype train rolling Caution aren't breaks\",\n",
       " 'Blizzard Gamin ight',\n",
       " 'peanut butter cookie dough blizzard',\n",
       " \"mic controllers aren't working second\",\n",
       " \"Tomorrow's Announcement VODs\",\n",
       " 'Updated Windows error',\n",
       " 'New Expansion Ideas Bard Class Holy Trinity',\n",
       " 'DaBorsch really shocking blizzard lured old fanbase WoD disappointed hardcore everyones leaving',\n",
       " 'best thing DQ cotton candy blizzard',\n",
       " 'Lizard Wizard Blizzard LWB',\n",
       " 'Blizzard draco LoneWolffur need',\n",
       " 'FAIRx x PlayOverwatch BlizzardCS blizzard love',\n",
       " 'blizzard clutch asf',\n",
       " 'Blizzard draco LoneWolffur like link',\n",
       " 'BubblyCuteOne ok ok okayyyyyy Ima act right bout blizzard tho',\n",
       " 'LoneWolffur control tora',\n",
       " 'Time Playing Hearthstone PC Thoughts',\n",
       " 'love cotton candy blizzard',\n",
       " 'TCGReno just hard reset Xbox',\n",
       " 'really wanna brownie batter blizzard',\n",
       " 'little bit blizzard',\n",
       " 'Market News Activision Blizzard Cognizant Technology Solar',\n",
       " 'SEAN END CAREER sG Blizzard vs KNOCKOUT',\n",
       " 'biggest regret hearthstone',\n",
       " 'walk DQ wanna Butterfinger Blizzard bad',\n",
       " 'horrible moment u open dryer looks like snowy blizzard cuz u left piece paper jeans pocket',\n",
       " 'LoneWolffur BRUH dies',\n",
       " 'Rip Blood',\n",
       " \"Chief CG nah young blood cook gone I'm cut haha\",\n",
       " 'Wall Invincible HIStory Blood Dance Floor',\n",
       " 'Broke nail real fake morning blood ah hurts ideas treat Help pretty',\n",
       " 'stars reviewers Dragon Blood Boxset Lindsay Buroker kindle',\n",
       " 'Blood Group ve associated Gastric Carcinoma says text book fragile gene body',\n",
       " 'friend like blood come wounded',\n",
       " 'gone blood',\n",
       " 'people tattoo u allowed donate blood receive blood',\n",
       " 'Doing dialyses grandpa oh lord blood makes light headed',\n",
       " 'SetZorah dad dont claim mean right look eyes blood xbox SMH',\n",
       " 'Omron HEM- C Automatic Blood Pressure Monitor STANDARD LARGE BP CUFFS',\n",
       " \"blood pressure roof don't need extra shit\",\n",
       " 'Chambered Blood Yeah SpeakingFromExperience',\n",
       " \"Can't believe people mid 's don't high blood pressure Life stressful DecisionsOnDecisions\",\n",
       " \"Guys Imouto Isn't Actually Related Blood\",\n",
       " 'Bruh white people buy ugliest shoes super tight blood going feet',\n",
       " 'day excellent dangerousbeans porridge seriously people blood orange porridge phenomenal',\n",
       " 'scotto happy birthday young blood',\n",
       " \"wasn't Blood\",\n",
       " 'kinda shii nasty blood pun intended',\n",
       " \"Ain't hoe blood\",\n",
       " 'Today hastle getting drug tested blood drown tb shot document filling',\n",
       " 'Man somebody gotta stop Sbee dude fuckin funny blood',\n",
       " 'winds gypsy blood time',\n",
       " 'Lobo paranoico Mad Men',\n",
       " \"Private thirsty night SAD BLOOD ROCK'N ROLL\",\n",
       " 'RolandoNaBeats Ellie Goulding Blood Acesse nosso site para ouvir',\n",
       " \"olrules Welcome Read free chapter new book Encounters Jesus It's hope\",\n",
       " 'Anthxvy runs blood',\n",
       " 'sethalphaeus personal favorites include paramore muse green day royal blood sos',\n",
       " 'Shed innocent blood sons daughters land polluted Psalms Help stop sin abortion',\n",
       " 'se pone cantar crying lightning',\n",
       " 'wasnt big stab deep stab theres like blood everwhe',\n",
       " 'Add items everyday eating habits research blood',\n",
       " 'LOOOOOOOOOOOOL bloody hell did',\n",
       " 'infected bloody ear piercings fun',\n",
       " 'aggressif bloody aggressive',\n",
       " 'entered win ENTIRE set butterLONDON Lip Crayons ram s enter bbloggers',\n",
       " 'slsandpet Hey Sally sorry emailed AWOL bloody work ARGH ResignInShame',\n",
       " \"I'm listening Bloody Jay\",\n",
       " \"LauradeHolanda Forrest version that's bloody awful xxx\",\n",
       " \"'A Nightmare Elm Street Getting Remade\",\n",
       " \"can't bloody wait Sony Sets Date Stephen King s The Dark Tower stephenking thedarktower bdisgusting\",\n",
       " 'Gotta try let bloody things Smh',\n",
       " 'TradCatKnight Russia played reason link BS Okanowa bloody mainline invasion looked like bloody',\n",
       " 'Bloody insomnia Grrrr Insomnia',\n",
       " 'zhenghxn tried eyes akame ga kill tokyo ghoul damn bloody dont dare watch',\n",
       " \"Fantosex suck that's you're bloody getting means amends\",\n",
       " \"weekends Bloody Mary times summer's new\",\n",
       " \"Bloody hell day haven't really Just Tired Thought vaca help did\",\n",
       " 'Damn bloody hot',\n",
       " \"MrTophyPup it's bloody sexy drools\",\n",
       " 'know say effects low amp really fast Son product acne cream effects bloody diarrhea',\n",
       " \"Ronda Rousey close making Floyd Mayweather's money fights Bloody Elbow boxing\",\n",
       " \"I'm awful painting did agree landscape bloody oils paints\",\n",
       " 'need life sin just girlfriend ride till bloody end Just girlfriend',\n",
       " \"itsmegss think it's bloody barking\",\n",
       " 'Eh hello cover bloody thighs bloody cleav Eh hello expose cleavage',\n",
       " 'MariaSherwood JohnJCampbell Mega bloody marvellous',\n",
       " 'Bloody Mary sink Beet juice',\n",
       " 'MelRises gayler wwwbigbaldhead jessienojoke melissaross Monty Python date bloody far wants',\n",
       " 'Meet bloody RS',\n",
       " \"Friday supposed happy day it's bloody friday hah zzzz\",\n",
       " 'chxrmingprince jones luna bloody hope said folding arms sitting chair',\n",
       " 'Wait tell college friend reafs bloody mary drama cd',\n",
       " \"'I came kill Indians FUN Video smirking remorseless Pakistani killer shows boasting\",\n",
       " \"I've just watched episode S E Bloody Monday bloodymonday tvshowtime\",\n",
       " 'Marlon Williams gt Elvis Presley gt Marlon Williams gt Steel Panther Shuffle mode like bloody legend',\n",
       " 'Black Friday turns Bloody shopping mystery',\n",
       " \"ezralevant told soccer moms getting pic blown daughter's bedroom walls\",\n",
       " 'Papcrdoll s g mentions blown choice deactivate leave good RESPECT CHOICES',\n",
       " \"ManUtd EmilymcfcHeslop Wow just wow club blown money fucked player like they'd pilloried\",\n",
       " 'Damn greinke got blown inning',\n",
       " 'Woke blown lol',\n",
       " \"aunt marge you're blown eheks\",\n",
       " 'troylercraft YEAH WORTH BC SPAMMERS amp TWITTER PROBABLY BLOWN SECOND',\n",
       " \"White family supposedly representing America's GREAT values gets blown horrible CGI nuclear strike LMFAOOOO\",\n",
       " \"Man hasn't machinegunkelly blown He's underground\",\n",
       " 'mentions blown wtf',\n",
       " 'ThisDayInHistory Confederate ship blown crew Read History',\n",
       " 'moment ur win mini uhc blown creeper kicked flying salty rn',\n",
       " \"libraryeliza did taylorswift bump approval probably he's blown MusicAdvisory\",\n",
       " \"'If truckload soldiers blown panics little lion dies loses mind'\",\n",
       " \"HEY LOOK Kash's Foundation Live Today got blown People Magazine's website Todd Blake\",\n",
       " \"KaylaK got month went EE shop Glad hasn't blown\",\n",
       " 'time tweet blown half day later',\n",
       " \"'We re blown away extension we ve seen options\",\n",
       " 'TheBoyOfMasks Thanks letting stay manor blown Anyways doing buddy',\n",
       " 'LONER DIARIES patterns sand blown away photos twos choked flames',\n",
       " 'BSG sufficiently hyped years delayed watching utterly utterly blown away',\n",
       " \"Vanessa's game officially blown LADIES GENTLEMEN real begin BB\",\n",
       " 'Guaranteed bitten mutant mosquito ankle blown Little cunts',\n",
       " 'Things pick tozlet seat butt leprosy full-blown jerkface syndrome lateral lisp amp toilet rickets',\n",
       " 'KalinAndMyles KalinWhite ig blown just hackers need stop givebackkalinwhiteaccount',\n",
       " 'luke winkie directing videos needs grab Nicki Minaj U S recognition minds blown',\n",
       " \"PrincessDuck week wanted th sense blown far good James win he's huge target gone soon\",\n",
       " 'orbette like BLOWN amirite',\n",
       " 'dog s just blown kennel Bloody Yorkshire Terrorist',\n",
       " 'Turn ESPN blown',\n",
       " 'HopefulBatgirl went beaten blown thing know Ra Al Ghul brought life escaped a---',\n",
       " 'thebriankrause leos ass just got metaphorically blown PiperWearsThePants charmed',\n",
       " 'anarchic teapot BoironUSA zeno Glononium C helps blown bashing bottle nitroglycerin book',\n",
       " 'New Ladies Shoulder Tote Handbag Women Cross Body Bag Faux Leather Fashion Purse re',\n",
       " 'New Ladies Shoulder Tote Handbag Women Cross Body Bag Faux Leather Fashion Purse re',\n",
       " 'son daughter like going war Iran come body bag Let Republicans know',\n",
       " 'questergirl ditto way feel drank vodka ice body bag',\n",
       " 'handbags Genuine Mulberry Antony Cross Body Messenger Bag Dark Oak Soft Buffalo Leather End Date W',\n",
       " 'Louis Vuitton Monogram Sophie Limited Edition Clutch Cross body Bag read eBay',\n",
       " 'Louis Vuitton Monogram Sophie Limited Edition Clutch Cross body Bag read eBay',\n",
       " 'Louis Vuitton CultSierre Monogram Shoulder Bag Cross Body Bag',\n",
       " 'Check Ameribag Healthy Bag Shoulder Cross Body Backpack Khaki Tan Beige Nylon eBay',\n",
       " 'Photo Bath amp Body Works cosmetic bag periwinkle blue copper piping corners',\n",
       " 'New Ladies Shoulder Tote Handbag Women Cross Body Bag Faux Leather Fashion Purse re',\n",
       " 'new summer long body bag hip word skirt Blue',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens RT en',\n",
       " \"TR jdavis Bruh wanna fight I'm meet cage bro better you're dealing end body bag\",\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens read',\n",
       " 'AUTH LOUIS VUITTON BROWN SAUMUR CROSS BODY SHOULDER BAG MONOGRAM - read',\n",
       " 'Check Vintage Longaberger Floral Fabric Shoulder Cross Body Bag Brown Leather Strap eBay',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens',\n",
       " 'new summer long body bag hip word skirt Blue',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens RT en',\n",
       " 'Louis Vuitton Monogram Sophie Limited Edition Clutch Cross body Bag read eBay',\n",
       " 'New Women Handbag Faux Leather Ladies Shoulder Tote Cross Body Bag Large Satchel re',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens',\n",
       " 'day heart gone zipped body bag',\n",
       " 'handbag fashion style Authentic Louis Vuitton Pochette Bosphore Shoulder Cross Body Bag',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens',\n",
       " 'handbag fashion style Vintage Coach Purse Camera Bag Cross Body Bids',\n",
       " 'genuine Leather man Bag Messenger fit iPad mini tablet case cross body air jp rea',\n",
       " 'Louis Vuitton Monogram Sophie Limited Edition Clutch Cross body Bag read eBay',\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens',\n",
       " \"Nuu FAM fwt I'm Leave Body bag\",\n",
       " 'New Ladies Shoulder Tote Handbag Faux Leather Hobo Purse Cross Body Bag Womens read',\n",
       " \"Rhee deliciousvomit I'm I'm saying they're lucky home families body bag\",\n",
       " \"matt bez oh I'm bagging body bangin I'm saying she's going rose\",\n",
       " \"ohmyloz RondaRousey bagging body She's smoking hot\",\n",
       " 'Drake really body bagging meek',\n",
       " 'fuckyeahcarey BornVerified drake killing dude tea bagging dead body point',\n",
       " 'MeekMill w rec k league ball Drake Olympic level body bagging like career trollingtilMeekdiss',\n",
       " 'Idgaf tough Canada north Philly meek acting like bitch amp drake body bagging ass tracks',\n",
       " 'Yankees body bagging mfs',\n",
       " 'BroseidonRex dapurplesharpie skimmed twitter missed body bagging',\n",
       " 'G Body Bagging lyrically',\n",
       " 'bomairinge EluTranscendent straight body bagging',\n",
       " 'Body Bagging Bitches',\n",
       " 'better feeling seeing stage day ones year friendships body bagging mics',\n",
       " 'Mopheme Bigstar Johnson problem game body bagging niggas VuzuHustle',\n",
       " 'body Bagging Today got post score',\n",
       " \"editaxohaze let bagging body's begin lol ain't cuffed shouldn't bad\",\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = df_train['text'].tolist()+df_test['text'].tolist()\n",
    "\n",
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1cd67",
   "metadata": {
    "papermill": {
     "duration": 0.026607,
     "end_time": "2024-06-02T10:22:23.992366",
     "exception": false,
     "start_time": "2024-06-02T10:22:23.965759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here, we don't know how the number of words is, so we just set a substantially large number, then adjust it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64519fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:24.047103Z",
     "iopub.status.busy": "2024-06-02T10:22:24.046803Z",
     "iopub.status.idle": "2024-06-02T10:22:24.582820Z",
     "shell.execute_reply": "2024-06-02T10:22:24.581896Z"
    },
    "papermill": {
     "duration": 0.567622,
     "end_time": "2024-06-02T10:22:24.586658",
     "exception": false,
     "start_time": "2024-06-02T10:22:24.019036",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5458, 661, 135, 1582, 3503],\n",
       " [71, 97, 689, 8061, 8062, 1223],\n",
       " [1378, 1293, 1873, 575, 8063, 1583, 150, 1873, 575, 1474, 1055],\n",
       " [8, 4222, 1164, 150, 1474, 30],\n",
       " [3, 23, 1294, 164, 5459, 2029, 158, 1164, 8064, 98],\n",
       " [2937, 265, 30, 1475, 709, 8065, 1056, 277, 5460, 1164],\n",
       " [94, 13, 710, 165, 912, 711, 174, 1584, 8066, 913, 2548, 1379],\n",
       " [4, 1874, 4223],\n",
       " [350, 10, 150, 1057, 441, 520],\n",
       " [4, 2030, 409, 136, 192],\n",
       " [8, 396, 142, 266, 442],\n",
       " [739,\n",
       "  423,\n",
       "  2549,\n",
       "  121,\n",
       "  2550,\n",
       "  4224,\n",
       "  529,\n",
       "  511,\n",
       "  87,\n",
       "  423,\n",
       "  2549,\n",
       "  151,\n",
       "  151,\n",
       "  8067,\n",
       "  174],\n",
       " [3504, 174, 1585, 8068, 2549, 456, 206, 612, 3505],\n",
       " [94, 5461, 836, 1380, 5461],\n",
       " [126, 98, 351, 2938, 46, 22, 212],\n",
       " [592, 29],\n",
       " [35, 5462],\n",
       " [137, 1476],\n",
       " [46, 662],\n",
       " [8069],\n",
       " [3506],\n",
       " [870, 478],\n",
       " [35, 8070],\n",
       " [2268, 24],\n",
       " [8071],\n",
       " [58, 54, 1058, 105],\n",
       " [1224, 222],\n",
       " [35, 2551],\n",
       " [8072],\n",
       " [2, 4225],\n",
       " [243],\n",
       " [8073, 2552, 1875, 443],\n",
       " [530, 690, 710, 1059, 19],\n",
       " [8074, 212, 7, 1721, 663, 159, 443, 914],\n",
       " [1477, 159, 443],\n",
       " [1876, 84, 1000, 100, 443],\n",
       " [8075, 5463, 2553, 1722, 3507, 5, 5464, 2554, 159, 8076, 443, 801],\n",
       " [5465, 1106, 5466, 159, 443],\n",
       " [8077, 8078, 2939, 740, 159, 443, 2555, 1723, 223, 152, 5467, 14, 8079],\n",
       " [443, 712, 145],\n",
       " [175, 2556],\n",
       " [593, 90, 443, 1001, 90, 40, 556],\n",
       " [837, 16, 5468, 5469, 223, 1106, 8080, 496, 443, 465, 5470, 5471, 777, 497],\n",
       " [5472, 8081, 443, 8082],\n",
       " [838, 159, 664, 443, 5473, 1586],\n",
       " [2557, 2031, 222, 26, 4226, 1877],\n",
       " [479, 224, 741, 1164, 443, 30],\n",
       " [441, 1381, 8083, 43, 630, 1584, 443],\n",
       " [175, 2556],\n",
       " [100, 8084, 9, 1478, 1382, 327, 444, 2558, 1060, 100, 39],\n",
       " [3508, 29, 576, 4227, 68, 159, 443],\n",
       " [29, 871, 61, 2940, 1165, 443, 4228],\n",
       " [2555, 1723, 223, 152, 5467, 14, 8085, 8086, 4229, 235],\n",
       " [14, 594, 8087, 159, 76, 1724, 531, 4230, 443],\n",
       " [8088, 1061, 8089, 5474, 5475, 557, 8090, 1002, 713, 8091, 8092],\n",
       " [8093, 8094, 5476, 663, 1062, 159, 443, 5477, 4231, 5476, 8095],\n",
       " [122, 443, 235, 8096, 915, 593, 8097, 5478, 3509, 2941],\n",
       " [159, 2559, 443, 99, 2560, 5479, 2, 3510, 8098],\n",
       " [1000, 443, 236, 2561, 2562, 4, 3511, 2942, 2563, 3512, 2032, 966, 26, 4232],\n",
       " [479, 224, 741, 1164, 443, 30, 714, 1725],\n",
       " [5480, 5481, 1726, 967, 1727, 8099, 443, 5481, 1587, 1295, 2269, 8100],\n",
       " [8101, 8102, 968, 1166, 159, 5482, 443, 2943, 8103],\n",
       " [5483, 443, 1, 8104, 969, 577, 2944, 3513, 4233, 5484],\n",
       " [8105, 1479, 27, 30, 8106, 8107, 1165, 5485, 443, 7],\n",
       " [641, 5483, 443, 1, 2033, 8108, 2945, 1225],\n",
       " [8109, 613, 2552, 1875, 443],\n",
       " [443, 16, 160, 665, 11, 26, 52, 75],\n",
       " [1226, 54, 424, 2034, 23, 5486, 51, 1, 3514, 8110, 9, 2946, 424, 742, 1878],\n",
       " [51, 352, 33, 8111, 532, 1296, 47, 3515, 2947],\n",
       " [51, 642, 1728, 1167, 8112, 8113, 4234, 101, 512, 5487, 8114, 532],\n",
       " [8115, 51, 137, 85, 2948, 1, 5488, 79, 8116],\n",
       " [8117, 8118, 8119, 912, 1107, 1879, 46, 51, 5489],\n",
       " [916, 2949, 715, 51, 8120, 8121, 480, 97, 8122, 1063, 5490, 715, 295],\n",
       " [8123, 87, 51, 5491],\n",
       " [352, 1383, 3516, 423, 5492, 5493, 715, 51, 8124, 709, 38],\n",
       " [19, 8125, 872, 8126, 873, 917, 46, 51],\n",
       " [5494, 51, 1384, 3517, 839],\n",
       " [532, 51, 252, 8127, 1475, 8128, 2950, 8129],\n",
       " [352, 1383, 3516, 423, 5492, 5493, 715, 51, 8130, 38],\n",
       " [8131, 840, 51, 1227, 3518, 5495],\n",
       " [614, 558, 68, 662, 8132, 614, 51, 122, 8133, 8134],\n",
       " [353, 46, 51, 244, 1064, 4, 578, 1480, 521, 88],\n",
       " [529, 2951, 5496, 466, 51],\n",
       " [8135, 8136, 8137, 5497, 1880, 1481],\n",
       " [51, 8138, 2035, 1482, 532],\n",
       " [4230, 51, 1483, 918, 352, 22, 46, 2564, 8139, 2565, 2952, 5498],\n",
       " [2566, 4235, 2566, 51, 615, 126, 3519, 5499, 480, 5500, 1003],\n",
       " [19, 8140, 51, 61, 2953, 8141, 8142, 97, 2270, 4236, 5501, 5502, 1065, 8143],\n",
       " [51,\n",
       "  467,\n",
       "  1728,\n",
       "  1167,\n",
       "  1588,\n",
       "  19,\n",
       "  4234,\n",
       "  8144,\n",
       "  480,\n",
       "  102,\n",
       "  532,\n",
       "  3520,\n",
       "  2567,\n",
       "  1729,\n",
       "  532],\n",
       " [51, 615, 126, 5499, 480, 5500, 1003],\n",
       " [51],\n",
       " [2566, 4235, 2566, 51, 615, 126, 8145, 6, 91, 152],\n",
       " [38, 532, 51, 213, 8146, 8147, 480],\n",
       " [8148, 5503, 8149, 8150, 97, 89, 51],\n",
       " [8151, 2271, 51],\n",
       " [1383,\n",
       "  5504,\n",
       "  352,\n",
       "  423,\n",
       "  51,\n",
       "  2568,\n",
       "  69,\n",
       "  2036,\n",
       "  1484,\n",
       "  8152,\n",
       "  480,\n",
       "  1730,\n",
       "  1297,\n",
       "  1297,\n",
       "  1297,\n",
       "  5505],\n",
       " [51, 2954, 43, 79, 4237, 2569, 8153, 533, 43, 497, 534, 1589, 31, 2037],\n",
       " [212, 1485, 2565, 46, 51, 616, 8154, 41, 245, 1385, 123, 8155],\n",
       " [8156, 8157, 51],\n",
       " [46, 222, 23, 179, 46, 51, 3521, 54, 179, 802],\n",
       " [8158, 8159, 14, 3522, 396, 354, 51, 48, 225],\n",
       " [246, 1724, 2955, 2956, 136, 2272, 1004, 51, 1298, 8160, 8161, 5506],\n",
       " [559, 5507, 5508, 1486, 87, 198, 8162],\n",
       " [1108, 29, 802, 2273, 425, 5509, 29, 716, 5510, 8163],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [3523, 2038, 2039, 2570, 2571, 5511, 970, 5512],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8164, 5513, 410, 511, 2957, 8165],\n",
       " [559, 5507, 113, 2572, 1005, 5508, 5514, 8, 243, 121, 1486],\n",
       " [1108, 8166, 2958, 4238, 8167, 2040, 8168],\n",
       " [8169, 31, 4239, 8170, 457, 1477, 8171],\n",
       " [919, 481, 1067, 166, 559, 5515],\n",
       " [559, 2959, 64, 4240, 5516, 206, 8172, 206],\n",
       " [559],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8173, 4, 841, 482, 559],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8174, 8175, 375, 778, 5517, 77, 5518, 43, 8176],\n",
       " [559, 1593, 2573, 803, 1168, 278, 970, 8177],\n",
       " [779, 1735, 4240, 5516, 579, 3, 226, 180, 167, 8178, 559],\n",
       " [559, 4241, 4242, 42],\n",
       " [57, 57, 559, 1593, 2573, 803, 1168, 617, 8179],\n",
       " [161, 8180, 90, 483, 691, 90, 483, 69, 2960, 8181],\n",
       " [1108, 253, 2574, 1487, 530, 5519, 481, 468, 2960, 498],\n",
       " [8182, 88, 5520, 2274, 2, 8183, 8184],\n",
       " [8185, 31, 513, 64, 58, 26, 1386, 2041, 804, 1594, 1479, 805],\n",
       " [806, 2575, 559, 41, 43],\n",
       " [8186, 35, 1387],\n",
       " [559],\n",
       " [559, 98, 1881, 101, 63, 521, 535, 468, 101, 100],\n",
       " [8, 56, 8187, 483, 2275, 8188, 5521],\n",
       " [1108, 29, 425, 8189, 511, 29, 425, 2576, 5522, 8190],\n",
       " [62,\n",
       "  129,\n",
       "  15,\n",
       "  236,\n",
       "  469,\n",
       "  4243,\n",
       "  2276,\n",
       "  1486,\n",
       "  2961,\n",
       "  5523,\n",
       "  2962,\n",
       "  8191,\n",
       "  8192,\n",
       "  8193,\n",
       "  8194,\n",
       "  8195,\n",
       "  8196,\n",
       "  559,\n",
       "  1595],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 130, 7],\n",
       " [4244, 5524, 2963, 296, 51, 1229, 2577, 1230, 8197, 4245, 2964],\n",
       " [8198, 4246, 3524, 1488, 296, 51],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 3525],\n",
       " [8199, 5525, 618, 296, 51, 618, 151, 513, 8200, 55, 619],\n",
       " [143, 843, 8201, 1489, 1596, 396, 296, 51, 5526, 8202, 1736, 105, 328],\n",
       " [29, 445, 296, 2965, 51, 42],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [8203, 296, 51, 5527, 3526, 2043, 1299, 48, 485, 1597, 123, 3526, 446, 780],\n",
       " [353, 51, 29, 396, 2042, 296, 8204],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 920],\n",
       " [807,\n",
       "  842,\n",
       "  1109,\n",
       "  1388,\n",
       "  296,\n",
       "  146,\n",
       "  484,\n",
       "  267,\n",
       "  1228,\n",
       "  199,\n",
       "  51,\n",
       "  807,\n",
       "  920,\n",
       "  1737,\n",
       "  1388,\n",
       "  59],\n",
       " [2966, 2967, 1110, 8205, 1389, 8206, 143, 843, 48, 8207, 51],\n",
       " [1294, 4247, 8208, 51, 521, 88, 296, 1006],\n",
       " [8209, 8210, 48, 296, 51, 100, 46, 124, 1231, 9, 64],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 2277],\n",
       " [8211, 2578, 29, 411, 296, 666, 8212, 2278, 51, 5528],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [8213, 1489, 8214, 143, 40, 296, 22, 8215, 51],\n",
       " [1229, 918, 470, 22, 46, 1390, 42, 22, 666, 296, 1229, 73, 51, 8216],\n",
       " [4244, 5524, 2963, 296, 51],\n",
       " [3527, 2279, 807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 1882],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 3525],\n",
       " [353, 51, 29, 396, 2042, 296, 1111, 2280, 59, 486, 560],\n",
       " [8217, 8218, 1068, 268, 296, 162, 514, 51, 467, 147, 921, 254, 808],\n",
       " [561, 353, 51, 29, 396, 2042, 296],\n",
       " [561, 11, 486, 1389, 1300, 296, 51, 8219, 8220, 8221, 667],\n",
       " [807,\n",
       "  842,\n",
       "  1109,\n",
       "  1388,\n",
       "  296,\n",
       "  146,\n",
       "  484,\n",
       "  267,\n",
       "  1228,\n",
       "  199,\n",
       "  51,\n",
       "  807,\n",
       "  920,\n",
       "  1737,\n",
       "  1388,\n",
       "  59],\n",
       " [296, 51],\n",
       " [397, 168, 2, 46, 844, 296, 51, 1301],\n",
       " [8222, 4, 917, 121, 48, 1391, 1488, 296, 51],\n",
       " [296, 2968, 118, 8223, 8, 513, 51],\n",
       " [457, 296, 51],\n",
       " [1883, 169, 296, 51, 2281, 2579, 295, 2282, 2969, 845, 5529, 8224],\n",
       " [846, 536, 1392, 5530, 115, 1, 2044, 33, 279, 1112, 8225],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [199, 4248, 840, 22, 740, 8226, 8227],\n",
       " [580, 48, 971, 199, 279, 595, 22, 562, 4249],\n",
       " [1490, 10, 110, 2045, 5531, 5, 279, 1598],\n",
       " [5532, 8228, 8229, 69, 350, 1113, 110, 4250, 3, 593, 8230, 279, 840],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [5, 5533, 2283, 1480, 1232, 227, 214, 5534],\n",
       " [8231, 8232, 8233, 8234, 4251, 581, 2284, 279, 874, 8235, 4252],\n",
       " [181, 279, 1169, 8236, 8237],\n",
       " [7, 580, 48, 971, 199, 279, 595, 22, 1884, 8238],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [8239, 1007, 279],\n",
       " [4253, 255, 62, 279, 5535, 55],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [620, 199, 279, 595, 22, 215],\n",
       " [8240, 8241, 8242, 255, 62, 1302, 2973, 279, 62],\n",
       " [1885, 1303, 8243, 1114, 1115, 4254, 5536, 1885, 8244, 279],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [8245, 8246, 175, 592, 2580, 487, 106, 39, 279, 152, 5537],\n",
       " [11, 26, 58, 279, 136, 540, 540],\n",
       " [562, 580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 69, 593, 75],\n",
       " [8247, 668, 4255, 280, 279],\n",
       " [1057, 8248, 1885, 279, 3528, 3529, 355, 2285],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22, 1393],\n",
       " [580, 48, 971, 199, 279, 595, 22, 1393],\n",
       " [592, 14, 279, 1233, 8249, 17, 26],\n",
       " [4256, 8250, 1595, 2581, 8251, 8252, 1739, 3519, 279, 922],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [8, 530, 1069, 1008, 279, 1886, 631],\n",
       " [972, 8253, 447, 8254, 8255, 105, 4257, 1116, 2974],\n",
       " [1491, 447, 8256, 8257, 4258, 3530, 1491, 8258],\n",
       " [2582, 3, 447, 1304, 1740],\n",
       " [8259, 632, 8260, 447, 2975, 1305, 5538, 8261],\n",
       " [8262, 8263, 8264, 447, 1585, 244, 2976, 1741, 8265, 1009, 3, 54, 5539, 8266],\n",
       " [447, 5540],\n",
       " [447, 2583, 2584, 8267, 4259, 1742, 520, 8268, 8269],\n",
       " [5541, 513, 447, 5542, 5543],\n",
       " [8270, 55, 5544, 596, 781, 447],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [447],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [2977, 692, 3, 5545, 447, 876, 3531],\n",
       " [8271, 447, 521, 8272],\n",
       " [253, 376, 88, 8273, 522, 447, 523, 427, 33, 4260],\n",
       " [8274, 919, 170, 255, 8275, 1306, 1888, 541, 121, 447],\n",
       " [61, 447, 8, 3532, 2585, 2978, 8276, 1599],\n",
       " [24, 5546, 377, 55, 8277, 877, 1307, 1600, 973, 669, 447, 8278, 213],\n",
       " [6, 8279, 8280, 2287, 8281, 447, 2048],\n",
       " [5541, 513, 447, 5542, 5543],\n",
       " [923, 447, 8282, 198],\n",
       " [8283, 8284, 8, 447, 4261, 138, 77, 5547, 356, 1601],\n",
       " [8285, 8286, 447, 8287, 1117, 1889, 1010, 375, 1170, 8288],\n",
       " [8289,\n",
       "  1743,\n",
       "  712,\n",
       "  597,\n",
       "  8290,\n",
       "  8291,\n",
       "  2288,\n",
       "  410,\n",
       "  5548,\n",
       "  533,\n",
       "  179,\n",
       "  447,\n",
       "  575,\n",
       "  8292,\n",
       "  4262],\n",
       " [2049, 5549, 5550, 8293, 8294, 1890, 9, 121, 447, 2979, 5551],\n",
       " [5552, 8295, 447, 515, 4263, 8296, 8297],\n",
       " [3, 1394, 447, 8298, 1395, 5553, 16, 1001],\n",
       " [5552, 447, 1891, 228, 5554, 4243, 1492, 1600, 4264, 5, 969, 2980],\n",
       " [8299, 1892, 170, 8300, 8301, 1171, 8302, 39, 101, 5555, 447, 579, 2050],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [447, 8303, 1234, 74, 8304, 8305, 1893, 8306, 847],\n",
       " [8307, 255, 924, 1394, 8308, 154, 8309, 598, 8310, 8311, 8312, 447, 33, 8313],\n",
       " [499, 3512, 557, 1115, 8314, 599, 8315, 8316, 23, 447, 4265, 198],\n",
       " [8317, 8318, 1482, 447, 669, 8319],\n",
       " [8320, 488, 3, 3533, 5519, 878, 471, 237, 782, 8321, 8322, 3534],\n",
       " [848, 582, 522, 466, 269, 596, 1308, 5556, 281, 471],\n",
       " [8323, 471, 5557],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8324, 8325, 471],\n",
       " [36, 471, 925, 270, 4266, 5558, 25, 5559, 2981],\n",
       " [8326, 8327, 8328, 471, 42],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8329, 743, 849, 1114, 247, 95, 1894, 2982],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8330, 175, 643, 281, 1894, 79, 102, 471, 839, 8331, 139, 1595, 1895],\n",
       " [2983, 4267, 4268, 471],\n",
       " [8332, 8333, 1396, 670, 8334, 471, 522],\n",
       " [8335, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [5560, 2586, 8336, 1172, 8337, 8338, 3535, 3536, 270, 471],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [471, 1744, 3537, 1, 8339, 500, 297, 88, 2051, 24],\n",
       " [8340, 2587, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [8341, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [632, 5561, 743, 1114, 247, 5562, 8342, 8343, 1896, 783, 314, 489],\n",
       " [102, 471, 643, 281, 95, 1894, 139],\n",
       " [8344, 280, 2984, 4270, 8345, 2289, 3538, 1397, 2952, 396, 471],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [743, 1, 19, 247, 5563],\n",
       " [465, 3505, 4271, 879, 809, 471, 428, 3539],\n",
       " [36, 471, 925, 270, 4266, 5558, 25, 5559, 2981],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [4, 151, 448, 5564, 449],\n",
       " [5565, 8346, 558, 172, 238, 8347, 1173, 353, 850, 173, 8348, 718],\n",
       " [8349, 1398, 238],\n",
       " [19, 8350, 19, 8351, 8352, 44, 2985, 8353, 238, 8354, 744, 8355],\n",
       " [238],\n",
       " [1115, 9, 298, 38, 1493, 131, 5495, 1745, 238, 8356, 8357],\n",
       " [8358, 8359, 96, 2, 483, 2986, 1117, 5566, 238],\n",
       " [808, 1309, 80, 1236, 719, 41, 592, 926, 238, 927, 1494],\n",
       " [9, 238, 81, 4272, 47],\n",
       " [26, 9, 1310, 8360, 743, 238, 621],\n",
       " [2987, 235, 238, 1495, 8361, 8362, 8363],\n",
       " [238],\n",
       " [19,\n",
       "  8364,\n",
       "  1311,\n",
       "  100,\n",
       "  1169,\n",
       "  1012,\n",
       "  974,\n",
       "  1897,\n",
       "  238,\n",
       "  2290,\n",
       "  1,\n",
       "  974,\n",
       "  2588,\n",
       "  5567,\n",
       "  3540],\n",
       " [1602, 24, 1603, 238],\n",
       " [19, 1312, 1604, 973, 1237, 238, 2052, 1399, 928, 4273, 248, 1013],\n",
       " [8365, 1313, 8366, 1745, 238, 1072, 398, 398, 4274],\n",
       " [19, 8367, 182, 42, 12, 1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 1746],\n",
       " [170, 1898, 9, 3541, 4275, 238],\n",
       " [880, 1496, 2269, 168, 2, 1238, 3542, 85, 9, 481, 4276, 8368, 238],\n",
       " [8369, 238, 720, 222, 26, 245],\n",
       " [84, 8370, 5568, 69, 168, 2, 1899, 5569, 238, 193],\n",
       " [4228, 3, 1293, 1014, 238],\n",
       " [350, 20, 4277, 107, 127, 485, 238, 159, 164, 8371],\n",
       " [1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 974, 2588, 5567, 42],\n",
       " [576, 398, 5570, 31, 975, 35, 176, 398, 8372],\n",
       " [19, 8373, 238, 38, 4278, 8374, 2988, 1497, 2983, 24, 8375],\n",
       " [26, 5571],\n",
       " [107, 8376, 3543, 20, 1118, 238, 8377, 8378],\n",
       " [1747, 5572, 238],\n",
       " [973, 1237, 238, 2052, 1399, 928, 4273, 248, 1013],\n",
       " [3544, 87, 721, 25, 4279, 16, 2989, 4, 5573, 8379, 2990, 8380, 238],\n",
       " [182, 42, 12, 1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 974, 2588],\n",
       " [3545, 2991, 8381, 429, 430, 2992, 662, 3546, 661],\n",
       " [3545, 2991, 8382, 429, 430, 2992, 662, 3546, 661],\n",
       " [780, 2993, 5574, 57, 57, 57, 57, 4280, 5575, 5576, 57, 57, 429],\n",
       " [5577, 563, 35, 1, 429],\n",
       " [64, 193, 1481, 597, 429],\n",
       " [1073, 16, 11, 536, 1748, 2291, 429, 490],\n",
       " [131, 8383, 429, 2589, 2053, 85, 426],\n",
       " [8384, 26, 105, 445, 36, 929, 429],\n",
       " [19,\n",
       "  8385,\n",
       "  35,\n",
       "  5578,\n",
       "  2590,\n",
       "  5579,\n",
       "  4281,\n",
       "  5580,\n",
       "  5581,\n",
       "  5582,\n",
       "  5583,\n",
       "  238,\n",
       "  429,\n",
       "  8386],\n",
       " [113, 151, 976, 429, 8387, 8388, 3, 23, 5584, 530],\n",
       " [8389, 1498, 5585, 26, 350, 871, 424, 1174, 54, 79, 206, 1119, 429, 8390],\n",
       " [2054, 564, 314, 5586, 784, 564, 5587, 5588, 429, 2591, 1175, 2592],\n",
       " [397, 8391, 4282, 1176, 5589, 542, 1749, 5590, 1168, 429, 1605],\n",
       " [3545, 2991, 8392, 3547, 429, 430, 2992, 662, 3546, 661],\n",
       " [19,\n",
       "  8393,\n",
       "  35,\n",
       "  5578,\n",
       "  2590,\n",
       "  5579,\n",
       "  4281,\n",
       "  5580,\n",
       "  5581,\n",
       "  5582,\n",
       "  5583,\n",
       "  238,\n",
       "  429,\n",
       "  1898],\n",
       " [3545, 2991, 8394, 429, 430, 2992, 662, 3546, 661],\n",
       " [164, 5591, 67, 1120, 2593, 429, 972, 8395, 3548],\n",
       " [429],\n",
       " [8396, 8397, 55, 2055, 458, 100, 8398, 1750, 5592, 429],\n",
       " [8399, 429],\n",
       " [5593, 90, 541, 1397, 521, 2056, 1015, 58, 1016, 2994, 5594, 1400, 1314],\n",
       " [2995, 2292, 482, 670, 229, 1239, 881, 429],\n",
       " [3549, 2996, 1401, 5595, 1606, 8, 810, 429],\n",
       " [671, 132, 2594, 487, 8400, 2057, 2269, 513, 713, 5596, 429, 3550, 426, 5597],\n",
       " [83, 6, 665, 8401, 8402, 8403, 115, 69, 1177],\n",
       " [5598, 4228, 4283, 2978, 472, 4, 121, 2058, 811, 238, 429],\n",
       " [175, 8404, 5599, 2059, 2293, 3551, 8405, 8406, 1607, 2997, 426],\n",
       " [8407, 8408, 356, 1017, 271, 4251, 1735, 200, 429],\n",
       " [8409, 50, 8410, 115, 61, 514, 429, 8411, 8412, 1, 8413],\n",
       " [1751, 2060, 3543, 8414, 5600, 429],\n",
       " [429, 2294, 1061, 8415, 3552],\n",
       " [8416, 16, 500, 107, 929, 1402, 5601, 429, 1499, 4284],\n",
       " [4285, 8417, 930, 2061, 429],\n",
       " [2595, 785, 522, 8418, 8419, 8420, 449, 513, 429],\n",
       " [780, 2993, 57, 5574, 57, 57, 57, 57, 4280, 5575, 5576, 57, 57, 429],\n",
       " [458, 24, 490, 429, 8421],\n",
       " [8422, 160, 3553, 8423, 1018, 745, 5602, 1074, 429, 8424],\n",
       " [3554, 8425, 1168, 600, 8426, 1168, 429, 1403, 70, 1605, 1404, 8427],\n",
       " [1395, 8428, 3555, 2995, 2292, 482, 670, 229, 1239, 881, 8429, 3555, 931],\n",
       " [5593, 90, 1177, 69, 2062, 622, 1019, 8430, 2063, 8431],\n",
       " [780, 2993, 8432, 57, 57, 57, 57, 57, 57, 4280, 57, 57, 57, 429, 8433, 449],\n",
       " [5538, 2064, 2564, 8434, 458, 100, 429, 2295],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [1071, 137, 543, 431, 74, 3556],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [841, 74, 8435, 253, 2296, 8436, 23, 442],\n",
       " [1020, 2996, 1405, 74, 2038, 490, 667, 812, 2998, 8437],\n",
       " [2999, 74, 8438, 2596, 8439, 5603, 565, 2999, 74, 2297, 98, 2596],\n",
       " [1316, 1315, 925, 4286, 91, 1406, 5604, 8440],\n",
       " [1020, 74, 1240, 1407, 2298, 1240, 448, 2, 2298],\n",
       " [8441, 1408, 566, 1409],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [8442,\n",
       "  5605,\n",
       "  8443,\n",
       "  5606,\n",
       "  74,\n",
       "  8444,\n",
       "  4287,\n",
       "  5607,\n",
       "  44,\n",
       "  1500,\n",
       "  813,\n",
       "  5608,\n",
       "  4288,\n",
       "  1317],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [5609, 3000, 74, 814, 2597, 1753, 4289, 44],\n",
       " [19, 8445, 8446, 1075, 74, 5610],\n",
       " [1071, 137, 543, 431, 74, 3556],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [1754, 5611, 4290, 1410, 3557, 3001, 5612, 3558, 4291],\n",
       " [932, 543, 431, 74, 1315],\n",
       " [8447, 1501, 2299, 111, 1609, 74, 8448, 74, 4292, 74, 130, 3559],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [848, 8449, 8450, 8451, 2598, 667, 8452, 1755, 8453, 281, 5613, 786, 8454],\n",
       " [1021, 256, 692, 74, 2300, 79, 644, 5614, 690, 563, 850, 8],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [932, 543, 431, 74, 1315],\n",
       " [3003, 1610, 299, 2599, 1724, 329, 2065, 645],\n",
       " [5615, 1502, 56, 618, 4294, 3004, 329, 1611, 2, 1503, 4295],\n",
       " [741, 5616, 2596, 8455, 563, 5617, 329, 25, 928, 43],\n",
       " [1756, 5618, 329, 1411],\n",
       " [1757, 631, 1612, 5619, 978, 118, 5],\n",
       " [2561, 2562, 162, 329, 328, 1241, 27, 501, 92, 4296],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [432, 329],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [529],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [2600, 814, 159, 29, 299, 329, 4297],\n",
       " [73, 1121, 811, 329, 25, 522, 5620, 3003],\n",
       " [1121, 1107, 48, 646, 4298, 329, 25],\n",
       " [2601, 14, 2601, 1613, 1166, 2067, 329, 1761, 14, 2601, 1613, 1166, 5621],\n",
       " [5622, 121, 4299, 850, 201],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522, 2989, 3560, 4300],\n",
       " [8456, 8457, 19, 4301, 329, 328, 1241, 27, 501, 92, 30],\n",
       " [3003, 2955, 8458, 1112, 1121, 143, 3561, 329, 8459],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 3562, 2301],\n",
       " [8460, 8461, 1399, 329],\n",
       " [2601, 14, 2601, 1613, 1166, 2067, 329, 1761, 14, 2601, 1613, 1166, 5621],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 3562, 2301],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [329, 328, 1241, 27, 501, 92, 30, 4301],\n",
       " [2302, 3563, 874, 8462, 631, 1612, 163, 68, 329, 3563],\n",
       " [329, 328, 1241, 27, 501, 92, 30, 2561, 2562, 162],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [1614, 4302, 5623, 631, 282, 8, 1746],\n",
       " [641, 594, 4303, 2602, 5624],\n",
       " [2603, 722, 1179, 1903, 594, 1122],\n",
       " [5625, 3, 2303, 5626, 8463, 8464, 8465, 113, 4304, 16],\n",
       " [815, 1903, 594, 1122, 2304],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [647, 594, 5627, 2305, 8466],\n",
       " [3005, 5628, 15, 2306],\n",
       " [8467, 8468, 594, 224, 8469, 8470, 152, 8471],\n",
       " [8472, 594],\n",
       " [11, 1023, 56, 115, 1615],\n",
       " [3564, 8473, 12, 852, 594, 5629, 2307, 5630, 3006, 5631, 3564, 8474, 8475],\n",
       " [594, 1122, 1165, 27, 44, 236, 6, 3565, 8476, 5632, 8477],\n",
       " [12, 1123, 29, 4305, 544, 663, 314, 853, 623, 68, 21, 594, 8478],\n",
       " [8479, 3566, 50, 3567, 412],\n",
       " [8480, 8481, 594, 4303, 8482, 5624],\n",
       " [583],\n",
       " [672, 1904, 3, 8483, 1180, 8484, 598, 113, 594, 145],\n",
       " [8485, 2604, 633, 8486, 3568, 601, 8487, 8488, 723, 8489],\n",
       " [8490, 105],\n",
       " [8491, 201, 1170, 8492, 8493, 3569, 8494, 8495, 5633, 1754, 594],\n",
       " [647, 594, 2068],\n",
       " [2603, 722, 1179, 1903, 594, 1122, 8496],\n",
       " [8497, 5627, 113, 8498, 2605],\n",
       " [647, 594, 919, 15, 56, 9, 3, 105, 804],\n",
       " [594, 1762, 1224, 3564, 3006, 14, 2308],\n",
       " [182, 42, 12, 8499, 672, 1904, 716, 594],\n",
       " [3005, 5628, 8500, 2606, 5634, 2034],\n",
       " [158, 55, 249, 1058, 1181, 802, 1023, 46, 2558, 673, 8501],\n",
       " [430, 23, 1076, 24, 8502],\n",
       " [647, 594, 1024, 250, 87],\n",
       " [1227,\n",
       "  664,\n",
       "  192,\n",
       "  1022,\n",
       "  599,\n",
       "  1901,\n",
       "  329,\n",
       "  1902,\n",
       "  3007,\n",
       "  3008,\n",
       "  4306,\n",
       "  3570,\n",
       "  816,\n",
       "  594],\n",
       " [4307, 427, 102, 202, 2, 8503, 594, 879, 3571, 3009],\n",
       " [216, 21, 787, 177, 594, 239, 201],\n",
       " [502, 8504, 2607, 8505, 4308, 25, 8506, 8507],\n",
       " [3010, 8508, 5635, 8509, 8510, 2069],\n",
       " [2070, 882, 25, 883, 4309, 3011, 1905, 644],\n",
       " [212, 854, 25, 14, 119, 933],\n",
       " [5636, 5637, 77, 50, 398, 25, 5556, 1025, 8511],\n",
       " [5638, 350, 5, 5639, 34, 25, 1182, 2, 3, 3012, 5640, 1074],\n",
       " [3572, 746, 18, 65, 194, 257, 48],\n",
       " [8512, 5641, 378, 1616, 530, 25, 223],\n",
       " [3573, 25, 871, 2071, 1906, 8513, 7],\n",
       " [3574, 2309, 8514, 5642, 918, 3575, 398, 25, 1907],\n",
       " [8515, 1883, 25, 215, 815, 747, 531, 8516, 8517],\n",
       " [328, 107, 882, 25, 3576, 482],\n",
       " [747, 25, 14, 119, 933, 1319, 104, 8518, 8519],\n",
       " [212, 93, 503, 934, 2071, 183, 207, 444, 674, 25],\n",
       " [5643, 446, 3013, 166, 2037, 2310, 25, 851, 2608, 4310],\n",
       " [5, 119, 8520, 5, 8521, 8522, 25, 8523, 1763, 8524, 1026],\n",
       " [504, 1504, 2609, 68, 2603, 46, 25, 328, 2610],\n",
       " [3, 398, 25, 356, 4311, 40, 11, 1183, 3014, 748],\n",
       " [4, 151, 1320, 4, 1309, 923, 25, 2072, 39],\n",
       " [8525, 784, 1908, 5644, 724, 25, 2073, 69, 8526, 179, 4312],\n",
       " [5645, 25, 1242, 5645, 1002, 1909, 1762, 4313],\n",
       " [8527, 8528, 8529, 1184, 8530, 25, 8531, 5595, 488, 545, 5646, 8532, 5647],\n",
       " [14, 8533, 107, 193, 2074, 25, 2075, 3576, 482],\n",
       " [8534, 979, 465, 525, 398, 25],\n",
       " [328, 107, 2074, 25, 3576, 482],\n",
       " [473, 8535, 1910, 1911, 849, 1610, 1, 295, 25, 849, 5648, 8536],\n",
       " [1505,\n",
       "  5649,\n",
       "  230,\n",
       "  93,\n",
       "  5650,\n",
       "  162,\n",
       "  299,\n",
       "  2076,\n",
       "  1,\n",
       "  167,\n",
       "  2555,\n",
       "  2611,\n",
       "  623,\n",
       "  399,\n",
       "  3015,\n",
       "  8537],\n",
       " [5651, 5652, 25, 8538, 7, 584, 3575, 8539, 1077, 2612, 433, 619],\n",
       " [2070, 882, 25, 883, 4309, 3011, 1905, 644],\n",
       " [877, 116, 935, 621, 147, 173, 1078, 29, 184, 487, 8540, 377, 25],\n",
       " [8541, 8542, 8543, 8544, 8545, 25, 668, 631, 330, 884],\n",
       " [398, 926, 4314, 8546, 158],\n",
       " [8547, 192, 1595, 2274, 153, 5653, 25],\n",
       " [25, 4279, 198, 1486, 8548, 2311, 54, 529],\n",
       " [8549, 2070, 882, 25, 883, 4309, 3011, 1905, 644, 5654],\n",
       " [2613, 25],\n",
       " [668, 240, 154, 8550],\n",
       " [1391, 104, 5655, 747, 25, 933, 14, 119, 815, 747, 240, 14, 119],\n",
       " [5656, 5657, 5658, 5659, 1293, 67, 96, 240, 2987, 1293, 8551],\n",
       " [8552, 96, 240],\n",
       " [4, 695, 240],\n",
       " [76, 485, 558, 52, 58, 282, 485, 67, 76, 485, 121, 240],\n",
       " [486, 179, 8553, 2077, 31, 2312, 8554, 8555, 2077, 2050, 240, 474],\n",
       " [504, 5660, 240, 1617, 1506, 3, 5661, 8556, 5662],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [3577, 2313, 240, 2078, 2314, 2071, 4315, 8557, 788],\n",
       " [8558, 240, 8559, 68, 1321],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [2069, 450, 16, 3016, 8560, 240, 5663, 875],\n",
       " [113, 695, 240],\n",
       " [3578, 5664, 5665, 8561, 602, 2614, 240, 60, 602, 2615, 3578, 8562],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [5656, 5657, 5658, 5659, 726, 505, 5666, 2987, 240, 603, 8563],\n",
       " [8564, 8565, 240, 3017, 3579, 1124, 8566, 8567, 855],\n",
       " [8568, 8569, 8570, 727, 3580, 1125, 8571, 2, 808, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [240, 2315, 4316, 206, 4317, 878, 166, 5667, 5668, 4318, 5669],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8572, 5670, 8573, 3, 271, 1507, 96, 77, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [566,\n",
       "  1079,\n",
       "  271,\n",
       "  106,\n",
       "  199,\n",
       "  470,\n",
       "  2616,\n",
       "  5671,\n",
       "  585,\n",
       "  240,\n",
       "  1909,\n",
       "  883,\n",
       "  2316,\n",
       "  74,\n",
       "  5672],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8574, 540, 412, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8575, 8576, 459, 1912, 1171, 925, 152, 1027, 5673, 8577],\n",
       " [182, 42, 12, 3018, 3019, 459, 780, 12],\n",
       " [1913, 22, 2, 459],\n",
       " [913, 459, 5674, 780, 913, 459, 8578, 59, 2617, 3581, 813, 3582, 5675, 4319],\n",
       " [3583, 3584, 4320, 5676, 2561, 2562, 3585, 624, 459, 1914, 3586, 4321, 3587],\n",
       " [2079, 1764, 26, 1881, 1508],\n",
       " [147, 751, 5677, 459, 3588, 1605, 564, 3020, 8579, 2289],\n",
       " [3016, 8580, 2317, 2618, 8581, 8582, 913, 459, 8583],\n",
       " [3021, 3018, 3019, 3589, 2318, 980, 1915, 3022, 460, 12, 5678],\n",
       " [604, 1618, 459, 3590, 8584, 8585, 1509, 8586, 8587, 8588],\n",
       " [3583, 379, 117, 925, 3591, 459, 3592, 8589, 8590],\n",
       " [1765, 459, 8591, 1766, 270, 8592, 3593, 8593, 4322, 1185, 3023],\n",
       " [185, 2619, 5679, 1510, 8594, 5680, 459, 8595],\n",
       " [271, 101, 5681, 3024, 535, 1511, 460, 222, 175, 1243, 8596, 8597],\n",
       " [101, 3594, 3595, 2080, 1080, 5682, 3596, 3025, 5683, 459, 6],\n",
       " [3021, 3018, 3019, 3589, 2318, 980, 1915, 3022, 460, 12, 459],\n",
       " [752, 459, 567, 46, 222, 2, 752, 1767],\n",
       " [166, 426, 4323, 19, 5684, 459, 8598, 1768, 127, 122, 753, 1, 849],\n",
       " [8599, 5685, 54, 115, 526, 5686, 459],\n",
       " [1395,\n",
       "  5687,\n",
       "  8600,\n",
       "  8601,\n",
       "  970,\n",
       "  4324,\n",
       "  673,\n",
       "  3597,\n",
       "  459,\n",
       "  1916,\n",
       "  1,\n",
       "  8602,\n",
       "  1412,\n",
       "  8603,\n",
       "  12],\n",
       " [101, 16, 526, 459, 460, 2620, 8604, 8605, 2617],\n",
       " [8606, 8607, 459, 480, 5688, 1236, 801, 8608],\n",
       " [23, 1726, 3598, 23, 4325],\n",
       " [8609, 1322, 8610, 113],\n",
       " [459, 36, 297, 52, 3599, 4, 4326, 8611],\n",
       " [468, 5, 8612, 459, 39],\n",
       " [1170, 44, 4327, 8613, 1917, 460, 12, 459],\n",
       " [459, 99, 2032, 8614, 568, 719],\n",
       " [5684, 459, 540, 540, 444, 671, 624, 624, 2046, 1323, 4328, 122, 4329, 132],\n",
       " [8615, 459, 1173, 2078],\n",
       " [754, 1512, 380, 5689, 2293, 315, 4330, 5690, 85, 426],\n",
       " [2607, 53, 1028, 315, 5691, 181, 1408, 8616, 755, 5, 2319, 3600, 1186],\n",
       " [3026, 669, 1324, 315, 3027, 5692, 5693, 8617],\n",
       " [817, 12, 42, 938, 36, 4331, 315, 5694, 974, 2320, 2321, 8618, 8619],\n",
       " [1244, 173, 3028, 925, 8620, 315, 8621, 8622, 315, 5695, 8623, 8624, 1769],\n",
       " [1510, 2585, 315, 533, 448],\n",
       " [9, 8625, 8626, 8627, 315, 8628, 52, 376, 223, 2621, 1316],\n",
       " [3601, 3602, 63, 315, 1493, 53],\n",
       " [8629,\n",
       "  4332,\n",
       "  5696,\n",
       "  8630,\n",
       "  755,\n",
       "  8631,\n",
       "  101,\n",
       "  8632,\n",
       "  584,\n",
       "  2284,\n",
       "  4333,\n",
       "  3029,\n",
       "  315,\n",
       "  1325,\n",
       "  8633,\n",
       "  8634,\n",
       "  58],\n",
       " [76, 728, 729, 315, 1029, 754, 47, 1063, 1619, 1620, 1413, 163],\n",
       " [77, 616, 648, 696, 177, 1918, 729, 315, 4334, 3030, 4335, 8635, 8636, 4336],\n",
       " [315, 8637, 179],\n",
       " [1126, 315, 1012, 5697, 578],\n",
       " [8638, 3517, 4337, 315],\n",
       " [253, 315, 3031, 925, 2622, 5698],\n",
       " [175, 3603, 3, 23, 8639, 8640, 1919],\n",
       " [730, 1920, 8641, 8642, 315, 8643],\n",
       " [182, 42, 12, 3604, 925, 1614, 5699, 315],\n",
       " [8644, 8645, 778, 315],\n",
       " [8646, 206, 115, 8647, 3, 11, 1621, 8648, 66, 315, 1187, 3605, 675],\n",
       " [8649, 3032, 716, 315],\n",
       " [315, 5700],\n",
       " [754, 1512, 380, 5689, 2293, 315, 4330, 5690, 85, 426],\n",
       " [76, 728, 729, 315, 1029, 754, 130, 1063, 1619, 1620, 1413, 163],\n",
       " [5701, 2623, 605, 5702, 1877, 5703, 3606, 1402],\n",
       " [67,\n",
       "  697,\n",
       "  5704,\n",
       "  8650,\n",
       "  1245,\n",
       "  2988,\n",
       "  5705,\n",
       "  1770,\n",
       "  673,\n",
       "  8651,\n",
       "  119,\n",
       "  315,\n",
       "  8652,\n",
       "  8653],\n",
       " [7, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 1246],\n",
       " [782, 41, 381, 413, 606, 461, 1513, 4296],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [2624, 8654, 381, 1622, 2081, 1414, 461, 1513],\n",
       " [381, 413, 606, 461, 1513, 536, 676, 586, 757],\n",
       " [8655, 2322, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [7, 381, 413, 606, 461, 756, 536, 676, 586, 757, 56, 381, 1771],\n",
       " [381, 102, 4338, 461, 756, 586, 757, 381, 1307, 4338, 1623, 886],\n",
       " [381, 413, 844, 1414, 461, 1513, 8656],\n",
       " [4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 1188, 3035, 1246],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [118,\n",
       "  1725,\n",
       "  1,\n",
       "  1,\n",
       "  4341,\n",
       "  8657,\n",
       "  1189,\n",
       "  1772,\n",
       "  3036,\n",
       "  8658,\n",
       "  461,\n",
       "  4342,\n",
       "  74,\n",
       "  676,\n",
       "  757,\n",
       "  8659],\n",
       " [381, 2625, 606, 886, 3037, 1414, 461, 1513, 536, 676, 586, 757],\n",
       " [36, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 7, 397, 3038, 4343],\n",
       " [19, 8660, 5706, 381, 413, 606, 886, 3037, 461, 1513, 536, 676, 586, 757],\n",
       " [2323, 819, 5707, 381, 413, 606, 5708, 4344, 1246],\n",
       " [36, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 844, 1414, 461, 1513, 381, 4345, 5709, 5710, 413, 3039, 5711],\n",
       " [381, 413, 844, 1414, 461, 1513, 3607, 601, 5712],\n",
       " [521, 381, 413, 2081, 87, 8661, 1015, 1912],\n",
       " [641, 8662, 5713, 1224, 461, 5714, 114, 123, 8663, 8664, 5715],\n",
       " [381, 413, 606, 461, 756, 1246],\n",
       " [3608, 8665, 3040, 381, 1622, 2081, 1414, 461, 1513],\n",
       " [1246, 1503, 4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 5716],\n",
       " [1921, 381, 413, 2625, 606, 886, 3037, 1414, 8666],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [381, 1622, 2081, 1414, 461, 1513, 8667],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 2081, 461, 756, 8668, 566, 4346],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [1246, 1503, 4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 5716],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [5717, 717, 3041, 1877, 381, 1622, 4338, 1623, 3037, 886, 3609, 8669, 721],\n",
       " [448, 382, 1326],\n",
       " [182, 42, 12, 8670, 1415, 3042, 5718, 382, 2626, 103, 3014, 3610, 8671],\n",
       " [676, 382, 3036, 1624, 5719, 1409, 586, 717, 3611, 5720, 4347],\n",
       " [8672, 586, 1770, 8673, 4348, 448, 5721, 8674, 382],\n",
       " [8675,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110,\n",
       "  8676],\n",
       " [8677, 160, 101, 427, 526, 939, 382, 23, 818, 1301, 55, 75],\n",
       " [5722, 8678, 17, 4349, 2084, 8679, 382],\n",
       " [382, 635, 451, 8680, 8681, 8682, 8683, 8684, 4350, 383, 7, 2326],\n",
       " [8685,\n",
       "  8686,\n",
       "  714,\n",
       "  139,\n",
       "  382,\n",
       "  66,\n",
       "  4348,\n",
       "  8687,\n",
       "  3612,\n",
       "  43,\n",
       "  2327,\n",
       "  8688,\n",
       "  2630,\n",
       "  1306,\n",
       "  23,\n",
       "  1742],\n",
       " [981, 922, 2269, 3043, 649, 382, 3023, 8689, 8690, 5654],\n",
       " [130,\n",
       "  5723,\n",
       "  8691,\n",
       "  2085,\n",
       "  1747,\n",
       "  3613,\n",
       "  1030,\n",
       "  241,\n",
       "  1922,\n",
       "  5724,\n",
       "  1006,\n",
       "  1486,\n",
       "  1224,\n",
       "  5725,\n",
       "  2328,\n",
       "  2328,\n",
       "  642,\n",
       "  382],\n",
       " [8692, 506, 50, 3044, 9, 5726, 50, 382, 2969, 598, 8693],\n",
       " [5727, 1626, 8694, 10, 1774, 1775, 8695, 382, 13, 8696],\n",
       " [8697, 8698, 8699, 670, 544, 8700, 677, 929, 2603, 382, 886, 31, 252],\n",
       " [448, 382, 1326],\n",
       " [3036, 546, 478, 1627, 382, 4344],\n",
       " [446, 807, 2086, 468, 382, 1005, 1327, 8701, 3045, 2086],\n",
       " [160, 1074, 8702, 8703, 3614, 1247, 3611, 2037, 5728, 382, 5721, 820],\n",
       " [8704,\n",
       "  533,\n",
       "  8705,\n",
       "  2083,\n",
       "  1773,\n",
       "  648,\n",
       "  231,\n",
       "  1625,\n",
       "  8706,\n",
       "  2631,\n",
       "  5729,\n",
       "  1,\n",
       "  8707,\n",
       "  8708,\n",
       "  382,\n",
       "  1021],\n",
       " [1416, 1, 50, 5730, 4266, 2946, 11, 1478, 2, 1302, 8709, 382, 3046],\n",
       " [448, 382, 1326],\n",
       " [617, 4351, 1031, 5731, 5732, 1500, 8710, 382, 8711, 8712, 8713],\n",
       " [8714, 33, 8715, 2065, 8716, 8717, 8718, 2050, 3615, 1110, 15, 8719, 68, 400],\n",
       " [8720,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110],\n",
       " [448, 382, 1326],\n",
       " [8721,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110],\n",
       " [8722,\n",
       "  842,\n",
       "  382,\n",
       "  8723,\n",
       "  8724,\n",
       "  8725,\n",
       "  8726,\n",
       "  2325,\n",
       "  2087,\n",
       "  8727,\n",
       "  2324,\n",
       "  5733,\n",
       "  8728,\n",
       "  3616],\n",
       " [1628, 1915, 34, 2632, 483, 382, 427, 1032, 8729],\n",
       " [1033, 676, 3036, 5734],\n",
       " [382, 1502, 8730, 8731, 1328, 940, 3617, 3047, 8732, 1190, 941, 8733],\n",
       " [8734, 633, 4352, 15],\n",
       " [206, 1127, 24, 8735],\n",
       " [26, 789, 8736, 849, 8737],\n",
       " [43, 8738, 16, 581, 1034, 5735, 475, 633, 631, 928, 8739, 252, 5698],\n",
       " [633, 1300],\n",
       " [8740, 506, 35],\n",
       " [8741, 2329, 8742, 5736, 152, 5737, 853, 3618, 8743, 8744],\n",
       " [5738, 474, 1106, 106, 3048, 69, 8745, 633],\n",
       " [1191, 8746, 8747, 8748, 8749, 8750, 143, 3049, 8751, 8752],\n",
       " [35, 821, 633, 556, 5739, 5740],\n",
       " [8753, 8754, 153, 1913, 633],\n",
       " [633, 3619, 249, 1776, 8755, 4353],\n",
       " [1081, 2633, 779, 62, 8756, 1192, 425],\n",
       " [615, 2634, 30, 78, 699, 873, 222, 45, 633, 5741],\n",
       " [212, 790, 2635, 633, 722, 5742, 8757, 441],\n",
       " [1034, 35, 3620],\n",
       " [8758, 11, 2, 4354, 540, 1082, 669, 499, 8759],\n",
       " [182, 42, 12, 8760, 1311, 8761, 130, 633, 3621, 782, 4355, 357, 8762, 8763],\n",
       " [4356, 633, 2330, 1083, 514, 664, 14, 4357, 741, 76, 2331, 8764, 5743],\n",
       " [1322, 23, 3622, 4, 5744],\n",
       " [2332, 2333, 67, 633, 2334, 8765, 255, 478, 8766],\n",
       " [492, 633, 2088, 424, 84, 2, 8767, 8768, 161, 2],\n",
       " [168, 2, 39, 1629, 678, 940],\n",
       " [604, 8769, 3, 2636, 26],\n",
       " [8770, 4, 679, 1, 11, 633],\n",
       " [8771, 545, 26, 633, 813, 625, 378, 81, 5745, 1630, 514],\n",
       " [5746, 7, 5, 1329, 1923, 4358, 1248, 79, 8772, 8773, 1514],\n",
       " [8774, 3623, 633, 3624, 3050, 8775],\n",
       " [182, 42, 12, 1311, 972, 633, 3051, 8776],\n",
       " [615, 2634, 30, 78, 699, 873, 222, 45, 633, 5741, 2637],\n",
       " [8777, 3052, 67, 5, 8778, 973, 633],\n",
       " [8779, 8780, 878, 856, 2089, 604, 556, 633],\n",
       " [5747, 136, 2335, 8781, 69],\n",
       " [8782, 8783, 8784, 3625, 2336, 3626, 927, 39, 8, 633],\n",
       " [8785, 8786, 1515, 3053, 623, 42, 12],\n",
       " [506, 1631],\n",
       " [5748, 8787, 201, 5749, 5750, 283],\n",
       " [8788, 56, 4274, 1084, 8789, 1128, 4359, 8790, 6, 8791, 4360, 8792],\n",
       " [5751, 1, 401, 4361, 1398, 942, 4362],\n",
       " [250, 5752, 719, 401, 64, 5752, 641, 4363],\n",
       " [5753, 115, 401, 80, 138, 502, 8793, 4364, 99, 8794],\n",
       " [115, 879, 458, 298, 5754, 401, 4365, 5755, 36, 2337],\n",
       " [2090, 887, 1924, 6, 4366, 3627, 5756, 5757, 5758, 401, 2638, 248, 3054, 85],\n",
       " [2090, 887, 1924, 6, 4366, 3627, 5756, 5757, 5758, 401, 2638, 248, 3054, 85],\n",
       " [2091, 23, 804, 525, 206, 401, 525, 314],\n",
       " [8795, 8796, 280, 401, 198, 64, 8797, 3628, 3055, 5759, 47],\n",
       " [921, 8798, 925, 8799, 8800, 401, 4367, 758, 8801],\n",
       " [888, 8802, 8803, 87, 8804, 1, 3056, 8805, 8806, 401, 1325],\n",
       " [8807, 1777, 8808],\n",
       " [4, 232, 4368, 700, 1631, 401, 8809],\n",
       " [401, 1498, 8810, 8811, 8812, 77, 1751, 115, 1129, 93],\n",
       " [8813, 8814, 8815, 169, 8816, 401, 80, 2, 138, 5760],\n",
       " [6,\n",
       "  4369,\n",
       "  3629,\n",
       "  3548,\n",
       "  1314,\n",
       "  45,\n",
       "  2639,\n",
       "  2640,\n",
       "  857,\n",
       "  4370,\n",
       "  672,\n",
       "  3057,\n",
       "  1882,\n",
       "  4371,\n",
       "  4372,\n",
       "  3630,\n",
       "  2641],\n",
       " [195, 398, 3631, 4373, 401, 1925, 4290, 3058, 8817],\n",
       " [8818, 8819, 739, 35],\n",
       " [8820, 913, 2642, 1193, 3632, 4374, 4375, 8821, 8822, 8823, 8824, 691],\n",
       " [532, 4376, 889, 401, 80, 142, 532],\n",
       " [4, 581, 181, 3059, 1926, 24, 142, 401, 890, 575, 4, 8825],\n",
       " [4, 401, 1130, 350, 102],\n",
       " [401],\n",
       " [8826, 33, 465, 3633, 3060, 16, 401, 890, 2643],\n",
       " [8827, 401],\n",
       " [8828, 612, 569, 5, 431, 3634, 581, 3628, 2644, 401, 891, 35, 1927],\n",
       " [83, 80, 401, 1516, 8829, 8830, 8831],\n",
       " [8832, 521, 8833, 356, 8834, 401, 80, 5761, 8835, 8836],\n",
       " [8837, 8838, 217, 401],\n",
       " [401, 8839, 8840, 11, 4377, 8841, 414, 2645],\n",
       " [2090,\n",
       "  887,\n",
       "  1924,\n",
       "  129,\n",
       "  3635,\n",
       "  4366,\n",
       "  3627,\n",
       "  4378,\n",
       "  401,\n",
       "  2638,\n",
       "  8842,\n",
       "  1417,\n",
       "  1517,\n",
       "  2,\n",
       "  402],\n",
       " [401, 80, 5762, 166, 3636, 1418, 1083, 5762, 943, 3636, 8843, 1916, 8844],\n",
       " [8845, 8846, 63, 1085, 8847, 1631, 401, 892, 1632, 2957, 8848, 1185, 529],\n",
       " [650, 8849, 175, 153, 1060, 8850, 29, 401],\n",
       " [11, 26, 500, 3061, 2632, 401, 890, 622, 1500],\n",
       " [1633, 8851, 316, 4379, 8852, 8853, 8854],\n",
       " [2092, 3062, 8855, 3063, 3637, 9, 316],\n",
       " [8856, 2646, 564, 8857, 316],\n",
       " [1194, 8858, 3638, 52, 102, 316, 3532, 2982, 8859],\n",
       " [2338, 316, 2, 61, 514],\n",
       " [316, 8860, 24, 442, 206, 2647, 3064, 8861],\n",
       " [8862,\n",
       "  54,\n",
       "  79,\n",
       "  111,\n",
       "  90,\n",
       "  255,\n",
       "  893,\n",
       "  62,\n",
       "  665,\n",
       "  1928,\n",
       "  8863,\n",
       "  58,\n",
       "  1486,\n",
       "  316,\n",
       "  73,\n",
       "  3065],\n",
       " [2648, 2335, 1082, 316],\n",
       " [1929, 90, 316, 8, 84, 1382, 81, 9, 1419, 1195],\n",
       " [2649, 8864, 8865, 560, 1634, 560, 316, 1249],\n",
       " [8866, 2630, 246, 136, 4380, 316],\n",
       " [8867, 67, 84, 1032, 316],\n",
       " [153, 1778, 4381, 316],\n",
       " [696, 8868, 314, 8869, 316, 4, 55],\n",
       " [804, 4382, 1032, 1196, 3639, 5763, 8870, 11, 96, 822, 9, 316, 105],\n",
       " [1930, 316],\n",
       " [8871, 500, 5663, 1086, 316],\n",
       " [9, 1330, 2339, 814, 1884, 8872, 2338, 1518, 316],\n",
       " [5764, 8873, 223, 2, 1322, 2338, 316],\n",
       " [206, 316, 3066, 96, 3067, 1931],\n",
       " [8874, 2340, 3640, 153, 223, 5765, 316, 252, 105],\n",
       " [8875, 506, 4, 316, 398, 5766],\n",
       " [8876, 3068, 3604, 2650, 26, 4383, 131, 3069, 176, 603, 54, 466, 280, 316],\n",
       " [8877, 8878, 1622, 316],\n",
       " [8879, 26, 15, 90, 316, 398, 5767, 4328, 4384],\n",
       " [8880, 837, 271, 316, 546, 176],\n",
       " [8881, 316, 1087, 11, 26, 169],\n",
       " [1197, 55, 3070, 90, 316, 1322, 161],\n",
       " [8882, 33, 8883, 316, 95, 412, 677, 1779, 2651, 75, 599],\n",
       " [8884, 316, 8885, 3071, 168, 297, 8886, 223],\n",
       " [1930, 316, 631, 8887],\n",
       " [3641, 927, 316],\n",
       " [8888, 8889, 132, 8890, 271, 5768, 566, 3631, 316],\n",
       " [8891, 1198, 841, 560, 316],\n",
       " [56, 176, 412, 839, 661, 5769, 569, 151, 102, 4385, 316],\n",
       " [8892, 1322, 3642, 317, 525, 662],\n",
       " [8893, 84, 3, 317, 33, 116, 32],\n",
       " [317, 5770, 8894, 169, 808, 1330, 808, 3072, 5771],\n",
       " [132, 8895, 581, 403, 317],\n",
       " [3073, 3, 317, 1929, 5772, 4386, 236, 478, 35],\n",
       " [564, 584, 8896, 317, 759, 2652, 8897, 8898],\n",
       " [8899, 8900, 450, 15, 2, 317, 198, 15, 5773, 11, 3585, 1416, 2341],\n",
       " [631, 8, 856, 8901, 26, 691, 1519, 598, 317, 602, 5774, 90, 4387],\n",
       " [50, 3, 317, 8902, 3643, 175, 808, 8903, 35],\n",
       " [5775, 31, 891, 3644, 928, 1484, 2093, 760, 317, 2949, 4388, 662, 4389],\n",
       " [1635, 481, 2631, 3045, 56, 890, 317, 810, 1737],\n",
       " [1420, 317, 2653, 966, 2342, 8904],\n",
       " [4390, 8905, 317],\n",
       " [8906, 2654, 3, 317, 2949, 377, 117, 23, 255],\n",
       " [3, 2343, 891, 8907, 58, 317, 8908, 198, 8909, 8910],\n",
       " [100, 3, 317, 8911],\n",
       " [317, 3645, 661],\n",
       " [4286, 4391, 377, 8912, 317, 4330, 644, 844, 195, 499, 54, 102, 8913],\n",
       " [8914,\n",
       "  16,\n",
       "  4392,\n",
       "  4393,\n",
       "  8915,\n",
       "  1199,\n",
       "  33,\n",
       "  4394,\n",
       "  1395,\n",
       "  1,\n",
       "  317,\n",
       "  621,\n",
       "  3074,\n",
       "  578,\n",
       "  69],\n",
       " [5776, 5777, 1, 3643, 317],\n",
       " [579, 8916, 2580, 487, 1, 67, 79, 218, 317, 4395, 121, 46],\n",
       " [317, 2094],\n",
       " [152, 16, 317, 397, 162, 1167, 486, 5776],\n",
       " [8917, 2, 602, 317],\n",
       " [8918, 12, 317],\n",
       " [8919, 1742, 58, 84, 55, 2344, 4396, 5778, 317],\n",
       " [1024, 300, 2095, 579, 317, 378, 1250, 2065, 1421],\n",
       " [1200, 4397, 317, 2096, 2655, 5779, 8920, 4349, 5780, 8921, 8922],\n",
       " [67, 8923, 1131, 810, 84, 2, 470, 317],\n",
       " [4398, 67, 253, 216, 3075, 317],\n",
       " [3076, 3, 317, 378],\n",
       " [1085, 317, 105],\n",
       " [8924, 8925, 5781, 525, 5782, 806, 5783, 317, 170, 1236],\n",
       " [8926, 8927, 101, 3646, 2, 874, 8928, 1072, 452],\n",
       " [8929, 1932, 481, 3647, 5784, 5785, 661, 452, 5786, 1422],\n",
       " [4399,\n",
       "  431,\n",
       "  96,\n",
       "  596,\n",
       "  36,\n",
       "  8930,\n",
       "  8931,\n",
       "  452,\n",
       "  5787,\n",
       "  8932,\n",
       "  3077,\n",
       "  487,\n",
       "  944,\n",
       "  626,\n",
       "  4400],\n",
       " [8933, 452],\n",
       " [850, 2097],\n",
       " [8934, 1128, 1384, 3648, 31, 1132, 1780, 452, 131, 8935, 791, 57, 57],\n",
       " [2656, 4401, 858, 4402, 452, 1933],\n",
       " [5, 119, 8936, 8937, 5788, 452, 1251, 218, 3051],\n",
       " [3049, 1781, 4403, 1781, 448, 452, 118, 1176],\n",
       " [3078, 8938, 8939, 838, 894, 452, 15, 23],\n",
       " [4404, 8940, 2288, 8941, 8942, 452, 8943, 3626, 5789, 8944, 1181, 2657, 5790],\n",
       " [5791, 3649, 1252, 452, 2345, 2273, 3079],\n",
       " [99, 1079, 79, 888, 452, 8945, 2302, 2346, 4405, 7],\n",
       " [8946, 506, 63, 5, 452, 1407],\n",
       " [1782, 8947, 651, 8948, 452, 8949, 8950, 8951, 3650, 8952, 4406],\n",
       " [3049, 1781, 4403, 1781, 448, 452, 118, 1176],\n",
       " [2054, 31, 8953, 8954, 216, 412, 5792, 8955, 147, 3651, 625, 26, 2347],\n",
       " [8956, 376, 8957, 1001, 452],\n",
       " [8958, 452, 521, 809],\n",
       " [8959, 8960, 452, 8961],\n",
       " [8962, 452, 46, 69, 940, 9],\n",
       " [2658, 2291, 452, 241, 1201, 8963, 8964, 8965],\n",
       " [2, 1934, 969, 1913, 8966, 45, 3027, 8967, 50, 1932, 151, 452],\n",
       " [8968, 873, 1636, 87, 1423],\n",
       " [8969, 8970, 1067, 295, 1424, 1293, 452, 524, 8971],\n",
       " [84, 547, 3651, 3080, 219, 8972, 8973, 277, 452, 8974, 2348, 3081, 449],\n",
       " [5793, 8975, 5784, 67, 761, 4407, 83, 452, 297],\n",
       " [5793, 3647, 583, 58, 1175, 184, 5786, 2098, 4374, 452],\n",
       " [1121, 3652, 318, 452, 1599, 1, 2099, 504, 821],\n",
       " [2349, 742, 8976, 982, 162, 3082, 330, 1062, 8977, 1637, 3653, 1638],\n",
       " [8978, 895, 8979, 8980, 452, 5794, 8981],\n",
       " [3083, 452, 8982, 8983],\n",
       " [2659, 8984, 469, 934, 1247, 2350, 478, 314, 4385],\n",
       " [1133, 526, 529, 792],\n",
       " [5, 2351, 4408, 1007, 314, 885, 548],\n",
       " [548, 3654, 166, 895, 8985],\n",
       " [8986, 8987, 3, 489, 548, 549, 5795, 56, 669, 8988, 2977, 166, 1180, 1783],\n",
       " [548, 8989, 38, 8990, 8991, 4409, 793, 52, 8992, 24, 1418],\n",
       " [77, 1067, 5796, 548, 614, 106, 919, 3655, 236],\n",
       " [5797, 8993, 1331, 5797, 486, 5798, 42, 8994, 2, 841, 87],\n",
       " [4226],\n",
       " [856, 8995, 5799, 8996, 330, 1120, 8997, 3, 84, 1507],\n",
       " [548, 896, 8998, 700, 8999, 3507, 49, 1935, 3656, 1088, 2660],\n",
       " [548, 9000, 5800],\n",
       " [3084, 2661, 9001, 9002, 548],\n",
       " [4410, 5801, 1088, 1018, 511],\n",
       " [4411, 3085, 5798],\n",
       " [1639, 2060, 2577],\n",
       " [5, 9003, 1520, 9004, 745, 1332, 5802],\n",
       " [9005, 77, 3657, 548, 9006, 45, 9007, 5803, 5804, 5805, 9008, 1333],\n",
       " [64, 253, 3655, 3582, 3086, 548],\n",
       " [4412, 9009, 548, 9010],\n",
       " [548, 3654, 3087, 62],\n",
       " [9011, 127, 9012, 5806, 548, 35],\n",
       " [548, 2100, 5750],\n",
       " [548, 3654, 3087, 2, 680],\n",
       " [9013, 255, 255, 9014, 3088, 1077, 69, 1784, 548, 1253],\n",
       " [3087, 644, 9015],\n",
       " [16, 675, 5807, 2352, 1785],\n",
       " [35, 3582, 3086, 548],\n",
       " [9016, 3, 713, 9017, 2053],\n",
       " [77, 475, 5808, 9018, 548],\n",
       " [147, 921, 548],\n",
       " [301, 7, 9019, 548, 9020, 1334, 2652],\n",
       " [3075, 243, 1009, 5809, 548, 925, 5810],\n",
       " [945, 4413, 5807],\n",
       " [1008, 3655, 475, 5811, 548, 176],\n",
       " [353,\n",
       "  779,\n",
       "  15,\n",
       "  761,\n",
       "  4414,\n",
       "  168,\n",
       "  2,\n",
       "  5812,\n",
       "  548,\n",
       "  2034,\n",
       "  15,\n",
       "  467,\n",
       "  751,\n",
       "  2101,\n",
       "  2353,\n",
       "  3658],\n",
       " [3087, 1640, 918],\n",
       " [1389, 214],\n",
       " [1335, 9021, 1521, 730, 214, 1522, 1035, 4, 818, 739],\n",
       " [946, 9022, 541, 214, 1066, 1425],\n",
       " [1036, 4415, 250, 1757, 607, 214, 1641, 2354, 1520, 2102, 79, 546],\n",
       " [1336, 9023, 3026, 214, 9024, 5670, 9025, 1642],\n",
       " [214, 330, 1202, 1523, 9026, 9027, 184, 893, 617, 5813, 9028, 17],\n",
       " [812, 2, 214, 115, 155],\n",
       " [1035, 214],\n",
       " [8, 4416, 15, 3065, 2355, 214, 4222, 214],\n",
       " [483, 9029, 4417, 195, 712, 214, 583, 300, 1936],\n",
       " [9030, 880, 625, 2312, 1037, 69, 84, 560, 214, 2053, 1038],\n",
       " [9031, 9032, 283, 2046, 214, 1786, 4418, 1254, 731, 3659, 5814],\n",
       " [214, 1786, 1925, 11, 62, 2328, 105],\n",
       " [9033, 214, 545, 9034],\n",
       " [54, 486, 8, 1926, 2662, 11, 248, 214, 1786, 43, 4419, 9035],\n",
       " [603, 9036, 622, 481, 1089, 214],\n",
       " [1640, 282, 8, 792, 9037, 2103, 762, 9038, 214, 31, 1086],\n",
       " [24, 3660, 9039, 5815, 1337, 8, 214, 2575, 5815, 5816],\n",
       " [9040, 557, 1398, 730, 214],\n",
       " [924, 214],\n",
       " [1309, 9041, 3567, 214, 3639, 9042],\n",
       " [791, 5817, 214],\n",
       " [41, 9043, 121, 3089, 3661, 214, 302, 3601, 576, 5818, 9044],\n",
       " [29, 1787, 1203, 102, 9045, 891, 2356, 1019, 214],\n",
       " [1039, 9046, 214, 16],\n",
       " [9047, 9048, 679, 485],\n",
       " [2608, 5819, 100, 763, 214, 9049, 3090],\n",
       " [9050, 9051, 9052, 214, 9053, 9054, 496, 9055, 9056],\n",
       " [9057, 1191, 85, 166, 4420, 5, 617, 5820, 2104, 9, 111],\n",
       " [9058, 1134, 214],\n",
       " [9059, 3091, 5821, 2663, 9060, 9061, 673, 24, 4421, 214, 979],\n",
       " [3092, 2664, 214, 3662, 3663, 462, 9062, 4422, 79, 102, 3093, 4423],\n",
       " [1643, 9063, 9064, 1477, 148],\n",
       " [5822, 216, 2648, 1132, 2648, 1932, 2, 214, 9065],\n",
       " [1756, 3664, 2105, 2649, 5517, 886, 214],\n",
       " [9066, 180, 488, 67],\n",
       " [9067, 180, 3641, 9068, 499],\n",
       " [9069, 180, 5823],\n",
       " [2665, 716, 759, 159, 9070, 3637, 9071, 5824, 6, 3094, 9072],\n",
       " [9073, 632, 5825, 596, 9074, 9075, 180, 75, 9076, 9077],\n",
       " [4, 1788, 180, 3665],\n",
       " [9078, 9079, 1495, 170, 180, 1937, 4424],\n",
       " [5826, 3666, 5827, 520, 121, 5828],\n",
       " [54,\n",
       "  180,\n",
       "  529,\n",
       "  4425,\n",
       "  1762,\n",
       "  814,\n",
       "  2357,\n",
       "  696,\n",
       "  6,\n",
       "  1010,\n",
       "  1497,\n",
       "  3667,\n",
       "  9080,\n",
       "  9081,\n",
       "  5829],\n",
       " [1203, 530, 83, 180, 412, 1038],\n",
       " [9082, 931, 1236, 661, 680, 4426, 9083, 180, 9084, 3668, 2358, 2, 180],\n",
       " [180, 4427, 5830, 4427],\n",
       " [9085, 806, 560, 9086, 4428, 430, 3669, 9087, 525, 180, 625, 3095, 44],\n",
       " [9088, 9089, 170, 90, 180, 121, 967, 5831],\n",
       " [5832, 180, 1604, 162, 9090, 5],\n",
       " [180, 488, 24, 1016, 77, 3, 1524, 356, 9091, 79, 67],\n",
       " [525, 180, 80],\n",
       " [9092, 9, 180, 3670, 9093],\n",
       " [26, 56, 1204, 476, 1, 77, 662, 1245, 4429, 9094, 1783, 1204, 180, 5833],\n",
       " [5834, 5835, 845, 535, 3671, 9095, 587, 2284, 180, 5836, 5572],\n",
       " [4, 1937, 3605, 67, 1128, 4430, 180, 9096, 5837],\n",
       " [62, 43, 3093, 3, 2551, 2044, 930, 180, 243, 3, 2551],\n",
       " [9097, 50, 9, 180, 9098],\n",
       " [2295, 1426, 939, 180, 5838, 180, 9099, 2295, 1426, 9100, 9101],\n",
       " [9102, 9103, 2359, 180, 5839],\n",
       " [180, 1604, 3096, 9104, 5840],\n",
       " [9105, 9106, 9107, 9108, 9109, 9110, 9111, 814, 180, 442, 1067],\n",
       " [1338, 180, 2666],\n",
       " [1135, 1644, 557, 24, 9, 180, 1135, 4224, 9112],\n",
       " [9113, 2667, 9114, 180, 111, 106, 9115, 2106, 1136, 4431],\n",
       " [529, 466, 3044, 812, 9116, 180, 1604, 3672, 3551],\n",
       " [1226, 410, 430, 9117, 499, 12, 9118, 9119, 971, 1205, 852, 9120],\n",
       " [206, 3, 1200, 972, 6, 357, 180, 4432, 9121, 9122],\n",
       " [5841,\n",
       "  3097,\n",
       "  57,\n",
       "  9123,\n",
       "  9124,\n",
       "  57,\n",
       "  5841,\n",
       "  3097,\n",
       "  57,\n",
       "  1743,\n",
       "  9125,\n",
       "  5842,\n",
       "  1006,\n",
       "  2,\n",
       "  180,\n",
       "  5843],\n",
       " [76, 1135, 3098, 180, 9126, 2668],\n",
       " [9127, 760, 3070, 4433, 121, 492, 284, 9128, 2669, 3099],\n",
       " [9129, 6, 667, 2094, 284, 1738, 9130, 630, 55, 1927, 3673],\n",
       " [5844, 9131, 550, 3, 550, 1938, 284, 587, 3100, 1525, 2, 3101, 9132],\n",
       " [525, 9133, 23, 284, 3674],\n",
       " [1526, 284, 81],\n",
       " [4434, 9134, 90, 284, 9135],\n",
       " [9136, 545, 1185, 598, 9137, 1, 378, 608, 284, 511],\n",
       " [282, 143, 9138, 9139, 2299, 101, 3675, 425, 284, 353, 9140, 34, 1090, 5845],\n",
       " [29, 1255, 9141, 284, 280, 9142],\n",
       " [2094, 284, 1111],\n",
       " [9143, 5846, 844, 284, 2107, 85, 541],\n",
       " [779, 415, 716, 1427, 9144, 284, 9145, 3071, 2360, 5847, 1130],\n",
       " [9146, 67, 5848, 5849, 5850, 608, 280, 284, 9147],\n",
       " [2054, 9148, 1206, 284, 9149, 147, 2298, 918, 4435, 9150],\n",
       " [632, 84, 9151, 9152, 87, 41, 23, 284, 8, 9153, 2670, 3078, 3676],\n",
       " [9154, 23, 968, 377, 4436, 1645, 1137, 1255, 284],\n",
       " [16, 856, 284, 358, 24, 1062],\n",
       " [2361, 402, 284, 297, 4437, 1789, 1202, 597, 2569],\n",
       " [9155, 465, 2636, 502, 9156, 284, 5851, 483, 4438],\n",
       " [9157, 9158, 5852, 3677, 284, 297, 609, 9159, 9160, 117],\n",
       " [9161, 9162, 9163, 61, 4439, 551, 3102, 3102, 284, 297],\n",
       " [9164, 198, 2049, 284, 764, 9165, 250, 1109, 1387],\n",
       " [5853, 9166, 5854, 5855, 2362, 284, 147, 4440],\n",
       " [412,\n",
       "  543,\n",
       "  9167,\n",
       "  1939,\n",
       "  3678,\n",
       "  9168,\n",
       "  4241,\n",
       "  284,\n",
       "  9169,\n",
       "  5856,\n",
       "  9170,\n",
       "  9171,\n",
       "  1,\n",
       "  2363,\n",
       "  9172],\n",
       " [9173, 9174, 2942, 284, 3, 5857, 62, 102, 9175],\n",
       " [3679, 9176, 9177, 1726, 727, 3680, 3681, 4441, 15, 6, 9178, 3682, 284],\n",
       " [9179,\n",
       "  222,\n",
       "  838,\n",
       "  91,\n",
       "  1175,\n",
       "  284,\n",
       "  442,\n",
       "  55,\n",
       "  2108,\n",
       "  716,\n",
       "  280,\n",
       "  613,\n",
       "  1232,\n",
       "  1035,\n",
       "  449],\n",
       " [9180, 2, 284, 4442],\n",
       " [668, 6, 3, 284, 9181, 180, 9182, 183],\n",
       " [888, 4443, 284],\n",
       " [9183, 377, 9184, 284, 253, 26, 4444, 1428, 9185, 1199, 43, 5858, 783],\n",
       " [9186, 9187, 125, 3, 23, 9188, 284, 9189, 9190],\n",
       " [9191, 9192, 9193, 9194, 9195, 283, 2109, 284, 9196, 2671, 9197, 617],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [1245, 2085, 2, 31, 53, 427, 115, 17, 217, 83, 1940, 26],\n",
       " [9198, 9199, 58, 96, 9200, 5859, 1180, 17, 217],\n",
       " [1527,\n",
       "  3103,\n",
       "  9201,\n",
       "  9202,\n",
       "  133,\n",
       "  17,\n",
       "  2364,\n",
       "  217,\n",
       "  1497,\n",
       "  2672,\n",
       "  4445,\n",
       "  2365,\n",
       "  694,\n",
       "  243,\n",
       "  814,\n",
       "  33],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [1027, 1941, 9203, 2110, 463, 217, 133, 17, 217],\n",
       " [175, 9204, 4446, 217, 463, 133, 17, 4447, 9205, 5860, 9206, 5608, 426],\n",
       " [164, 2112, 1, 17, 3052, 9207, 217, 9208, 813, 5861, 9209, 4448],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [5, 137, 314, 17, 217, 2674, 1193, 4395, 813],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 19, 1587],\n",
       " [2675,\n",
       "  9210,\n",
       "  1640,\n",
       "  475,\n",
       "  448,\n",
       "  4,\n",
       "  1338,\n",
       "  4449,\n",
       "  1300,\n",
       "  327,\n",
       "  90,\n",
       "  4450,\n",
       "  243,\n",
       "  17,\n",
       "  217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 85],\n",
       " [1773, 1027, 1941, 498, 5862, 133, 17, 463, 217, 2110, 85],\n",
       " [175, 2366, 9211, 9212, 4288, 463, 133, 17, 217, 498, 694, 9213, 426],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [5, 137, 314, 17, 217, 2674, 1193, 4395, 813],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 19, 1587],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [5, 241, 984, 1138, 694, 764, 463, 983, 133, 17, 217, 731, 2113, 402],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [24, 398, 1035, 9214, 17, 217],\n",
       " [984, 897, 1528, 5863, 1027, 1941, 9215, 9216, 463, 133, 17, 217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [984, 897, 1528, 2366, 2114, 1139, 2115, 217, 133, 17, 9217],\n",
       " [3103,\n",
       "  694,\n",
       "  29,\n",
       "  217,\n",
       "  2364,\n",
       "  2116,\n",
       "  3683,\n",
       "  1427,\n",
       "  1340,\n",
       "  515,\n",
       "  133,\n",
       "  17,\n",
       "  199,\n",
       "  4451,\n",
       "  1317],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [9218, 3634, 9219, 4, 630, 17, 217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 85],\n",
       " [9220, 9221, 4, 4, 898, 618, 1169, 68, 60, 17, 217],\n",
       " [3104, 9222, 195, 4, 331, 17, 9223, 4, 898, 808, 31, 2676],\n",
       " [9224, 4452, 331, 17, 808, 1942, 80],\n",
       " [652, 77, 17, 331, 794],\n",
       " [9225, 9226, 652, 985, 891, 1943, 331, 40, 17, 986],\n",
       " [2677, 33, 4453, 577, 2367, 669, 652, 3047, 651, 17, 331, 2, 1009, 9227],\n",
       " [4454, 2117, 1223, 531, 2678, 794, 2679, 2, 1085, 1, 652, 17, 331, 125, 1404],\n",
       " [3105, 17, 331, 3521],\n",
       " [9228, 9229, 9230, 378, 1646, 17, 331],\n",
       " [667, 17, 331, 9231],\n",
       " [9232, 9233, 1341, 17, 331],\n",
       " [17, 331, 2368],\n",
       " [327, 695, 841, 1177, 24, 1422, 39, 9234, 17, 331, 5864],\n",
       " [9235, 9236, 3684, 819, 198, 17, 331, 1034, 9237],\n",
       " [17, 331, 41, 23, 119, 3685],\n",
       " [9238, 83, 331, 5865, 1109, 81, 791, 5866, 1647, 176],\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words=999999)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "sequence=tokenizer.texts_to_sequences(all_text)\n",
    "\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "722db1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:24.646469Z",
     "iopub.status.busy": "2024-06-02T10:22:24.646178Z",
     "iopub.status.idle": "2024-06-02T10:22:24.681613Z",
     "shell.execute_reply": "2024-06-02T10:22:24.680702Z"
    },
    "papermill": {
     "duration": 0.067229,
     "end_time": "2024-06-02T10:22:24.683618",
     "exception": false,
     "start_time": "2024-06-02T10:22:24.616389",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amp': 1,\n",
       " 'like': 2,\n",
       " 'just': 3,\n",
       " \"i'm\": 4,\n",
       " 'new': 5,\n",
       " 's': 6,\n",
       " 'news': 7,\n",
       " 'people': 8,\n",
       " \"it's\": 9,\n",
       " 'emergency': 10,\n",
       " \"don't\": 11,\n",
       " 'video': 12,\n",
       " 'disaster': 13,\n",
       " 'police': 14,\n",
       " 'u': 15,\n",
       " 'time': 16,\n",
       " 'body': 17,\n",
       " 'suicide': 18,\n",
       " 'rt': 19,\n",
       " 'storm': 20,\n",
       " 'burning': 21,\n",
       " 'crash': 22,\n",
       " 'got': 23,\n",
       " 'day': 24,\n",
       " 'attack': 25,\n",
       " 'know': 26,\n",
       " 'fires': 27,\n",
       " 'buildings': 28,\n",
       " 'man': 29,\n",
       " 'california': 30,\n",
       " 'going': 31,\n",
       " 'bomb': 32,\n",
       " 'w': 33,\n",
       " 'nuclear': 34,\n",
       " 'love': 35,\n",
       " 'world': 36,\n",
       " 'hiroshima': 37,\n",
       " 'pm': 38,\n",
       " 'year': 39,\n",
       " 'dead': 40,\n",
       " 'today': 41,\n",
       " 'youtube': 42,\n",
       " 'life': 43,\n",
       " 'watch': 44,\n",
       " 'old': 45,\n",
       " 'car': 46,\n",
       " 'm': 47,\n",
       " 'killed': 48,\n",
       " 'train': 49,\n",
       " 'think': 50,\n",
       " 'accident': 51,\n",
       " 'make': 52,\n",
       " 'war': 53,\n",
       " \"can't\": 54,\n",
       " 'good': 55,\n",
       " 'say': 56,\n",
       " 'gt': 57,\n",
       " 'way': 58,\n",
       " 't': 59,\n",
       " 'families': 60,\n",
       " 'years': 61,\n",
       " 'need': 62,\n",
       " 'want': 63,\n",
       " 'best': 64,\n",
       " 'bombing': 65,\n",
       " 'mass': 66,\n",
       " 'did': 67,\n",
       " 'home': 68,\n",
       " 'right': 69,\n",
       " 'collapse': 70,\n",
       " 'forest': 71,\n",
       " 'water': 72,\n",
       " 'death': 73,\n",
       " 'army': 74,\n",
       " 'work': 75,\n",
       " 'black': 76,\n",
       " 'really': 77,\n",
       " 'wildfire': 78,\n",
       " 'help': 79,\n",
       " 'hot': 80,\n",
       " 'lol': 81,\n",
       " 'mh': 82,\n",
       " 'let': 83,\n",
       " 'look': 84,\n",
       " 'read': 85,\n",
       " 'bomber': 86,\n",
       " 'live': 87,\n",
       " 'god': 88,\n",
       " 'fatal': 89,\n",
       " \"you're\": 90,\n",
       " 'th': 91,\n",
       " 'northern': 92,\n",
       " 'obama': 93,\n",
       " 'flood': 94,\n",
       " 'wild': 95,\n",
       " 'feel': 96,\n",
       " 'near': 97,\n",
       " 'school': 98,\n",
       " 'city': 99,\n",
       " 'night': 100,\n",
       " 'great': 101,\n",
       " 'stop': 102,\n",
       " 'japan': 103,\n",
       " 'injured': 104,\n",
       " 'shit': 105,\n",
       " 'said': 106,\n",
       " 'latest': 107,\n",
       " 'homes': 108,\n",
       " 'typhoon': 109,\n",
       " 'services': 110,\n",
       " 'hope': 111,\n",
       " 'floods': 112,\n",
       " 'im': 113,\n",
       " 'fear': 114,\n",
       " 'come': 115,\n",
       " 'atomic': 116,\n",
       " 'flames': 117,\n",
       " 'house': 118,\n",
       " 'post': 119,\n",
       " 'legionnaires': 120,\n",
       " 'getting': 121,\n",
       " 'truck': 122,\n",
       " 'state': 123,\n",
       " 'wreck': 124,\n",
       " 'ass': 125,\n",
       " 'damage': 126,\n",
       " 'x': 127,\n",
       " 'content': 128,\n",
       " 'p': 129,\n",
       " 'o': 130,\n",
       " 'red': 131,\n",
       " 'b': 132,\n",
       " 'cross': 133,\n",
       " 'oil': 134,\n",
       " 'earthquake': 135,\n",
       " 'coming': 136,\n",
       " 'summer': 137,\n",
       " 'weather': 138,\n",
       " 'change': 139,\n",
       " 'plan': 140,\n",
       " 'severe': 141,\n",
       " 'heat': 142,\n",
       " 'family': 143,\n",
       " 'military': 144,\n",
       " 'd': 145,\n",
       " 'debris': 146,\n",
       " 'little': 147,\n",
       " 'lightning': 148,\n",
       " 'thunderstorm': 149,\n",
       " 'evacuation': 150,\n",
       " 'gonna': 151,\n",
       " 'st': 152,\n",
       " 'hit': 153,\n",
       " 'food': 154,\n",
       " 'wounded': 155,\n",
       " 'natural': 156,\n",
       " 'devastated': 157,\n",
       " 'smoke': 158,\n",
       " 'set': 159,\n",
       " 'does': 160,\n",
       " 'face': 161,\n",
       " 'times': 162,\n",
       " 'destroyed': 163,\n",
       " 'photo': 164,\n",
       " 'rain': 165,\n",
       " 'free': 166,\n",
       " 'murder': 167,\n",
       " 'looks': 168,\n",
       " 'cause': 169,\n",
       " \"that's\": 170,\n",
       " 'national': 171,\n",
       " 'survive': 172,\n",
       " 'boy': 173,\n",
       " 'flooding': 174,\n",
       " 'check': 175,\n",
       " 'bad': 176,\n",
       " 'story': 177,\n",
       " 'injuries': 178,\n",
       " 'fucking': 179,\n",
       " 'bloody': 180,\n",
       " 'run': 181,\n",
       " 'liked': 182,\n",
       " 'terrorist': 183,\n",
       " 'says': 184,\n",
       " 'fall': 185,\n",
       " 'hurricane': 186,\n",
       " 'service': 187,\n",
       " 'spill': 188,\n",
       " 'trapped': 189,\n",
       " 'malaysia': 190,\n",
       " 'refugees': 191,\n",
       " 'area': 192,\n",
       " 'movie': 193,\n",
       " 'saudi': 194,\n",
       " 'oh': 195,\n",
       " 'failure': 196,\n",
       " 'loud': 197,\n",
       " 'game': 198,\n",
       " 'air': 199,\n",
       " 'hail': 200,\n",
       " 'girl': 201,\n",
       " 'terrorism': 202,\n",
       " 'boat': 203,\n",
       " 'landslide': 204,\n",
       " 'thunder': 205,\n",
       " \"i've\": 206,\n",
       " 'weapon': 207,\n",
       " 'drought': 208,\n",
       " 'confirmed': 209,\n",
       " 'rescue': 210,\n",
       " 'reddit': 211,\n",
       " 'breaking': 212,\n",
       " 'injury': 213,\n",
       " 'blood': 214,\n",
       " 'kills': 215,\n",
       " 'big': 216,\n",
       " 'bag': 217,\n",
       " 'wind': 218,\n",
       " 'report': 219,\n",
       " 'migrants': 220,\n",
       " 'survived': 221,\n",
       " 'week': 222,\n",
       " 'head': 223,\n",
       " 'burned': 224,\n",
       " 'explosion': 225,\n",
       " 'screaming': 226,\n",
       " 'destroy': 227,\n",
       " 'survivors': 228,\n",
       " 'warning': 229,\n",
       " 'released': 230,\n",
       " 'hostage': 231,\n",
       " 'weapons': 232,\n",
       " 'hazard': 233,\n",
       " 'panic': 234,\n",
       " 'r': 235,\n",
       " 'tonight': 236,\n",
       " 'destruction': 237,\n",
       " 'apocalypse': 238,\n",
       " 'missing': 239,\n",
       " 'attacked': 240,\n",
       " 'women': 241,\n",
       " 'rescued': 242,\n",
       " 'end': 243,\n",
       " 'past': 244,\n",
       " \"i'll\": 245,\n",
       " 'heard': 246,\n",
       " 'save': 247,\n",
       " 'high': 248,\n",
       " 'fuck': 249,\n",
       " 'real': 250,\n",
       " 'bridge': 251,\n",
       " 'n': 252,\n",
       " 'thing': 253,\n",
       " 'trauma': 254,\n",
       " 'ok': 255,\n",
       " 'violent': 256,\n",
       " 'mosque': 257,\n",
       " 'bags': 258,\n",
       " 'rescuers': 259,\n",
       " 'sinking': 260,\n",
       " 'responders': 261,\n",
       " 'hundreds': 262,\n",
       " 'deaths': 263,\n",
       " 'sinkhole': 264,\n",
       " 'update': 265,\n",
       " 'wave': 266,\n",
       " 'island': 267,\n",
       " 'crashed': 268,\n",
       " \"we're\": 269,\n",
       " 'self': 270,\n",
       " 'saw': 271,\n",
       " 'catastrophic': 272,\n",
       " 'collapsed': 273,\n",
       " 'demolished': 274,\n",
       " 'drowning': 275,\n",
       " 'bang': 276,\n",
       " 'county': 277,\n",
       " 'meltdown': 278,\n",
       " 'ambulance': 279,\n",
       " \"he's\": 280,\n",
       " 'river': 281,\n",
       " 'white': 282,\n",
       " 'c': 283,\n",
       " 'blown': 284,\n",
       " 'explode': 285,\n",
       " 'displaced': 286,\n",
       " 'evacuate': 287,\n",
       " 'hazardous': 288,\n",
       " 'collision': 289,\n",
       " 'crush': 290,\n",
       " 'stock': 291,\n",
       " 'dust': 292,\n",
       " 'massacre': 293,\n",
       " 'lava': 294,\n",
       " 'use': 295,\n",
       " 'airplane': 296,\n",
       " 'away': 297,\n",
       " 'august': 298,\n",
       " 'charged': 299,\n",
       " 'light': 300,\n",
       " 'market': 301,\n",
       " 'drown': 302,\n",
       " 'bombed': 303,\n",
       " 'lives': 304,\n",
       " 'riot': 305,\n",
       " 'traumatised': 306,\n",
       " 'derailment': 307,\n",
       " 'catastrophe': 308,\n",
       " 'collided': 309,\n",
       " 'trouble': 310,\n",
       " 'screamed': 311,\n",
       " 'outbreak': 312,\n",
       " 'structural': 313,\n",
       " 'long': 314,\n",
       " 'battle': 315,\n",
       " 'bleeding': 316,\n",
       " 'blew': 317,\n",
       " 'tragedy': 318,\n",
       " 'ruin': 319,\n",
       " 'crushed': 320,\n",
       " 'wounds': 321,\n",
       " 'electrocuted': 322,\n",
       " 'exploded': 323,\n",
       " 'wreckage': 324,\n",
       " 'quarantine': 325,\n",
       " 'screams': 326,\n",
       " 'better': 327,\n",
       " 'suspect': 328,\n",
       " 'arson': 329,\n",
       " 'group': 330,\n",
       " 'bagging': 331,\n",
       " 'baby': 332,\n",
       " 'anniversary': 333,\n",
       " 'yr': 334,\n",
       " 'caused': 335,\n",
       " 'cliff': 336,\n",
       " 'curfew': 337,\n",
       " 'cyclone': 338,\n",
       " 'blast': 339,\n",
       " 'deluged': 340,\n",
       " 'devastation': 341,\n",
       " 'engulfed': 342,\n",
       " 'investigators': 343,\n",
       " 'flattened': 344,\n",
       " 'inundated': 345,\n",
       " 'obliteration': 346,\n",
       " 'panicking': 347,\n",
       " 'quarantined': 348,\n",
       " 'windstorm': 349,\n",
       " \"there's\": 350,\n",
       " 'bus': 351,\n",
       " 'i': 352,\n",
       " 'horrible': 353,\n",
       " 'road': 354,\n",
       " 'sirens': 355,\n",
       " 'thought': 356,\n",
       " 'e': 357,\n",
       " 'half': 358,\n",
       " 'rioting': 359,\n",
       " 'casualties': 360,\n",
       " 'chemical': 361,\n",
       " 'harm': 362,\n",
       " 'danger': 363,\n",
       " 'derailed': 364,\n",
       " 'detonation': 365,\n",
       " 'volcano': 366,\n",
       " 'hostages': 367,\n",
       " 'mudslide': 368,\n",
       " 'obliterate': 369,\n",
       " 'obliterated': 370,\n",
       " 'sandstorm': 371,\n",
       " 'sunk': 372,\n",
       " 'twister': 373,\n",
       " 'wrecked': 374,\n",
       " 'minute': 375,\n",
       " 'sure': 376,\n",
       " 'went': 377,\n",
       " 'twitter': 378,\n",
       " 'calgary': 379,\n",
       " 'power': 380,\n",
       " 'fedex': 381,\n",
       " 'bioterrorism': 382,\n",
       " 'security': 383,\n",
       " 'send': 384,\n",
       " 'collide': 385,\n",
       " 'demolish': 386,\n",
       " 'derail': 387,\n",
       " 'desolation': 388,\n",
       " 'detonate': 389,\n",
       " 'seismic': 390,\n",
       " 'famine': 391,\n",
       " 'hijacker': 392,\n",
       " 'hijacking': 393,\n",
       " 'rainstorm': 394,\n",
       " 'razed': 395,\n",
       " 'died': 396,\n",
       " 'phone': 397,\n",
       " 'heart': 398,\n",
       " 'woman': 399,\n",
       " 'abc': 400,\n",
       " 'blazing': 401,\n",
       " 're': 402,\n",
       " 'line': 403,\n",
       " 'pkk': 404,\n",
       " 'tsunami': 405,\n",
       " 'eyewitness': 406,\n",
       " 'stretcher': 407,\n",
       " 'whirlwind': 408,\n",
       " 'tornado': 409,\n",
       " 'came': 410,\n",
       " 'airport': 411,\n",
       " 'things': 412,\n",
       " 'longer': 413,\n",
       " 'song': 414,\n",
       " 'ur': 415,\n",
       " 'casualty': 416,\n",
       " 'electrocute': 417,\n",
       " 'fatality': 418,\n",
       " 'pandemonium': 419,\n",
       " 'rubble': 420,\n",
       " 'snowstorm': 421,\n",
       " 'detonated': 422,\n",
       " 'south': 423,\n",
       " 'kids': 424,\n",
       " 'gets': 425,\n",
       " 'ebay': 426,\n",
       " 'iran': 427,\n",
       " 'zone': 428,\n",
       " 'armageddon': 429,\n",
       " 'kill': 430,\n",
       " 'fan': 431,\n",
       " 'sound': 432,\n",
       " 'trying': 433,\n",
       " 'aug': 434,\n",
       " 'affected': 435,\n",
       " 'drowned': 436,\n",
       " 'evacuated': 437,\n",
       " 'fatalities': 438,\n",
       " 'hijack': 439,\n",
       " 'murderer': 440,\n",
       " 'building': 441,\n",
       " 'far': 442,\n",
       " 'ablaze': 443,\n",
       " 'used': 444,\n",
       " 'goes': 445,\n",
       " 'government': 446,\n",
       " 'annihilated': 447,\n",
       " 'fight': 448,\n",
       " 'soon': 449,\n",
       " 'remember': 450,\n",
       " 'health': 451,\n",
       " 'blight': 452,\n",
       " 'turkey': 453,\n",
       " 'deluge': 454,\n",
       " 'demolition': 455,\n",
       " 'days': 456,\n",
       " 'shooting': 457,\n",
       " 'tomorrow': 458,\n",
       " 'avalanche': 459,\n",
       " 'music': 460,\n",
       " 'bioterror': 461,\n",
       " 'land': 462,\n",
       " 'shoulder': 463,\n",
       " 'isis': 464,\n",
       " 'thanks': 465,\n",
       " 'tell': 466,\n",
       " 'left': 467,\n",
       " 'possible': 468,\n",
       " 'play': 469,\n",
       " 'plane': 470,\n",
       " 'annihilation': 471,\n",
       " 'stand': 472,\n",
       " 'india': 473,\n",
       " 'media': 474,\n",
       " 'wanna': 475,\n",
       " 'low': 476,\n",
       " 'officer': 477,\n",
       " 'cool': 478,\n",
       " 'west': 479,\n",
       " 'rd': 480,\n",
       " 'actually': 481,\n",
       " 'issues': 482,\n",
       " 'doing': 483,\n",
       " 'reunion': 484,\n",
       " 'men': 485,\n",
       " 'believe': 486,\n",
       " 'lot': 487,\n",
       " 'hell': 488,\n",
       " 'order': 489,\n",
       " 'start': 490,\n",
       " \"doesn't\": 491,\n",
       " 'pic': 492,\n",
       " 'desolate': 493,\n",
       " 'hellfire': 494,\n",
       " 'prebreak': 495,\n",
       " 'site': 496,\n",
       " 'care': 497,\n",
       " 'brown': 498,\n",
       " 'fun': 499,\n",
       " 'hours': 500,\n",
       " 'caught': 501,\n",
       " 'stay': 502,\n",
       " 'officials': 503,\n",
       " 'israeli': 504,\n",
       " 'person': 505,\n",
       " 'yes': 506,\n",
       " 'bush': 507,\n",
       " 'hailstorm': 508,\n",
       " 'mayhem': 509,\n",
       " 'upheaval': 510,\n",
       " 'second': 511,\n",
       " 'america': 512,\n",
       " 'die': 513,\n",
       " 'ago': 514,\n",
       " 'case': 515,\n",
       " 'nearby': 516,\n",
       " 'mp': 517,\n",
       " 'km': 518,\n",
       " 'offensive': 519,\n",
       " 'street': 520,\n",
       " 'thank': 521,\n",
       " 'israel': 522,\n",
       " 'horror': 523,\n",
       " 'park': 524,\n",
       " 'damn': 525,\n",
       " 'deal': 526,\n",
       " 'rise': 527,\n",
       " 'siren': 528,\n",
       " 'wait': 529,\n",
       " 'try': 530,\n",
       " 'north': 531,\n",
       " 'traffic': 532,\n",
       " 'support': 533,\n",
       " 'plans': 534,\n",
       " 'making': 535,\n",
       " 'wake': 536,\n",
       " 'hear': 537,\n",
       " 'china': 538,\n",
       " 'reactor': 539,\n",
       " 'lt': 540,\n",
       " 'history': 541,\n",
       " 'data': 542,\n",
       " 'pick': 543,\n",
       " 'american': 544,\n",
       " 'yeah': 545,\n",
       " 'pretty': 546,\n",
       " 'policy': 547,\n",
       " 'blizzard': 548,\n",
       " 'pay': 549,\n",
       " 'wow': 550,\n",
       " 'watching': 551,\n",
       " 'takes': 552,\n",
       " 'declares': 553,\n",
       " 'hat': 554,\n",
       " 'swallowed': 555,\n",
       " 'inside': 556,\n",
       " 'happy': 557,\n",
       " \"didn't\": 558,\n",
       " 'aftershock': 559,\n",
       " 'eyes': 560,\n",
       " 'omg': 561,\n",
       " 'reuters': 562,\n",
       " 'peace': 563,\n",
       " \"i'd\": 564,\n",
       " 'children': 565,\n",
       " 'tv': 566,\n",
       " 'having': 567,\n",
       " 'listen': 568,\n",
       " 'words': 569,\n",
       " 'emmerdale': 570,\n",
       " 'saipan': 571,\n",
       " 'trench': 572,\n",
       " 'sue': 573,\n",
       " 'conclusively': 574,\n",
       " 'place': 575,\n",
       " 'shot': 576,\n",
       " 'k': 577,\n",
       " 'finally': 578,\n",
       " 'guy': 579,\n",
       " 'feared': 580,\n",
       " 'crazy': 581,\n",
       " 'maybe': 582,\n",
       " 'makes': 583,\n",
       " 'team': 584,\n",
       " 'fukushima': 585,\n",
       " 'lab': 586,\n",
       " 'money': 587,\n",
       " 'bigger': 588,\n",
       " 'memories': 589,\n",
       " 'waves': 590,\n",
       " 'effect': 591,\n",
       " \"what's\": 592,\n",
       " 'outside': 593,\n",
       " 'arsonist': 594,\n",
       " 'helicopter': 595,\n",
       " 'sorry': 596,\n",
       " 'seen': 597,\n",
       " 'bc': 598,\n",
       " 'bar': 599,\n",
       " 'crisis': 600,\n",
       " 'business': 601,\n",
       " 'literally': 602,\n",
       " 'guys': 603,\n",
       " 'beautiful': 604,\n",
       " 'australia': 605,\n",
       " 'transport': 606,\n",
       " 'morning': 607,\n",
       " 'probably': 608,\n",
       " 'photos': 609,\n",
       " 'houses': 610,\n",
       " 'bbc': 611,\n",
       " 'lost': 612,\n",
       " 'huge': 613,\n",
       " 'mom': 614,\n",
       " 'property': 615,\n",
       " 'happened': 616,\n",
       " 'book': 617,\n",
       " \"they're\": 618,\n",
       " 'job': 619,\n",
       " 'pakistan': 620,\n",
       " 'called': 621,\n",
       " \"isn't\": 622,\n",
       " 'ca': 623,\n",
       " 'v': 624,\n",
       " 'dont': 625,\n",
       " 'searching': 626,\n",
       " 'saved': 627,\n",
       " 'soudelor': 628,\n",
       " 'bestnaijamade': 629,\n",
       " 'leave': 630,\n",
       " 'hate': 631,\n",
       " 'hey': 632,\n",
       " 'blaze': 633,\n",
       " 'muslims': 634,\n",
       " 'public': 635,\n",
       " 'pray': 636,\n",
       " 'likely': 637,\n",
       " 'crews': 638,\n",
       " 'knock': 639,\n",
       " 'sensor': 640,\n",
       " 'nowplaying': 641,\n",
       " 'center': 642,\n",
       " 'salt': 643,\n",
       " 'control': 644,\n",
       " 'truth': 645,\n",
       " 'amid': 646,\n",
       " 'local': 647,\n",
       " 'taken': 648,\n",
       " 'hollywood': 649,\n",
       " 'follow': 650,\n",
       " 'level': 651,\n",
       " 'drake': 652,\n",
       " 'spot': 653,\n",
       " 'marks': 654,\n",
       " 'businesses': 655,\n",
       " 'room': 656,\n",
       " 'bodies': 657,\n",
       " 'signs': 658,\n",
       " 'islam': 659,\n",
       " 'manslaughter': 660,\n",
       " 'reason': 661,\n",
       " 'fast': 662,\n",
       " 'flag': 663,\n",
       " 'chicago': 664,\n",
       " 'talk': 665,\n",
       " 'aircraft': 666,\n",
       " 'g': 667,\n",
       " 'dog': 668,\n",
       " 'ball': 669,\n",
       " 'major': 670,\n",
       " 'l': 671,\n",
       " 'town': 672,\n",
       " 'green': 673,\n",
       " 'texas': 674,\n",
       " 'playing': 675,\n",
       " 'anthrax': 676,\n",
       " 'running': 677,\n",
       " 'computers': 678,\n",
       " 'mad': 679,\n",
       " 'link': 680,\n",
       " 'appears': 681,\n",
       " 'giant': 682,\n",
       " 'course': 683,\n",
       " 'quiz': 684,\n",
       " 'utc': 685,\n",
       " 'projected': 686,\n",
       " 'led': 687,\n",
       " 'gems': 688,\n",
       " 'la': 689,\n",
       " 'bring': 690,\n",
       " 'wrong': 691,\n",
       " 'country': 692,\n",
       " 'horse': 693,\n",
       " 'leather': 694,\n",
       " 'feeling': 695,\n",
       " 'king': 696,\n",
       " 'miss': 697,\n",
       " 'f': 698,\n",
       " 'nearly': 699,\n",
       " \"let's\": 700,\n",
       " 'russian': 701,\n",
       " 'trains': 702,\n",
       " 'uk': 703,\n",
       " 'flight': 704,\n",
       " 'image': 705,\n",
       " 'banned': 706,\n",
       " 'ignition': 707,\n",
       " 'virgin': 708,\n",
       " 'closed': 709,\n",
       " 'heavy': 710,\n",
       " 'flash': 711,\n",
       " 'lord': 712,\n",
       " 'hard': 713,\n",
       " 'climate': 714,\n",
       " 'vehicle': 715,\n",
       " 'win': 716,\n",
       " 'safety': 717,\n",
       " 'child': 718,\n",
       " 'radio': 719,\n",
       " 'comes': 720,\n",
       " 'action': 721,\n",
       " 'east': 722,\n",
       " 'reports': 723,\n",
       " 'shift': 724,\n",
       " 'christian': 725,\n",
       " 'worst': 726,\n",
       " 'needs': 727,\n",
       " 'eye': 728,\n",
       " 'space': 729,\n",
       " 'young': 730,\n",
       " 'large': 731,\n",
       " 'thursday': 732,\n",
       " 'mph': 733,\n",
       " 'issued': 734,\n",
       " 'cnn': 735,\n",
       " 'ancient': 736,\n",
       " 'refugio': 737,\n",
       " 'costlier': 738,\n",
       " 'haha': 739,\n",
       " 'cars': 740,\n",
       " 'thousands': 741,\n",
       " 'michael': 742,\n",
       " 'sign': 743,\n",
       " 'film': 744,\n",
       " 'class': 745,\n",
       " 'claims': 746,\n",
       " 'militants': 747,\n",
       " 'okay': 748,\n",
       " 'temple': 749,\n",
       " 'mount': 750,\n",
       " 'piece': 751,\n",
       " 'driving': 752,\n",
       " 'favorite': 753,\n",
       " 'star': 754,\n",
       " 'nd': 755,\n",
       " 'germs': 756,\n",
       " 'mishaps': 757,\n",
       " 'online': 758,\n",
       " 'entire': 759,\n",
       " 'told': 760,\n",
       " 'open': 761,\n",
       " 'super': 762,\n",
       " 'sad': 763,\n",
       " 'ladies': 764,\n",
       " 'village': 765,\n",
       " 'human': 766,\n",
       " 'insurance': 767,\n",
       " 'strong': 768,\n",
       " 'don': 769,\n",
       " 'picking': 770,\n",
       " 'funtenna': 771,\n",
       " 'unconfirmed': 772,\n",
       " \"neighbour's\": 773,\n",
       " \"china's\": 774,\n",
       " 'galactic': 775,\n",
       " 'miners': 776,\n",
       " 'taking': 777,\n",
       " 'daily': 778,\n",
       " 'moment': 779,\n",
       " 'official': 780,\n",
       " \"won't\": 781,\n",
       " 'usa': 782,\n",
       " 'a': 783,\n",
       " 'hand': 784,\n",
       " 'united': 785,\n",
       " 'learn': 786,\n",
       " 'true': 787,\n",
       " 'youth': 788,\n",
       " 'looking': 789,\n",
       " 'firefighters': 790,\n",
       " \"ain't\": 791,\n",
       " 'buy': 792,\n",
       " 'rock': 793,\n",
       " 'meek': 794,\n",
       " 'gbbo': 795,\n",
       " 'alarm': 796,\n",
       " 'friends': 797,\n",
       " 'apollo': 798,\n",
       " 'declaration': 799,\n",
       " 'disea': 800,\n",
       " 'season': 801,\n",
       " 'drive': 802,\n",
       " 'global': 803,\n",
       " 'thinking': 804,\n",
       " 'jobs': 805,\n",
       " 'tried': 806,\n",
       " 'experts': 807,\n",
       " \"she's\": 808,\n",
       " 'following': 809,\n",
       " 'earth': 810,\n",
       " 'toddler': 811,\n",
       " 'friend': 812,\n",
       " 'blue': 813,\n",
       " 'date': 814,\n",
       " 'suspected': 815,\n",
       " 'hiring': 816,\n",
       " 'added': 817,\n",
       " 'cut': 818,\n",
       " 'problem': 819,\n",
       " 'ebola': 820,\n",
       " 'living': 821,\n",
       " 'pain': 822,\n",
       " 'washington': 823,\n",
       " 'libya': 824,\n",
       " 'islamic': 825,\n",
       " 'brother': 826,\n",
       " 'sounds': 827,\n",
       " 'ppl': 828,\n",
       " 'angry': 829,\n",
       " 'coaches': 830,\n",
       " 'wonder': 831,\n",
       " 'break': 832,\n",
       " 'cree': 833,\n",
       " \"reddit's\": 834,\n",
       " 'subreddits': 835,\n",
       " 'myanmar': 836,\n",
       " 'awesome': 837,\n",
       " 'wanted': 838,\n",
       " 'happen': 839,\n",
       " 'scene': 840,\n",
       " 'seeing': 841,\n",
       " 'france': 842,\n",
       " 'members': 843,\n",
       " 'ship': 844,\n",
       " 'close': 845,\n",
       " 'early': 846,\n",
       " 'pile': 847,\n",
       " 'potus': 848,\n",
       " 'share': 849,\n",
       " 'poor': 850,\n",
       " 'victims': 851,\n",
       " 'shows': 852,\n",
       " 'beach': 853,\n",
       " 'terror': 854,\n",
       " 'view': 855,\n",
       " 'tweet': 856,\n",
       " 'lady': 857,\n",
       " 'loving': 858,\n",
       " 'womens': 859,\n",
       " 'centre': 860,\n",
       " 'ashes': 861,\n",
       " 'social': 862,\n",
       " 'press': 863,\n",
       " 'chance': 864,\n",
       " 'nagasaki': 865,\n",
       " 'york': 866,\n",
       " 'shape': 867,\n",
       " 'internally': 868,\n",
       " 'nigerian': 869,\n",
       " 'london': 870,\n",
       " 'wife': 871,\n",
       " 'sleeping': 872,\n",
       " 'double': 873,\n",
       " 'couple': 874,\n",
       " 'driver': 875,\n",
       " 'h': 876,\n",
       " 'fact': 877,\n",
       " 'total': 878,\n",
       " 'join': 879,\n",
       " 'dad': 880,\n",
       " 'escape': 881,\n",
       " 'theater': 882,\n",
       " 'gun': 883,\n",
       " 'gop': 884,\n",
       " 'pamela': 885,\n",
       " 'research': 886,\n",
       " 'silver': 887,\n",
       " 'turn': 888,\n",
       " 'cold': 889,\n",
       " 'sun': 890,\n",
       " 'dude': 891,\n",
       " 'cake': 892,\n",
       " 'text': 893,\n",
       " 'info': 894,\n",
       " 'art': 895,\n",
       " 'fans': 896,\n",
       " 'fashion': 897,\n",
       " 'saying': 898,\n",
       " 'holding': 899,\n",
       " 'ross': 900,\n",
       " 'sex': 901,\n",
       " 'absolutely': 902,\n",
       " 'landing': 903,\n",
       " 'sea': 904,\n",
       " 'patience': 905,\n",
       " 'madhya': 906,\n",
       " 'pradesh': 907,\n",
       " 'nws': 908,\n",
       " 'bayelsa': 909,\n",
       " 'rly': 910,\n",
       " 'chile': 911,\n",
       " 'causes': 912,\n",
       " 'colorado': 913,\n",
       " 'aba': 914,\n",
       " 'ave': 915,\n",
       " 'reported': 916,\n",
       " 'risk': 917,\n",
       " 'dies': 918,\n",
       " 'guess': 919,\n",
       " 'wednesday': 920,\n",
       " 'bit': 921,\n",
       " 'trust': 922,\n",
       " 'ready': 923,\n",
       " \"wasn't\": 924,\n",
       " 'vs': 925,\n",
       " 'disease': 926,\n",
       " 'started': 927,\n",
       " 'took': 928,\n",
       " 'series': 929,\n",
       " 'till': 930,\n",
       " 'russia': 931,\n",
       " 'direction': 932,\n",
       " 'udhampur': 933,\n",
       " 'gave': 934,\n",
       " 'bombs': 935,\n",
       " 'waving': 936,\n",
       " 'geller': 937,\n",
       " 'playlist': 938,\n",
       " 'cover': 939,\n",
       " 'ahead': 940,\n",
       " 'event': 941,\n",
       " 'party': 942,\n",
       " 'giving': 943,\n",
       " 'soul': 944,\n",
       " 'biggest': 945,\n",
       " 'wall': 946,\n",
       " 'record': 947,\n",
       " 'small': 948,\n",
       " 'middle': 949,\n",
       " 'central': 950,\n",
       " 'outrage': 951,\n",
       " 'downtown': 952,\n",
       " 'camp': 953,\n",
       " 'british': 954,\n",
       " 'instead': 955,\n",
       " 'tree': 956,\n",
       " 'download': 957,\n",
       " 'interesting': 958,\n",
       " 'spring': 959,\n",
       " 'passengers': 960,\n",
       " 'information': 961,\n",
       " 'brooklyn': 962,\n",
       " 'rules': 963,\n",
       " 'offroad': 964,\n",
       " 'lamp': 965,\n",
       " 'shots': 966,\n",
       " 'means': 967,\n",
       " 'month': 968,\n",
       " 'secret': 969,\n",
       " 'david': 970,\n",
       " 'pakistani': 971,\n",
       " 'episode': 972,\n",
       " 'short': 973,\n",
       " 'mod': 974,\n",
       " 'totally': 975,\n",
       " 'beat': 976,\n",
       " 'japanese': 977,\n",
       " 'burn': 978,\n",
       " 'sos': 979,\n",
       " 'album': 980,\n",
       " 'lies': 981,\n",
       " 'metro': 982,\n",
       " 'tote': 983,\n",
       " 'handbag': 984,\n",
       " 'killing': 985,\n",
       " 'point': 986,\n",
       " 'colour': 987,\n",
       " 'you': 988,\n",
       " 'planned': 989,\n",
       " 'account': 990,\n",
       " 'denver': 991,\n",
       " 'delivers': 992,\n",
       " 'international': 993,\n",
       " 'marians': 994,\n",
       " 'sick': 995,\n",
       " 'travel': 996,\n",
       " 'apc': 997,\n",
       " 'parole': 998,\n",
       " 'recount': 999,\n",
       " 'sky': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577c1eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:24.744250Z",
     "iopub.status.busy": "2024-06-02T10:22:24.743997Z",
     "iopub.status.idle": "2024-06-02T10:22:24.749359Z",
     "shell.execute_reply": "2024-06-02T10:22:24.748465Z"
    },
    "papermill": {
     "duration": 0.037622,
     "end_time": "2024-06-02T10:22:24.751208",
     "exception": false,
     "start_time": "2024-06-02T10:22:24.713586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20575"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf488ca",
   "metadata": {
    "papermill": {
     "duration": 0.029526,
     "end_time": "2024-06-02T10:22:24.810450",
     "exception": false,
     "start_time": "2024-06-02T10:22:24.780924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see that there is 20575 different words in all texts (including training and testing texts). So, we adjust the num_words parameter correspondingly and tokenize all the texts again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76dfe5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:24.872396Z",
     "iopub.status.busy": "2024-06-02T10:22:24.871735Z",
     "iopub.status.idle": "2024-06-02T10:22:25.255332Z",
     "shell.execute_reply": "2024-06-02T10:22:25.254458Z"
    },
    "papermill": {
     "duration": 0.417439,
     "end_time": "2024-06-02T10:22:25.258053",
     "exception": false,
     "start_time": "2024-06-02T10:22:24.840614",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amp': 1,\n",
       " 'like': 2,\n",
       " 'just': 3,\n",
       " \"i'm\": 4,\n",
       " 'new': 5,\n",
       " 's': 6,\n",
       " 'news': 7,\n",
       " 'people': 8,\n",
       " \"it's\": 9,\n",
       " 'emergency': 10,\n",
       " \"don't\": 11,\n",
       " 'video': 12,\n",
       " 'disaster': 13,\n",
       " 'police': 14,\n",
       " 'u': 15,\n",
       " 'time': 16,\n",
       " 'body': 17,\n",
       " 'suicide': 18,\n",
       " 'rt': 19,\n",
       " 'storm': 20,\n",
       " 'burning': 21,\n",
       " 'crash': 22,\n",
       " 'got': 23,\n",
       " 'day': 24,\n",
       " 'attack': 25,\n",
       " 'know': 26,\n",
       " 'fires': 27,\n",
       " 'buildings': 28,\n",
       " 'man': 29,\n",
       " 'california': 30,\n",
       " 'going': 31,\n",
       " 'bomb': 32,\n",
       " 'w': 33,\n",
       " 'nuclear': 34,\n",
       " 'love': 35,\n",
       " 'world': 36,\n",
       " 'hiroshima': 37,\n",
       " 'pm': 38,\n",
       " 'year': 39,\n",
       " 'dead': 40,\n",
       " 'today': 41,\n",
       " 'youtube': 42,\n",
       " 'life': 43,\n",
       " 'watch': 44,\n",
       " 'old': 45,\n",
       " 'car': 46,\n",
       " 'm': 47,\n",
       " 'killed': 48,\n",
       " 'train': 49,\n",
       " 'think': 50,\n",
       " 'accident': 51,\n",
       " 'make': 52,\n",
       " 'war': 53,\n",
       " \"can't\": 54,\n",
       " 'good': 55,\n",
       " 'say': 56,\n",
       " 'gt': 57,\n",
       " 'way': 58,\n",
       " 't': 59,\n",
       " 'families': 60,\n",
       " 'years': 61,\n",
       " 'need': 62,\n",
       " 'want': 63,\n",
       " 'best': 64,\n",
       " 'bombing': 65,\n",
       " 'mass': 66,\n",
       " 'did': 67,\n",
       " 'home': 68,\n",
       " 'right': 69,\n",
       " 'collapse': 70,\n",
       " 'forest': 71,\n",
       " 'water': 72,\n",
       " 'death': 73,\n",
       " 'army': 74,\n",
       " 'work': 75,\n",
       " 'black': 76,\n",
       " 'really': 77,\n",
       " 'wildfire': 78,\n",
       " 'help': 79,\n",
       " 'hot': 80,\n",
       " 'lol': 81,\n",
       " 'mh': 82,\n",
       " 'let': 83,\n",
       " 'look': 84,\n",
       " 'read': 85,\n",
       " 'bomber': 86,\n",
       " 'live': 87,\n",
       " 'god': 88,\n",
       " 'fatal': 89,\n",
       " \"you're\": 90,\n",
       " 'th': 91,\n",
       " 'northern': 92,\n",
       " 'obama': 93,\n",
       " 'flood': 94,\n",
       " 'wild': 95,\n",
       " 'feel': 96,\n",
       " 'near': 97,\n",
       " 'school': 98,\n",
       " 'city': 99,\n",
       " 'night': 100,\n",
       " 'great': 101,\n",
       " 'stop': 102,\n",
       " 'japan': 103,\n",
       " 'injured': 104,\n",
       " 'shit': 105,\n",
       " 'said': 106,\n",
       " 'latest': 107,\n",
       " 'homes': 108,\n",
       " 'typhoon': 109,\n",
       " 'services': 110,\n",
       " 'hope': 111,\n",
       " 'floods': 112,\n",
       " 'im': 113,\n",
       " 'fear': 114,\n",
       " 'come': 115,\n",
       " 'atomic': 116,\n",
       " 'flames': 117,\n",
       " 'house': 118,\n",
       " 'post': 119,\n",
       " 'legionnaires': 120,\n",
       " 'getting': 121,\n",
       " 'truck': 122,\n",
       " 'state': 123,\n",
       " 'wreck': 124,\n",
       " 'ass': 125,\n",
       " 'damage': 126,\n",
       " 'x': 127,\n",
       " 'content': 128,\n",
       " 'p': 129,\n",
       " 'o': 130,\n",
       " 'red': 131,\n",
       " 'b': 132,\n",
       " 'cross': 133,\n",
       " 'oil': 134,\n",
       " 'earthquake': 135,\n",
       " 'coming': 136,\n",
       " 'summer': 137,\n",
       " 'weather': 138,\n",
       " 'change': 139,\n",
       " 'plan': 140,\n",
       " 'severe': 141,\n",
       " 'heat': 142,\n",
       " 'family': 143,\n",
       " 'military': 144,\n",
       " 'd': 145,\n",
       " 'debris': 146,\n",
       " 'little': 147,\n",
       " 'lightning': 148,\n",
       " 'thunderstorm': 149,\n",
       " 'evacuation': 150,\n",
       " 'gonna': 151,\n",
       " 'st': 152,\n",
       " 'hit': 153,\n",
       " 'food': 154,\n",
       " 'wounded': 155,\n",
       " 'natural': 156,\n",
       " 'devastated': 157,\n",
       " 'smoke': 158,\n",
       " 'set': 159,\n",
       " 'does': 160,\n",
       " 'face': 161,\n",
       " 'times': 162,\n",
       " 'destroyed': 163,\n",
       " 'photo': 164,\n",
       " 'rain': 165,\n",
       " 'free': 166,\n",
       " 'murder': 167,\n",
       " 'looks': 168,\n",
       " 'cause': 169,\n",
       " \"that's\": 170,\n",
       " 'national': 171,\n",
       " 'survive': 172,\n",
       " 'boy': 173,\n",
       " 'flooding': 174,\n",
       " 'check': 175,\n",
       " 'bad': 176,\n",
       " 'story': 177,\n",
       " 'injuries': 178,\n",
       " 'fucking': 179,\n",
       " 'bloody': 180,\n",
       " 'run': 181,\n",
       " 'liked': 182,\n",
       " 'terrorist': 183,\n",
       " 'says': 184,\n",
       " 'fall': 185,\n",
       " 'hurricane': 186,\n",
       " 'service': 187,\n",
       " 'spill': 188,\n",
       " 'trapped': 189,\n",
       " 'malaysia': 190,\n",
       " 'refugees': 191,\n",
       " 'area': 192,\n",
       " 'movie': 193,\n",
       " 'saudi': 194,\n",
       " 'oh': 195,\n",
       " 'failure': 196,\n",
       " 'loud': 197,\n",
       " 'game': 198,\n",
       " 'air': 199,\n",
       " 'hail': 200,\n",
       " 'girl': 201,\n",
       " 'terrorism': 202,\n",
       " 'boat': 203,\n",
       " 'landslide': 204,\n",
       " 'thunder': 205,\n",
       " \"i've\": 206,\n",
       " 'weapon': 207,\n",
       " 'drought': 208,\n",
       " 'confirmed': 209,\n",
       " 'rescue': 210,\n",
       " 'reddit': 211,\n",
       " 'breaking': 212,\n",
       " 'injury': 213,\n",
       " 'blood': 214,\n",
       " 'kills': 215,\n",
       " 'big': 216,\n",
       " 'bag': 217,\n",
       " 'wind': 218,\n",
       " 'report': 219,\n",
       " 'migrants': 220,\n",
       " 'survived': 221,\n",
       " 'week': 222,\n",
       " 'head': 223,\n",
       " 'burned': 224,\n",
       " 'explosion': 225,\n",
       " 'screaming': 226,\n",
       " 'destroy': 227,\n",
       " 'survivors': 228,\n",
       " 'warning': 229,\n",
       " 'released': 230,\n",
       " 'hostage': 231,\n",
       " 'weapons': 232,\n",
       " 'hazard': 233,\n",
       " 'panic': 234,\n",
       " 'r': 235,\n",
       " 'tonight': 236,\n",
       " 'destruction': 237,\n",
       " 'apocalypse': 238,\n",
       " 'missing': 239,\n",
       " 'attacked': 240,\n",
       " 'women': 241,\n",
       " 'rescued': 242,\n",
       " 'end': 243,\n",
       " 'past': 244,\n",
       " \"i'll\": 245,\n",
       " 'heard': 246,\n",
       " 'save': 247,\n",
       " 'high': 248,\n",
       " 'fuck': 249,\n",
       " 'real': 250,\n",
       " 'bridge': 251,\n",
       " 'n': 252,\n",
       " 'thing': 253,\n",
       " 'trauma': 254,\n",
       " 'ok': 255,\n",
       " 'violent': 256,\n",
       " 'mosque': 257,\n",
       " 'bags': 258,\n",
       " 'rescuers': 259,\n",
       " 'sinking': 260,\n",
       " 'responders': 261,\n",
       " 'hundreds': 262,\n",
       " 'deaths': 263,\n",
       " 'sinkhole': 264,\n",
       " 'update': 265,\n",
       " 'wave': 266,\n",
       " 'island': 267,\n",
       " 'crashed': 268,\n",
       " \"we're\": 269,\n",
       " 'self': 270,\n",
       " 'saw': 271,\n",
       " 'catastrophic': 272,\n",
       " 'collapsed': 273,\n",
       " 'demolished': 274,\n",
       " 'drowning': 275,\n",
       " 'bang': 276,\n",
       " 'county': 277,\n",
       " 'meltdown': 278,\n",
       " 'ambulance': 279,\n",
       " \"he's\": 280,\n",
       " 'river': 281,\n",
       " 'white': 282,\n",
       " 'c': 283,\n",
       " 'blown': 284,\n",
       " 'explode': 285,\n",
       " 'displaced': 286,\n",
       " 'evacuate': 287,\n",
       " 'hazardous': 288,\n",
       " 'collision': 289,\n",
       " 'crush': 290,\n",
       " 'stock': 291,\n",
       " 'dust': 292,\n",
       " 'massacre': 293,\n",
       " 'lava': 294,\n",
       " 'use': 295,\n",
       " 'airplane': 296,\n",
       " 'away': 297,\n",
       " 'august': 298,\n",
       " 'charged': 299,\n",
       " 'light': 300,\n",
       " 'market': 301,\n",
       " 'drown': 302,\n",
       " 'bombed': 303,\n",
       " 'lives': 304,\n",
       " 'riot': 305,\n",
       " 'traumatised': 306,\n",
       " 'derailment': 307,\n",
       " 'catastrophe': 308,\n",
       " 'collided': 309,\n",
       " 'trouble': 310,\n",
       " 'screamed': 311,\n",
       " 'outbreak': 312,\n",
       " 'structural': 313,\n",
       " 'long': 314,\n",
       " 'battle': 315,\n",
       " 'bleeding': 316,\n",
       " 'blew': 317,\n",
       " 'tragedy': 318,\n",
       " 'ruin': 319,\n",
       " 'crushed': 320,\n",
       " 'wounds': 321,\n",
       " 'electrocuted': 322,\n",
       " 'exploded': 323,\n",
       " 'wreckage': 324,\n",
       " 'quarantine': 325,\n",
       " 'screams': 326,\n",
       " 'better': 327,\n",
       " 'suspect': 328,\n",
       " 'arson': 329,\n",
       " 'group': 330,\n",
       " 'bagging': 331,\n",
       " 'baby': 332,\n",
       " 'anniversary': 333,\n",
       " 'yr': 334,\n",
       " 'caused': 335,\n",
       " 'cliff': 336,\n",
       " 'curfew': 337,\n",
       " 'cyclone': 338,\n",
       " 'blast': 339,\n",
       " 'deluged': 340,\n",
       " 'devastation': 341,\n",
       " 'engulfed': 342,\n",
       " 'investigators': 343,\n",
       " 'flattened': 344,\n",
       " 'inundated': 345,\n",
       " 'obliteration': 346,\n",
       " 'panicking': 347,\n",
       " 'quarantined': 348,\n",
       " 'windstorm': 349,\n",
       " \"there's\": 350,\n",
       " 'bus': 351,\n",
       " 'i': 352,\n",
       " 'horrible': 353,\n",
       " 'road': 354,\n",
       " 'sirens': 355,\n",
       " 'thought': 356,\n",
       " 'e': 357,\n",
       " 'half': 358,\n",
       " 'rioting': 359,\n",
       " 'casualties': 360,\n",
       " 'chemical': 361,\n",
       " 'harm': 362,\n",
       " 'danger': 363,\n",
       " 'derailed': 364,\n",
       " 'detonation': 365,\n",
       " 'volcano': 366,\n",
       " 'hostages': 367,\n",
       " 'mudslide': 368,\n",
       " 'obliterate': 369,\n",
       " 'obliterated': 370,\n",
       " 'sandstorm': 371,\n",
       " 'sunk': 372,\n",
       " 'twister': 373,\n",
       " 'wrecked': 374,\n",
       " 'minute': 375,\n",
       " 'sure': 376,\n",
       " 'went': 377,\n",
       " 'twitter': 378,\n",
       " 'calgary': 379,\n",
       " 'power': 380,\n",
       " 'fedex': 381,\n",
       " 'bioterrorism': 382,\n",
       " 'security': 383,\n",
       " 'send': 384,\n",
       " 'collide': 385,\n",
       " 'demolish': 386,\n",
       " 'derail': 387,\n",
       " 'desolation': 388,\n",
       " 'detonate': 389,\n",
       " 'seismic': 390,\n",
       " 'famine': 391,\n",
       " 'hijacker': 392,\n",
       " 'hijacking': 393,\n",
       " 'rainstorm': 394,\n",
       " 'razed': 395,\n",
       " 'died': 396,\n",
       " 'phone': 397,\n",
       " 'heart': 398,\n",
       " 'woman': 399,\n",
       " 'abc': 400,\n",
       " 'blazing': 401,\n",
       " 're': 402,\n",
       " 'line': 403,\n",
       " 'pkk': 404,\n",
       " 'tsunami': 405,\n",
       " 'eyewitness': 406,\n",
       " 'stretcher': 407,\n",
       " 'whirlwind': 408,\n",
       " 'tornado': 409,\n",
       " 'came': 410,\n",
       " 'airport': 411,\n",
       " 'things': 412,\n",
       " 'longer': 413,\n",
       " 'song': 414,\n",
       " 'ur': 415,\n",
       " 'casualty': 416,\n",
       " 'electrocute': 417,\n",
       " 'fatality': 418,\n",
       " 'pandemonium': 419,\n",
       " 'rubble': 420,\n",
       " 'snowstorm': 421,\n",
       " 'detonated': 422,\n",
       " 'south': 423,\n",
       " 'kids': 424,\n",
       " 'gets': 425,\n",
       " 'ebay': 426,\n",
       " 'iran': 427,\n",
       " 'zone': 428,\n",
       " 'armageddon': 429,\n",
       " 'kill': 430,\n",
       " 'fan': 431,\n",
       " 'sound': 432,\n",
       " 'trying': 433,\n",
       " 'aug': 434,\n",
       " 'affected': 435,\n",
       " 'drowned': 436,\n",
       " 'evacuated': 437,\n",
       " 'fatalities': 438,\n",
       " 'hijack': 439,\n",
       " 'murderer': 440,\n",
       " 'building': 441,\n",
       " 'far': 442,\n",
       " 'ablaze': 443,\n",
       " 'used': 444,\n",
       " 'goes': 445,\n",
       " 'government': 446,\n",
       " 'annihilated': 447,\n",
       " 'fight': 448,\n",
       " 'soon': 449,\n",
       " 'remember': 450,\n",
       " 'health': 451,\n",
       " 'blight': 452,\n",
       " 'turkey': 453,\n",
       " 'deluge': 454,\n",
       " 'demolition': 455,\n",
       " 'days': 456,\n",
       " 'shooting': 457,\n",
       " 'tomorrow': 458,\n",
       " 'avalanche': 459,\n",
       " 'music': 460,\n",
       " 'bioterror': 461,\n",
       " 'land': 462,\n",
       " 'shoulder': 463,\n",
       " 'isis': 464,\n",
       " 'thanks': 465,\n",
       " 'tell': 466,\n",
       " 'left': 467,\n",
       " 'possible': 468,\n",
       " 'play': 469,\n",
       " 'plane': 470,\n",
       " 'annihilation': 471,\n",
       " 'stand': 472,\n",
       " 'india': 473,\n",
       " 'media': 474,\n",
       " 'wanna': 475,\n",
       " 'low': 476,\n",
       " 'officer': 477,\n",
       " 'cool': 478,\n",
       " 'west': 479,\n",
       " 'rd': 480,\n",
       " 'actually': 481,\n",
       " 'issues': 482,\n",
       " 'doing': 483,\n",
       " 'reunion': 484,\n",
       " 'men': 485,\n",
       " 'believe': 486,\n",
       " 'lot': 487,\n",
       " 'hell': 488,\n",
       " 'order': 489,\n",
       " 'start': 490,\n",
       " \"doesn't\": 491,\n",
       " 'pic': 492,\n",
       " 'desolate': 493,\n",
       " 'hellfire': 494,\n",
       " 'prebreak': 495,\n",
       " 'site': 496,\n",
       " 'care': 497,\n",
       " 'brown': 498,\n",
       " 'fun': 499,\n",
       " 'hours': 500,\n",
       " 'caught': 501,\n",
       " 'stay': 502,\n",
       " 'officials': 503,\n",
       " 'israeli': 504,\n",
       " 'person': 505,\n",
       " 'yes': 506,\n",
       " 'bush': 507,\n",
       " 'hailstorm': 508,\n",
       " 'mayhem': 509,\n",
       " 'upheaval': 510,\n",
       " 'second': 511,\n",
       " 'america': 512,\n",
       " 'die': 513,\n",
       " 'ago': 514,\n",
       " 'case': 515,\n",
       " 'nearby': 516,\n",
       " 'mp': 517,\n",
       " 'km': 518,\n",
       " 'offensive': 519,\n",
       " 'street': 520,\n",
       " 'thank': 521,\n",
       " 'israel': 522,\n",
       " 'horror': 523,\n",
       " 'park': 524,\n",
       " 'damn': 525,\n",
       " 'deal': 526,\n",
       " 'rise': 527,\n",
       " 'siren': 528,\n",
       " 'wait': 529,\n",
       " 'try': 530,\n",
       " 'north': 531,\n",
       " 'traffic': 532,\n",
       " 'support': 533,\n",
       " 'plans': 534,\n",
       " 'making': 535,\n",
       " 'wake': 536,\n",
       " 'hear': 537,\n",
       " 'china': 538,\n",
       " 'reactor': 539,\n",
       " 'lt': 540,\n",
       " 'history': 541,\n",
       " 'data': 542,\n",
       " 'pick': 543,\n",
       " 'american': 544,\n",
       " 'yeah': 545,\n",
       " 'pretty': 546,\n",
       " 'policy': 547,\n",
       " 'blizzard': 548,\n",
       " 'pay': 549,\n",
       " 'wow': 550,\n",
       " 'watching': 551,\n",
       " 'takes': 552,\n",
       " 'declares': 553,\n",
       " 'hat': 554,\n",
       " 'swallowed': 555,\n",
       " 'inside': 556,\n",
       " 'happy': 557,\n",
       " \"didn't\": 558,\n",
       " 'aftershock': 559,\n",
       " 'eyes': 560,\n",
       " 'omg': 561,\n",
       " 'reuters': 562,\n",
       " 'peace': 563,\n",
       " \"i'd\": 564,\n",
       " 'children': 565,\n",
       " 'tv': 566,\n",
       " 'having': 567,\n",
       " 'listen': 568,\n",
       " 'words': 569,\n",
       " 'emmerdale': 570,\n",
       " 'saipan': 571,\n",
       " 'trench': 572,\n",
       " 'sue': 573,\n",
       " 'conclusively': 574,\n",
       " 'place': 575,\n",
       " 'shot': 576,\n",
       " 'k': 577,\n",
       " 'finally': 578,\n",
       " 'guy': 579,\n",
       " 'feared': 580,\n",
       " 'crazy': 581,\n",
       " 'maybe': 582,\n",
       " 'makes': 583,\n",
       " 'team': 584,\n",
       " 'fukushima': 585,\n",
       " 'lab': 586,\n",
       " 'money': 587,\n",
       " 'bigger': 588,\n",
       " 'memories': 589,\n",
       " 'waves': 590,\n",
       " 'effect': 591,\n",
       " \"what's\": 592,\n",
       " 'outside': 593,\n",
       " 'arsonist': 594,\n",
       " 'helicopter': 595,\n",
       " 'sorry': 596,\n",
       " 'seen': 597,\n",
       " 'bc': 598,\n",
       " 'bar': 599,\n",
       " 'crisis': 600,\n",
       " 'business': 601,\n",
       " 'literally': 602,\n",
       " 'guys': 603,\n",
       " 'beautiful': 604,\n",
       " 'australia': 605,\n",
       " 'transport': 606,\n",
       " 'morning': 607,\n",
       " 'probably': 608,\n",
       " 'photos': 609,\n",
       " 'houses': 610,\n",
       " 'bbc': 611,\n",
       " 'lost': 612,\n",
       " 'huge': 613,\n",
       " 'mom': 614,\n",
       " 'property': 615,\n",
       " 'happened': 616,\n",
       " 'book': 617,\n",
       " \"they're\": 618,\n",
       " 'job': 619,\n",
       " 'pakistan': 620,\n",
       " 'called': 621,\n",
       " \"isn't\": 622,\n",
       " 'ca': 623,\n",
       " 'v': 624,\n",
       " 'dont': 625,\n",
       " 'searching': 626,\n",
       " 'saved': 627,\n",
       " 'soudelor': 628,\n",
       " 'bestnaijamade': 629,\n",
       " 'leave': 630,\n",
       " 'hate': 631,\n",
       " 'hey': 632,\n",
       " 'blaze': 633,\n",
       " 'muslims': 634,\n",
       " 'public': 635,\n",
       " 'pray': 636,\n",
       " 'likely': 637,\n",
       " 'crews': 638,\n",
       " 'knock': 639,\n",
       " 'sensor': 640,\n",
       " 'nowplaying': 641,\n",
       " 'center': 642,\n",
       " 'salt': 643,\n",
       " 'control': 644,\n",
       " 'truth': 645,\n",
       " 'amid': 646,\n",
       " 'local': 647,\n",
       " 'taken': 648,\n",
       " 'hollywood': 649,\n",
       " 'follow': 650,\n",
       " 'level': 651,\n",
       " 'drake': 652,\n",
       " 'spot': 653,\n",
       " 'marks': 654,\n",
       " 'businesses': 655,\n",
       " 'room': 656,\n",
       " 'bodies': 657,\n",
       " 'signs': 658,\n",
       " 'islam': 659,\n",
       " 'manslaughter': 660,\n",
       " 'reason': 661,\n",
       " 'fast': 662,\n",
       " 'flag': 663,\n",
       " 'chicago': 664,\n",
       " 'talk': 665,\n",
       " 'aircraft': 666,\n",
       " 'g': 667,\n",
       " 'dog': 668,\n",
       " 'ball': 669,\n",
       " 'major': 670,\n",
       " 'l': 671,\n",
       " 'town': 672,\n",
       " 'green': 673,\n",
       " 'texas': 674,\n",
       " 'playing': 675,\n",
       " 'anthrax': 676,\n",
       " 'running': 677,\n",
       " 'computers': 678,\n",
       " 'mad': 679,\n",
       " 'link': 680,\n",
       " 'appears': 681,\n",
       " 'giant': 682,\n",
       " 'course': 683,\n",
       " 'quiz': 684,\n",
       " 'utc': 685,\n",
       " 'projected': 686,\n",
       " 'led': 687,\n",
       " 'gems': 688,\n",
       " 'la': 689,\n",
       " 'bring': 690,\n",
       " 'wrong': 691,\n",
       " 'country': 692,\n",
       " 'horse': 693,\n",
       " 'leather': 694,\n",
       " 'feeling': 695,\n",
       " 'king': 696,\n",
       " 'miss': 697,\n",
       " 'f': 698,\n",
       " 'nearly': 699,\n",
       " \"let's\": 700,\n",
       " 'russian': 701,\n",
       " 'trains': 702,\n",
       " 'uk': 703,\n",
       " 'flight': 704,\n",
       " 'image': 705,\n",
       " 'banned': 706,\n",
       " 'ignition': 707,\n",
       " 'virgin': 708,\n",
       " 'closed': 709,\n",
       " 'heavy': 710,\n",
       " 'flash': 711,\n",
       " 'lord': 712,\n",
       " 'hard': 713,\n",
       " 'climate': 714,\n",
       " 'vehicle': 715,\n",
       " 'win': 716,\n",
       " 'safety': 717,\n",
       " 'child': 718,\n",
       " 'radio': 719,\n",
       " 'comes': 720,\n",
       " 'action': 721,\n",
       " 'east': 722,\n",
       " 'reports': 723,\n",
       " 'shift': 724,\n",
       " 'christian': 725,\n",
       " 'worst': 726,\n",
       " 'needs': 727,\n",
       " 'eye': 728,\n",
       " 'space': 729,\n",
       " 'young': 730,\n",
       " 'large': 731,\n",
       " 'thursday': 732,\n",
       " 'mph': 733,\n",
       " 'issued': 734,\n",
       " 'cnn': 735,\n",
       " 'ancient': 736,\n",
       " 'refugio': 737,\n",
       " 'costlier': 738,\n",
       " 'haha': 739,\n",
       " 'cars': 740,\n",
       " 'thousands': 741,\n",
       " 'michael': 742,\n",
       " 'sign': 743,\n",
       " 'film': 744,\n",
       " 'class': 745,\n",
       " 'claims': 746,\n",
       " 'militants': 747,\n",
       " 'okay': 748,\n",
       " 'temple': 749,\n",
       " 'mount': 750,\n",
       " 'piece': 751,\n",
       " 'driving': 752,\n",
       " 'favorite': 753,\n",
       " 'star': 754,\n",
       " 'nd': 755,\n",
       " 'germs': 756,\n",
       " 'mishaps': 757,\n",
       " 'online': 758,\n",
       " 'entire': 759,\n",
       " 'told': 760,\n",
       " 'open': 761,\n",
       " 'super': 762,\n",
       " 'sad': 763,\n",
       " 'ladies': 764,\n",
       " 'village': 765,\n",
       " 'human': 766,\n",
       " 'insurance': 767,\n",
       " 'strong': 768,\n",
       " 'don': 769,\n",
       " 'picking': 770,\n",
       " 'funtenna': 771,\n",
       " 'unconfirmed': 772,\n",
       " \"neighbour's\": 773,\n",
       " \"china's\": 774,\n",
       " 'galactic': 775,\n",
       " 'miners': 776,\n",
       " 'taking': 777,\n",
       " 'daily': 778,\n",
       " 'moment': 779,\n",
       " 'official': 780,\n",
       " \"won't\": 781,\n",
       " 'usa': 782,\n",
       " 'a': 783,\n",
       " 'hand': 784,\n",
       " 'united': 785,\n",
       " 'learn': 786,\n",
       " 'true': 787,\n",
       " 'youth': 788,\n",
       " 'looking': 789,\n",
       " 'firefighters': 790,\n",
       " \"ain't\": 791,\n",
       " 'buy': 792,\n",
       " 'rock': 793,\n",
       " 'meek': 794,\n",
       " 'gbbo': 795,\n",
       " 'alarm': 796,\n",
       " 'friends': 797,\n",
       " 'apollo': 798,\n",
       " 'declaration': 799,\n",
       " 'disea': 800,\n",
       " 'season': 801,\n",
       " 'drive': 802,\n",
       " 'global': 803,\n",
       " 'thinking': 804,\n",
       " 'jobs': 805,\n",
       " 'tried': 806,\n",
       " 'experts': 807,\n",
       " \"she's\": 808,\n",
       " 'following': 809,\n",
       " 'earth': 810,\n",
       " 'toddler': 811,\n",
       " 'friend': 812,\n",
       " 'blue': 813,\n",
       " 'date': 814,\n",
       " 'suspected': 815,\n",
       " 'hiring': 816,\n",
       " 'added': 817,\n",
       " 'cut': 818,\n",
       " 'problem': 819,\n",
       " 'ebola': 820,\n",
       " 'living': 821,\n",
       " 'pain': 822,\n",
       " 'washington': 823,\n",
       " 'libya': 824,\n",
       " 'islamic': 825,\n",
       " 'brother': 826,\n",
       " 'sounds': 827,\n",
       " 'ppl': 828,\n",
       " 'angry': 829,\n",
       " 'coaches': 830,\n",
       " 'wonder': 831,\n",
       " 'break': 832,\n",
       " 'cree': 833,\n",
       " \"reddit's\": 834,\n",
       " 'subreddits': 835,\n",
       " 'myanmar': 836,\n",
       " 'awesome': 837,\n",
       " 'wanted': 838,\n",
       " 'happen': 839,\n",
       " 'scene': 840,\n",
       " 'seeing': 841,\n",
       " 'france': 842,\n",
       " 'members': 843,\n",
       " 'ship': 844,\n",
       " 'close': 845,\n",
       " 'early': 846,\n",
       " 'pile': 847,\n",
       " 'potus': 848,\n",
       " 'share': 849,\n",
       " 'poor': 850,\n",
       " 'victims': 851,\n",
       " 'shows': 852,\n",
       " 'beach': 853,\n",
       " 'terror': 854,\n",
       " 'view': 855,\n",
       " 'tweet': 856,\n",
       " 'lady': 857,\n",
       " 'loving': 858,\n",
       " 'womens': 859,\n",
       " 'centre': 860,\n",
       " 'ashes': 861,\n",
       " 'social': 862,\n",
       " 'press': 863,\n",
       " 'chance': 864,\n",
       " 'nagasaki': 865,\n",
       " 'york': 866,\n",
       " 'shape': 867,\n",
       " 'internally': 868,\n",
       " 'nigerian': 869,\n",
       " 'london': 870,\n",
       " 'wife': 871,\n",
       " 'sleeping': 872,\n",
       " 'double': 873,\n",
       " 'couple': 874,\n",
       " 'driver': 875,\n",
       " 'h': 876,\n",
       " 'fact': 877,\n",
       " 'total': 878,\n",
       " 'join': 879,\n",
       " 'dad': 880,\n",
       " 'escape': 881,\n",
       " 'theater': 882,\n",
       " 'gun': 883,\n",
       " 'gop': 884,\n",
       " 'pamela': 885,\n",
       " 'research': 886,\n",
       " 'silver': 887,\n",
       " 'turn': 888,\n",
       " 'cold': 889,\n",
       " 'sun': 890,\n",
       " 'dude': 891,\n",
       " 'cake': 892,\n",
       " 'text': 893,\n",
       " 'info': 894,\n",
       " 'art': 895,\n",
       " 'fans': 896,\n",
       " 'fashion': 897,\n",
       " 'saying': 898,\n",
       " 'holding': 899,\n",
       " 'ross': 900,\n",
       " 'sex': 901,\n",
       " 'absolutely': 902,\n",
       " 'landing': 903,\n",
       " 'sea': 904,\n",
       " 'patience': 905,\n",
       " 'madhya': 906,\n",
       " 'pradesh': 907,\n",
       " 'nws': 908,\n",
       " 'bayelsa': 909,\n",
       " 'rly': 910,\n",
       " 'chile': 911,\n",
       " 'causes': 912,\n",
       " 'colorado': 913,\n",
       " 'aba': 914,\n",
       " 'ave': 915,\n",
       " 'reported': 916,\n",
       " 'risk': 917,\n",
       " 'dies': 918,\n",
       " 'guess': 919,\n",
       " 'wednesday': 920,\n",
       " 'bit': 921,\n",
       " 'trust': 922,\n",
       " 'ready': 923,\n",
       " \"wasn't\": 924,\n",
       " 'vs': 925,\n",
       " 'disease': 926,\n",
       " 'started': 927,\n",
       " 'took': 928,\n",
       " 'series': 929,\n",
       " 'till': 930,\n",
       " 'russia': 931,\n",
       " 'direction': 932,\n",
       " 'udhampur': 933,\n",
       " 'gave': 934,\n",
       " 'bombs': 935,\n",
       " 'waving': 936,\n",
       " 'geller': 937,\n",
       " 'playlist': 938,\n",
       " 'cover': 939,\n",
       " 'ahead': 940,\n",
       " 'event': 941,\n",
       " 'party': 942,\n",
       " 'giving': 943,\n",
       " 'soul': 944,\n",
       " 'biggest': 945,\n",
       " 'wall': 946,\n",
       " 'record': 947,\n",
       " 'small': 948,\n",
       " 'middle': 949,\n",
       " 'central': 950,\n",
       " 'outrage': 951,\n",
       " 'downtown': 952,\n",
       " 'camp': 953,\n",
       " 'british': 954,\n",
       " 'instead': 955,\n",
       " 'tree': 956,\n",
       " 'download': 957,\n",
       " 'interesting': 958,\n",
       " 'spring': 959,\n",
       " 'passengers': 960,\n",
       " 'information': 961,\n",
       " 'brooklyn': 962,\n",
       " 'rules': 963,\n",
       " 'offroad': 964,\n",
       " 'lamp': 965,\n",
       " 'shots': 966,\n",
       " 'means': 967,\n",
       " 'month': 968,\n",
       " 'secret': 969,\n",
       " 'david': 970,\n",
       " 'pakistani': 971,\n",
       " 'episode': 972,\n",
       " 'short': 973,\n",
       " 'mod': 974,\n",
       " 'totally': 975,\n",
       " 'beat': 976,\n",
       " 'japanese': 977,\n",
       " 'burn': 978,\n",
       " 'sos': 979,\n",
       " 'album': 980,\n",
       " 'lies': 981,\n",
       " 'metro': 982,\n",
       " 'tote': 983,\n",
       " 'handbag': 984,\n",
       " 'killing': 985,\n",
       " 'point': 986,\n",
       " 'colour': 987,\n",
       " 'you': 988,\n",
       " 'planned': 989,\n",
       " 'account': 990,\n",
       " 'denver': 991,\n",
       " 'delivers': 992,\n",
       " 'international': 993,\n",
       " 'marians': 994,\n",
       " 'sick': 995,\n",
       " 'travel': 996,\n",
       " 'apc': 997,\n",
       " 'parole': 998,\n",
       " 'recount': 999,\n",
       " 'sky': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words=20575)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "sequence=tokenizer.texts_to_sequences(all_text)\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3b9c37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:25.321832Z",
     "iopub.status.busy": "2024-06-02T10:22:25.321556Z",
     "iopub.status.idle": "2024-06-02T10:22:25.326834Z",
     "shell.execute_reply": "2024-06-02T10:22:25.326032Z"
    },
    "papermill": {
     "duration": 0.039165,
     "end_time": "2024-06-02T10:22:25.328746",
     "exception": false,
     "start_time": "2024-06-02T10:22:25.289581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20575"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81f6030c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:25.396371Z",
     "iopub.status.busy": "2024-06-02T10:22:25.395496Z",
     "iopub.status.idle": "2024-06-02T10:22:25.611647Z",
     "shell.execute_reply": "2024-06-02T10:22:25.610644Z"
    },
    "papermill": {
     "duration": 0.256443,
     "end_time": "2024-06-02T10:22:25.616558",
     "exception": false,
     "start_time": "2024-06-02T10:22:25.360115",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5458, 661, 135, 1582, 3503],\n",
       " [71, 97, 689, 8061, 8062, 1223],\n",
       " [1378, 1293, 1873, 575, 8063, 1583, 150, 1873, 575, 1474, 1055],\n",
       " [8, 4222, 1164, 150, 1474, 30],\n",
       " [3, 23, 1294, 164, 5459, 2029, 158, 1164, 8064, 98],\n",
       " [2937, 265, 30, 1475, 709, 8065, 1056, 277, 5460, 1164],\n",
       " [94, 13, 710, 165, 912, 711, 174, 1584, 8066, 913, 2548, 1379],\n",
       " [4, 1874, 4223],\n",
       " [350, 10, 150, 1057, 441, 520],\n",
       " [4, 2030, 409, 136, 192],\n",
       " [8, 396, 142, 266, 442],\n",
       " [739,\n",
       "  423,\n",
       "  2549,\n",
       "  121,\n",
       "  2550,\n",
       "  4224,\n",
       "  529,\n",
       "  511,\n",
       "  87,\n",
       "  423,\n",
       "  2549,\n",
       "  151,\n",
       "  151,\n",
       "  8067,\n",
       "  174],\n",
       " [3504, 174, 1585, 8068, 2549, 456, 206, 612, 3505],\n",
       " [94, 5461, 836, 1380, 5461],\n",
       " [126, 98, 351, 2938, 46, 22, 212],\n",
       " [592, 29],\n",
       " [35, 5462],\n",
       " [137, 1476],\n",
       " [46, 662],\n",
       " [8069],\n",
       " [3506],\n",
       " [870, 478],\n",
       " [35, 8070],\n",
       " [2268, 24],\n",
       " [8071],\n",
       " [58, 54, 1058, 105],\n",
       " [1224, 222],\n",
       " [35, 2551],\n",
       " [8072],\n",
       " [2, 4225],\n",
       " [243],\n",
       " [8073, 2552, 1875, 443],\n",
       " [530, 690, 710, 1059, 19],\n",
       " [8074, 212, 7, 1721, 663, 159, 443, 914],\n",
       " [1477, 159, 443],\n",
       " [1876, 84, 1000, 100, 443],\n",
       " [8075, 5463, 2553, 1722, 3507, 5, 5464, 2554, 159, 8076, 443, 801],\n",
       " [5465, 1106, 5466, 159, 443],\n",
       " [8077, 8078, 2939, 740, 159, 443, 2555, 1723, 223, 152, 5467, 14, 8079],\n",
       " [443, 712, 145],\n",
       " [175, 2556],\n",
       " [593, 90, 443, 1001, 90, 40, 556],\n",
       " [837, 16, 5468, 5469, 223, 1106, 8080, 496, 443, 465, 5470, 5471, 777, 497],\n",
       " [5472, 8081, 443, 8082],\n",
       " [838, 159, 664, 443, 5473, 1586],\n",
       " [2557, 2031, 222, 26, 4226, 1877],\n",
       " [479, 224, 741, 1164, 443, 30],\n",
       " [441, 1381, 8083, 43, 630, 1584, 443],\n",
       " [175, 2556],\n",
       " [100, 8084, 9, 1478, 1382, 327, 444, 2558, 1060, 100, 39],\n",
       " [3508, 29, 576, 4227, 68, 159, 443],\n",
       " [29, 871, 61, 2940, 1165, 443, 4228],\n",
       " [2555, 1723, 223, 152, 5467, 14, 8085, 8086, 4229, 235],\n",
       " [14, 594, 8087, 159, 76, 1724, 531, 4230, 443],\n",
       " [8088, 1061, 8089, 5474, 5475, 557, 8090, 1002, 713, 8091, 8092],\n",
       " [8093, 8094, 5476, 663, 1062, 159, 443, 5477, 4231, 5476, 8095],\n",
       " [122, 443, 235, 8096, 915, 593, 8097, 5478, 3509, 2941],\n",
       " [159, 2559, 443, 99, 2560, 5479, 2, 3510, 8098],\n",
       " [1000, 443, 236, 2561, 2562, 4, 3511, 2942, 2563, 3512, 2032, 966, 26, 4232],\n",
       " [479, 224, 741, 1164, 443, 30, 714, 1725],\n",
       " [5480, 5481, 1726, 967, 1727, 8099, 443, 5481, 1587, 1295, 2269, 8100],\n",
       " [8101, 8102, 968, 1166, 159, 5482, 443, 2943, 8103],\n",
       " [5483, 443, 1, 8104, 969, 577, 2944, 3513, 4233, 5484],\n",
       " [8105, 1479, 27, 30, 8106, 8107, 1165, 5485, 443, 7],\n",
       " [641, 5483, 443, 1, 2033, 8108, 2945, 1225],\n",
       " [8109, 613, 2552, 1875, 443],\n",
       " [443, 16, 160, 665, 11, 26, 52, 75],\n",
       " [1226, 54, 424, 2034, 23, 5486, 51, 1, 3514, 8110, 9, 2946, 424, 742, 1878],\n",
       " [51, 352, 33, 8111, 532, 1296, 47, 3515, 2947],\n",
       " [51, 642, 1728, 1167, 8112, 8113, 4234, 101, 512, 5487, 8114, 532],\n",
       " [8115, 51, 137, 85, 2948, 1, 5488, 79, 8116],\n",
       " [8117, 8118, 8119, 912, 1107, 1879, 46, 51, 5489],\n",
       " [916, 2949, 715, 51, 8120, 8121, 480, 97, 8122, 1063, 5490, 715, 295],\n",
       " [8123, 87, 51, 5491],\n",
       " [352, 1383, 3516, 423, 5492, 5493, 715, 51, 8124, 709, 38],\n",
       " [19, 8125, 872, 8126, 873, 917, 46, 51],\n",
       " [5494, 51, 1384, 3517, 839],\n",
       " [532, 51, 252, 8127, 1475, 8128, 2950, 8129],\n",
       " [352, 1383, 3516, 423, 5492, 5493, 715, 51, 8130, 38],\n",
       " [8131, 840, 51, 1227, 3518, 5495],\n",
       " [614, 558, 68, 662, 8132, 614, 51, 122, 8133, 8134],\n",
       " [353, 46, 51, 244, 1064, 4, 578, 1480, 521, 88],\n",
       " [529, 2951, 5496, 466, 51],\n",
       " [8135, 8136, 8137, 5497, 1880, 1481],\n",
       " [51, 8138, 2035, 1482, 532],\n",
       " [4230, 51, 1483, 918, 352, 22, 46, 2564, 8139, 2565, 2952, 5498],\n",
       " [2566, 4235, 2566, 51, 615, 126, 3519, 5499, 480, 5500, 1003],\n",
       " [19, 8140, 51, 61, 2953, 8141, 8142, 97, 2270, 4236, 5501, 5502, 1065, 8143],\n",
       " [51,\n",
       "  467,\n",
       "  1728,\n",
       "  1167,\n",
       "  1588,\n",
       "  19,\n",
       "  4234,\n",
       "  8144,\n",
       "  480,\n",
       "  102,\n",
       "  532,\n",
       "  3520,\n",
       "  2567,\n",
       "  1729,\n",
       "  532],\n",
       " [51, 615, 126, 5499, 480, 5500, 1003],\n",
       " [51],\n",
       " [2566, 4235, 2566, 51, 615, 126, 8145, 6, 91, 152],\n",
       " [38, 532, 51, 213, 8146, 8147, 480],\n",
       " [8148, 5503, 8149, 8150, 97, 89, 51],\n",
       " [8151, 2271, 51],\n",
       " [1383,\n",
       "  5504,\n",
       "  352,\n",
       "  423,\n",
       "  51,\n",
       "  2568,\n",
       "  69,\n",
       "  2036,\n",
       "  1484,\n",
       "  8152,\n",
       "  480,\n",
       "  1730,\n",
       "  1297,\n",
       "  1297,\n",
       "  1297,\n",
       "  5505],\n",
       " [51, 2954, 43, 79, 4237, 2569, 8153, 533, 43, 497, 534, 1589, 31, 2037],\n",
       " [212, 1485, 2565, 46, 51, 616, 8154, 41, 245, 1385, 123, 8155],\n",
       " [8156, 8157, 51],\n",
       " [46, 222, 23, 179, 46, 51, 3521, 54, 179, 802],\n",
       " [8158, 8159, 14, 3522, 396, 354, 51, 48, 225],\n",
       " [246, 1724, 2955, 2956, 136, 2272, 1004, 51, 1298, 8160, 8161, 5506],\n",
       " [559, 5507, 5508, 1486, 87, 198, 8162],\n",
       " [1108, 29, 802, 2273, 425, 5509, 29, 716, 5510, 8163],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [3523, 2038, 2039, 2570, 2571, 5511, 970, 5512],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8164, 5513, 410, 511, 2957, 8165],\n",
       " [559, 5507, 113, 2572, 1005, 5508, 5514, 8, 243, 121, 1486],\n",
       " [1108, 8166, 2958, 4238, 8167, 2040, 8168],\n",
       " [8169, 31, 4239, 8170, 457, 1477, 8171],\n",
       " [919, 481, 1067, 166, 559, 5515],\n",
       " [559, 2959, 64, 4240, 5516, 206, 8172, 206],\n",
       " [559],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8173, 4, 841, 482, 559],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [1590, 1731, 559, 1732, 1591, 1733, 1592, 1225, 1066, 1734],\n",
       " [8174, 8175, 375, 778, 5517, 77, 5518, 43, 8176],\n",
       " [559, 1593, 2573, 803, 1168, 278, 970, 8177],\n",
       " [779, 1735, 4240, 5516, 579, 3, 226, 180, 167, 8178, 559],\n",
       " [559, 4241, 4242, 42],\n",
       " [57, 57, 559, 1593, 2573, 803, 1168, 617, 8179],\n",
       " [161, 8180, 90, 483, 691, 90, 483, 69, 2960, 8181],\n",
       " [1108, 253, 2574, 1487, 530, 5519, 481, 468, 2960, 498],\n",
       " [8182, 88, 5520, 2274, 2, 8183, 8184],\n",
       " [8185, 31, 513, 64, 58, 26, 1386, 2041, 804, 1594, 1479, 805],\n",
       " [806, 2575, 559, 41, 43],\n",
       " [8186, 35, 1387],\n",
       " [559],\n",
       " [559, 98, 1881, 101, 63, 521, 535, 468, 101, 100],\n",
       " [8, 56, 8187, 483, 2275, 8188, 5521],\n",
       " [1108, 29, 425, 8189, 511, 29, 425, 2576, 5522, 8190],\n",
       " [62,\n",
       "  129,\n",
       "  15,\n",
       "  236,\n",
       "  469,\n",
       "  4243,\n",
       "  2276,\n",
       "  1486,\n",
       "  2961,\n",
       "  5523,\n",
       "  2962,\n",
       "  8191,\n",
       "  8192,\n",
       "  8193,\n",
       "  8194,\n",
       "  8195,\n",
       "  8196,\n",
       "  559,\n",
       "  1595],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 130, 7],\n",
       " [4244, 5524, 2963, 296, 51, 1229, 2577, 1230, 8197, 4245, 2964],\n",
       " [8198, 4246, 3524, 1488, 296, 51],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 3525],\n",
       " [8199, 5525, 618, 296, 51, 618, 151, 513, 8200, 55, 619],\n",
       " [143, 843, 8201, 1489, 1596, 396, 296, 51, 5526, 8202, 1736, 105, 328],\n",
       " [29, 445, 296, 2965, 51, 42],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [8203, 296, 51, 5527, 3526, 2043, 1299, 48, 485, 1597, 123, 3526, 446, 780],\n",
       " [353, 51, 29, 396, 2042, 296, 8204],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 920],\n",
       " [807,\n",
       "  842,\n",
       "  1109,\n",
       "  1388,\n",
       "  296,\n",
       "  146,\n",
       "  484,\n",
       "  267,\n",
       "  1228,\n",
       "  199,\n",
       "  51,\n",
       "  807,\n",
       "  920,\n",
       "  1737,\n",
       "  1388,\n",
       "  59],\n",
       " [2966, 2967, 1110, 8205, 1389, 8206, 143, 843, 48, 8207, 51],\n",
       " [1294, 4247, 8208, 51, 521, 88, 296, 1006],\n",
       " [8209, 8210, 48, 296, 51, 100, 46, 124, 1231, 9, 64],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 2277],\n",
       " [8211, 2578, 29, 411, 296, 666, 8212, 2278, 51, 5528],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [353, 51, 29, 396, 2042, 296],\n",
       " [8213, 1489, 8214, 143, 40, 296, 22, 8215, 51],\n",
       " [1229, 918, 470, 22, 46, 1390, 42, 22, 666, 296, 1229, 73, 51, 8216],\n",
       " [4244, 5524, 2963, 296, 51],\n",
       " [3527, 2279, 807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 1882],\n",
       " [807, 842, 1109, 1388, 296, 146, 484, 267, 1228, 199, 51, 807, 3525],\n",
       " [353, 51, 29, 396, 2042, 296, 1111, 2280, 59, 486, 560],\n",
       " [8217, 8218, 1068, 268, 296, 162, 514, 51, 467, 147, 921, 254, 808],\n",
       " [561, 353, 51, 29, 396, 2042, 296],\n",
       " [561, 11, 486, 1389, 1300, 296, 51, 8219, 8220, 8221, 667],\n",
       " [807,\n",
       "  842,\n",
       "  1109,\n",
       "  1388,\n",
       "  296,\n",
       "  146,\n",
       "  484,\n",
       "  267,\n",
       "  1228,\n",
       "  199,\n",
       "  51,\n",
       "  807,\n",
       "  920,\n",
       "  1737,\n",
       "  1388,\n",
       "  59],\n",
       " [296, 51],\n",
       " [397, 168, 2, 46, 844, 296, 51, 1301],\n",
       " [8222, 4, 917, 121, 48, 1391, 1488, 296, 51],\n",
       " [296, 2968, 118, 8223, 8, 513, 51],\n",
       " [457, 296, 51],\n",
       " [1883, 169, 296, 51, 2281, 2579, 295, 2282, 2969, 845, 5529, 8224],\n",
       " [846, 536, 1392, 5530, 115, 1, 2044, 33, 279, 1112, 8225],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [199, 4248, 840, 22, 740, 8226, 8227],\n",
       " [580, 48, 971, 199, 279, 595, 22, 562, 4249],\n",
       " [1490, 10, 110, 2045, 5531, 5, 279, 1598],\n",
       " [5532, 8228, 8229, 69, 350, 1113, 110, 4250, 3, 593, 8230, 279, 840],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [5, 5533, 2283, 1480, 1232, 227, 214, 5534],\n",
       " [8231, 8232, 8233, 8234, 4251, 581, 2284, 279, 874, 8235, 4252],\n",
       " [181, 279, 1169, 8236, 8237],\n",
       " [7, 580, 48, 971, 199, 279, 595, 22, 1884, 8238],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [8239, 1007, 279],\n",
       " [4253, 255, 62, 279, 5535, 55],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [620, 199, 279, 595, 22, 215],\n",
       " [8240, 8241, 8242, 255, 62, 1302, 2973, 279, 62],\n",
       " [1885, 1303, 8243, 1114, 1115, 4254, 5536, 1885, 8244, 279],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [279, 2970, 2046, 2971, 715, 1738, 2972, 2047, 426],\n",
       " [8245, 8246, 175, 592, 2580, 487, 106, 39, 279, 152, 5537],\n",
       " [11, 26, 58, 279, 136, 540, 540],\n",
       " [562, 580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [279, 69, 593, 75],\n",
       " [8247, 668, 4255, 280, 279],\n",
       " [1057, 8248, 1885, 279, 3528, 3529, 355, 2285],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [580, 48, 971, 199, 279, 595, 22, 1393],\n",
       " [580, 48, 971, 199, 279, 595, 22, 1393],\n",
       " [592, 14, 279, 1233, 8249, 17, 26],\n",
       " [4256, 8250, 1595, 2581, 8251, 8252, 1739, 3519, 279, 922],\n",
       " [580, 48, 971, 199, 279, 595, 22],\n",
       " [8, 530, 1069, 1008, 279, 1886, 631],\n",
       " [972, 8253, 447, 8254, 8255, 105, 4257, 1116, 2974],\n",
       " [1491, 447, 8256, 8257, 4258, 3530, 1491, 8258],\n",
       " [2582, 3, 447, 1304, 1740],\n",
       " [8259, 632, 8260, 447, 2975, 1305, 5538, 8261],\n",
       " [8262, 8263, 8264, 447, 1585, 244, 2976, 1741, 8265, 1009, 3, 54, 5539, 8266],\n",
       " [447, 5540],\n",
       " [447, 2583, 2584, 8267, 4259, 1742, 520, 8268, 8269],\n",
       " [5541, 513, 447, 5542, 5543],\n",
       " [8270, 55, 5544, 596, 781, 447],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [447],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [2977, 692, 3, 5545, 447, 876, 3531],\n",
       " [8271, 447, 521, 8272],\n",
       " [253, 376, 88, 8273, 522, 447, 523, 427, 33, 4260],\n",
       " [8274, 919, 170, 255, 8275, 1306, 1888, 541, 121, 447],\n",
       " [61, 447, 8, 3532, 2585, 2978, 8276, 1599],\n",
       " [24, 5546, 377, 55, 8277, 877, 1307, 1600, 973, 669, 447, 8278, 213],\n",
       " [6, 8279, 8280, 2287, 8281, 447, 2048],\n",
       " [5541, 513, 447, 5542, 5543],\n",
       " [923, 447, 8282, 198],\n",
       " [8283, 8284, 8, 447, 4261, 138, 77, 5547, 356, 1601],\n",
       " [8285, 8286, 447, 8287, 1117, 1889, 1010, 375, 1170, 8288],\n",
       " [8289,\n",
       "  1743,\n",
       "  712,\n",
       "  597,\n",
       "  8290,\n",
       "  8291,\n",
       "  2288,\n",
       "  410,\n",
       "  5548,\n",
       "  533,\n",
       "  179,\n",
       "  447,\n",
       "  575,\n",
       "  8292,\n",
       "  4262],\n",
       " [2049, 5549, 5550, 8293, 8294, 1890, 9, 121, 447, 2979, 5551],\n",
       " [5552, 8295, 447, 515, 4263, 8296, 8297],\n",
       " [3, 1394, 447, 8298, 1395, 5553, 16, 1001],\n",
       " [5552, 447, 1891, 228, 5554, 4243, 1492, 1600, 4264, 5, 969, 2980],\n",
       " [8299, 1892, 170, 8300, 8301, 1171, 8302, 39, 101, 5555, 447, 579, 2050],\n",
       " [1391, 2286, 1070, 875, 717, 1071, 46, 153, 49, 1887],\n",
       " [447, 8303, 1234, 74, 8304, 8305, 1893, 8306, 847],\n",
       " [8307, 255, 924, 1394, 8308, 154, 8309, 598, 8310, 8311, 8312, 447, 33, 8313],\n",
       " [499, 3512, 557, 1115, 8314, 599, 8315, 8316, 23, 447, 4265, 198],\n",
       " [8317, 8318, 1482, 447, 669, 8319],\n",
       " [8320, 488, 3, 3533, 5519, 878, 471, 237, 782, 8321, 8322, 3534],\n",
       " [848, 582, 522, 466, 269, 596, 1308, 5556, 281, 471],\n",
       " [8323, 471, 5557],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8324, 8325, 471],\n",
       " [36, 471, 925, 270, 4266, 5558, 25, 5559, 2981],\n",
       " [8326, 8327, 8328, 471, 42],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8329, 743, 849, 1114, 247, 95, 1894, 2982],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [8330, 175, 643, 281, 1894, 79, 102, 471, 839, 8331, 139, 1595, 1895],\n",
       " [2983, 4267, 4268, 471],\n",
       " [8332, 8333, 1396, 670, 8334, 471, 522],\n",
       " [8335, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [5560, 2586, 8336, 1172, 8337, 8338, 3535, 3536, 270, 471],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [471, 1744, 3537, 1, 8339, 500, 297, 88, 2051, 24],\n",
       " [8340, 2587, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [8341, 62, 79, 1894, 513, 19, 1, 743, 1114, 472, 1, 1235, 4269],\n",
       " [632, 5561, 743, 1114, 247, 5562, 8342, 8343, 1896, 783, 314, 489],\n",
       " [102, 471, 643, 281, 95, 1894, 139],\n",
       " [8344, 280, 2984, 4270, 8345, 2289, 3538, 1397, 2952, 396, 471],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [743, 1, 19, 247, 5563],\n",
       " [465, 3505, 4271, 879, 809, 471, 428, 3539],\n",
       " [36, 471, 925, 270, 4266, 5558, 25, 5559, 2981],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [15, 6, 171, 524, 110, 1011, 171, 71, 102, 471, 643, 281, 95, 693, 139],\n",
       " [4, 151, 448, 5564, 449],\n",
       " [5565, 8346, 558, 172, 238, 8347, 1173, 353, 850, 173, 8348, 718],\n",
       " [8349, 1398, 238],\n",
       " [19, 8350, 19, 8351, 8352, 44, 2985, 8353, 238, 8354, 744, 8355],\n",
       " [238],\n",
       " [1115, 9, 298, 38, 1493, 131, 5495, 1745, 238, 8356, 8357],\n",
       " [8358, 8359, 96, 2, 483, 2986, 1117, 5566, 238],\n",
       " [808, 1309, 80, 1236, 719, 41, 592, 926, 238, 927, 1494],\n",
       " [9, 238, 81, 4272, 47],\n",
       " [26, 9, 1310, 8360, 743, 238, 621],\n",
       " [2987, 235, 238, 1495, 8361, 8362, 8363],\n",
       " [238],\n",
       " [19,\n",
       "  8364,\n",
       "  1311,\n",
       "  100,\n",
       "  1169,\n",
       "  1012,\n",
       "  974,\n",
       "  1897,\n",
       "  238,\n",
       "  2290,\n",
       "  1,\n",
       "  974,\n",
       "  2588,\n",
       "  5567,\n",
       "  3540],\n",
       " [1602, 24, 1603, 238],\n",
       " [19, 1312, 1604, 973, 1237, 238, 2052, 1399, 928, 4273, 248, 1013],\n",
       " [8365, 1313, 8366, 1745, 238, 1072, 398, 398, 4274],\n",
       " [19, 8367, 182, 42, 12, 1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 1746],\n",
       " [170, 1898, 9, 3541, 4275, 238],\n",
       " [880, 1496, 2269, 168, 2, 1238, 3542, 85, 9, 481, 4276, 8368, 238],\n",
       " [8369, 238, 720, 222, 26, 245],\n",
       " [84, 8370, 5568, 69, 168, 2, 1899, 5569, 238, 193],\n",
       " [4228, 3, 1293, 1014, 238],\n",
       " [350, 20, 4277, 107, 127, 485, 238, 159, 164, 8371],\n",
       " [1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 974, 2588, 5567, 42],\n",
       " [576, 398, 5570, 31, 975, 35, 176, 398, 8372],\n",
       " [19, 8373, 238, 38, 4278, 8374, 2988, 1497, 2983, 24, 8375],\n",
       " [26, 5571],\n",
       " [107, 8376, 3543, 20, 1118, 238, 8377, 8378],\n",
       " [1747, 5572, 238],\n",
       " [973, 1237, 238, 2052, 1399, 928, 4273, 248, 1013],\n",
       " [3544, 87, 721, 25, 4279, 16, 2989, 4, 5573, 8379, 2990, 8380, 238],\n",
       " [182, 42, 12, 1311, 100, 1169, 1012, 974, 1897, 238, 2290, 1, 974, 2588],\n",
       " [3545, 2991, 8381, 429, 430, 2992, 662, 3546, 661],\n",
       " [3545, 2991, 8382, 429, 430, 2992, 662, 3546, 661],\n",
       " [780, 2993, 5574, 57, 57, 57, 57, 4280, 5575, 5576, 57, 57, 429],\n",
       " [5577, 563, 35, 1, 429],\n",
       " [64, 193, 1481, 597, 429],\n",
       " [1073, 16, 11, 536, 1748, 2291, 429, 490],\n",
       " [131, 8383, 429, 2589, 2053, 85, 426],\n",
       " [8384, 26, 105, 445, 36, 929, 429],\n",
       " [19,\n",
       "  8385,\n",
       "  35,\n",
       "  5578,\n",
       "  2590,\n",
       "  5579,\n",
       "  4281,\n",
       "  5580,\n",
       "  5581,\n",
       "  5582,\n",
       "  5583,\n",
       "  238,\n",
       "  429,\n",
       "  8386],\n",
       " [113, 151, 976, 429, 8387, 8388, 3, 23, 5584, 530],\n",
       " [8389, 1498, 5585, 26, 350, 871, 424, 1174, 54, 79, 206, 1119, 429, 8390],\n",
       " [2054, 564, 314, 5586, 784, 564, 5587, 5588, 429, 2591, 1175, 2592],\n",
       " [397, 8391, 4282, 1176, 5589, 542, 1749, 5590, 1168, 429, 1605],\n",
       " [3545, 2991, 8392, 3547, 429, 430, 2992, 662, 3546, 661],\n",
       " [19,\n",
       "  8393,\n",
       "  35,\n",
       "  5578,\n",
       "  2590,\n",
       "  5579,\n",
       "  4281,\n",
       "  5580,\n",
       "  5581,\n",
       "  5582,\n",
       "  5583,\n",
       "  238,\n",
       "  429,\n",
       "  1898],\n",
       " [3545, 2991, 8394, 429, 430, 2992, 662, 3546, 661],\n",
       " [164, 5591, 67, 1120, 2593, 429, 972, 8395, 3548],\n",
       " [429],\n",
       " [8396, 8397, 55, 2055, 458, 100, 8398, 1750, 5592, 429],\n",
       " [8399, 429],\n",
       " [5593, 90, 541, 1397, 521, 2056, 1015, 58, 1016, 2994, 5594, 1400, 1314],\n",
       " [2995, 2292, 482, 670, 229, 1239, 881, 429],\n",
       " [3549, 2996, 1401, 5595, 1606, 8, 810, 429],\n",
       " [671, 132, 2594, 487, 8400, 2057, 2269, 513, 713, 5596, 429, 3550, 426, 5597],\n",
       " [83, 6, 665, 8401, 8402, 8403, 115, 69, 1177],\n",
       " [5598, 4228, 4283, 2978, 472, 4, 121, 2058, 811, 238, 429],\n",
       " [175, 8404, 5599, 2059, 2293, 3551, 8405, 8406, 1607, 2997, 426],\n",
       " [8407, 8408, 356, 1017, 271, 4251, 1735, 200, 429],\n",
       " [8409, 50, 8410, 115, 61, 514, 429, 8411, 8412, 1, 8413],\n",
       " [1751, 2060, 3543, 8414, 5600, 429],\n",
       " [429, 2294, 1061, 8415, 3552],\n",
       " [8416, 16, 500, 107, 929, 1402, 5601, 429, 1499, 4284],\n",
       " [4285, 8417, 930, 2061, 429],\n",
       " [2595, 785, 522, 8418, 8419, 8420, 449, 513, 429],\n",
       " [780, 2993, 57, 5574, 57, 57, 57, 57, 4280, 5575, 5576, 57, 57, 429],\n",
       " [458, 24, 490, 429, 8421],\n",
       " [8422, 160, 3553, 8423, 1018, 745, 5602, 1074, 429, 8424],\n",
       " [3554, 8425, 1168, 600, 8426, 1168, 429, 1403, 70, 1605, 1404, 8427],\n",
       " [1395, 8428, 3555, 2995, 2292, 482, 670, 229, 1239, 881, 8429, 3555, 931],\n",
       " [5593, 90, 1177, 69, 2062, 622, 1019, 8430, 2063, 8431],\n",
       " [780, 2993, 8432, 57, 57, 57, 57, 57, 57, 4280, 57, 57, 57, 429, 8433, 449],\n",
       " [5538, 2064, 2564, 8434, 458, 100, 429, 2295],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [1071, 137, 543, 431, 74, 3556],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [841, 74, 8435, 253, 2296, 8436, 23, 442],\n",
       " [1020, 2996, 1405, 74, 2038, 490, 667, 812, 2998, 8437],\n",
       " [2999, 74, 8438, 2596, 8439, 5603, 565, 2999, 74, 2297, 98, 2596],\n",
       " [1316, 1315, 925, 4286, 91, 1406, 5604, 8440],\n",
       " [1020, 74, 1240, 1407, 2298, 1240, 448, 2, 2298],\n",
       " [8441, 1408, 566, 1409],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [8442,\n",
       "  5605,\n",
       "  8443,\n",
       "  5606,\n",
       "  74,\n",
       "  8444,\n",
       "  4287,\n",
       "  5607,\n",
       "  44,\n",
       "  1500,\n",
       "  813,\n",
       "  5608,\n",
       "  4288,\n",
       "  1317],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [5609, 3000, 74, 814, 2597, 1753, 4289, 44],\n",
       " [19, 8445, 8446, 1075, 74, 5610],\n",
       " [1071, 137, 543, 431, 74, 3556],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [1754, 5611, 4290, 1410, 3557, 3001, 5612, 3558, 4291],\n",
       " [932, 543, 431, 74, 1315],\n",
       " [8447, 1501, 2299, 111, 1609, 74, 8448, 74, 4292, 74, 130, 3559],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [1608, 543, 431, 74, 1752],\n",
       " [848, 8449, 8450, 8451, 2598, 667, 8452, 1755, 8453, 281, 5613, 786, 8454],\n",
       " [1021, 256, 692, 74, 2300, 79, 644, 5614, 690, 563, 850, 8],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [4293, 1178, 977, 74, 1318, 144, 103, 694, 44, 53, 3002, 1402, 85, 426],\n",
       " [932, 543, 431, 74, 1315, 127],\n",
       " [932, 543, 431, 74, 1315],\n",
       " [3003, 1610, 299, 2599, 1724, 329, 2065, 645],\n",
       " [5615, 1502, 56, 618, 4294, 3004, 329, 1611, 2, 1503, 4295],\n",
       " [741, 5616, 2596, 8455, 563, 5617, 329, 25, 928, 43],\n",
       " [1756, 5618, 329, 1411],\n",
       " [1757, 631, 1612, 5619, 978, 118, 5],\n",
       " [2561, 2562, 162, 329, 328, 1241, 27, 501, 92, 4296],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [432, 329],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [529],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [2600, 814, 159, 29, 299, 329, 4297],\n",
       " [73, 1121, 811, 329, 25, 522, 5620, 3003],\n",
       " [1121, 1107, 48, 646, 4298, 329, 25],\n",
       " [2601, 14, 2601, 1613, 1166, 2067, 329, 1761, 14, 2601, 1613, 1166, 5621],\n",
       " [5622, 121, 4299, 850, 201],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522, 2989, 3560, 4300],\n",
       " [8456, 8457, 19, 4301, 329, 328, 1241, 27, 501, 92, 30],\n",
       " [3003, 2955, 8458, 1112, 1121, 143, 3561, 329, 8459],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 3562, 2301],\n",
       " [8460, 8461, 1399, 329],\n",
       " [2601, 14, 2601, 1613, 1166, 2067, 329, 1761, 14, 2601, 1613, 1166, 5621],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 3562, 2301],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [2066, 1758, 1759, 329, 851, 1900, 1231, 1760, 522],\n",
       " [1227, 664, 192, 1022, 599, 1901, 329, 1902, 2301],\n",
       " [329, 328, 1241, 27, 501, 92, 30, 4301],\n",
       " [2302, 3563, 874, 8462, 631, 1612, 163, 68, 329, 3563],\n",
       " [329, 328, 1241, 27, 501, 92, 30, 2561, 2562, 162],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [1614, 4302, 5623, 631, 282, 8, 1746],\n",
       " [641, 594, 4303, 2602, 5624],\n",
       " [2603, 722, 1179, 1903, 594, 1122],\n",
       " [5625, 3, 2303, 5626, 8463, 8464, 8465, 113, 4304, 16],\n",
       " [815, 1903, 594, 1122, 2304],\n",
       " [329, 328, 1241, 27, 501, 92, 30],\n",
       " [647, 594, 5627, 2305, 8466],\n",
       " [3005, 5628, 15, 2306],\n",
       " [8467, 8468, 594, 224, 8469, 8470, 152, 8471],\n",
       " [8472, 594],\n",
       " [11, 1023, 56, 115, 1615],\n",
       " [3564, 8473, 12, 852, 594, 5629, 2307, 5630, 3006, 5631, 3564, 8474, 8475],\n",
       " [594, 1122, 1165, 27, 44, 236, 6, 3565, 8476, 5632, 8477],\n",
       " [12, 1123, 29, 4305, 544, 663, 314, 853, 623, 68, 21, 594, 8478],\n",
       " [8479, 3566, 50, 3567, 412],\n",
       " [8480, 8481, 594, 4303, 8482, 5624],\n",
       " [583],\n",
       " [672, 1904, 3, 8483, 1180, 8484, 598, 113, 594, 145],\n",
       " [8485, 2604, 633, 8486, 3568, 601, 8487, 8488, 723, 8489],\n",
       " [8490, 105],\n",
       " [8491, 201, 1170, 8492, 8493, 3569, 8494, 8495, 5633, 1754, 594],\n",
       " [647, 594, 2068],\n",
       " [2603, 722, 1179, 1903, 594, 1122, 8496],\n",
       " [8497, 5627, 113, 8498, 2605],\n",
       " [647, 594, 919, 15, 56, 9, 3, 105, 804],\n",
       " [594, 1762, 1224, 3564, 3006, 14, 2308],\n",
       " [182, 42, 12, 8499, 672, 1904, 716, 594],\n",
       " [3005, 5628, 8500, 2606, 5634, 2034],\n",
       " [158, 55, 249, 1058, 1181, 802, 1023, 46, 2558, 673, 8501],\n",
       " [430, 23, 1076, 24, 8502],\n",
       " [647, 594, 1024, 250, 87],\n",
       " [1227,\n",
       "  664,\n",
       "  192,\n",
       "  1022,\n",
       "  599,\n",
       "  1901,\n",
       "  329,\n",
       "  1902,\n",
       "  3007,\n",
       "  3008,\n",
       "  4306,\n",
       "  3570,\n",
       "  816,\n",
       "  594],\n",
       " [4307, 427, 102, 202, 2, 8503, 594, 879, 3571, 3009],\n",
       " [216, 21, 787, 177, 594, 239, 201],\n",
       " [502, 8504, 2607, 8505, 4308, 25, 8506, 8507],\n",
       " [3010, 8508, 5635, 8509, 8510, 2069],\n",
       " [2070, 882, 25, 883, 4309, 3011, 1905, 644],\n",
       " [212, 854, 25, 14, 119, 933],\n",
       " [5636, 5637, 77, 50, 398, 25, 5556, 1025, 8511],\n",
       " [5638, 350, 5, 5639, 34, 25, 1182, 2, 3, 3012, 5640, 1074],\n",
       " [3572, 746, 18, 65, 194, 257, 48],\n",
       " [8512, 5641, 378, 1616, 530, 25, 223],\n",
       " [3573, 25, 871, 2071, 1906, 8513, 7],\n",
       " [3574, 2309, 8514, 5642, 918, 3575, 398, 25, 1907],\n",
       " [8515, 1883, 25, 215, 815, 747, 531, 8516, 8517],\n",
       " [328, 107, 882, 25, 3576, 482],\n",
       " [747, 25, 14, 119, 933, 1319, 104, 8518, 8519],\n",
       " [212, 93, 503, 934, 2071, 183, 207, 444, 674, 25],\n",
       " [5643, 446, 3013, 166, 2037, 2310, 25, 851, 2608, 4310],\n",
       " [5, 119, 8520, 5, 8521, 8522, 25, 8523, 1763, 8524, 1026],\n",
       " [504, 1504, 2609, 68, 2603, 46, 25, 328, 2610],\n",
       " [3, 398, 25, 356, 4311, 40, 11, 1183, 3014, 748],\n",
       " [4, 151, 1320, 4, 1309, 923, 25, 2072, 39],\n",
       " [8525, 784, 1908, 5644, 724, 25, 2073, 69, 8526, 179, 4312],\n",
       " [5645, 25, 1242, 5645, 1002, 1909, 1762, 4313],\n",
       " [8527, 8528, 8529, 1184, 8530, 25, 8531, 5595, 488, 545, 5646, 8532, 5647],\n",
       " [14, 8533, 107, 193, 2074, 25, 2075, 3576, 482],\n",
       " [8534, 979, 465, 525, 398, 25],\n",
       " [328, 107, 2074, 25, 3576, 482],\n",
       " [473, 8535, 1910, 1911, 849, 1610, 1, 295, 25, 849, 5648, 8536],\n",
       " [1505,\n",
       "  5649,\n",
       "  230,\n",
       "  93,\n",
       "  5650,\n",
       "  162,\n",
       "  299,\n",
       "  2076,\n",
       "  1,\n",
       "  167,\n",
       "  2555,\n",
       "  2611,\n",
       "  623,\n",
       "  399,\n",
       "  3015,\n",
       "  8537],\n",
       " [5651, 5652, 25, 8538, 7, 584, 3575, 8539, 1077, 2612, 433, 619],\n",
       " [2070, 882, 25, 883, 4309, 3011, 1905, 644],\n",
       " [877, 116, 935, 621, 147, 173, 1078, 29, 184, 487, 8540, 377, 25],\n",
       " [8541, 8542, 8543, 8544, 8545, 25, 668, 631, 330, 884],\n",
       " [398, 926, 4314, 8546, 158],\n",
       " [8547, 192, 1595, 2274, 153, 5653, 25],\n",
       " [25, 4279, 198, 1486, 8548, 2311, 54, 529],\n",
       " [8549, 2070, 882, 25, 883, 4309, 3011, 1905, 644, 5654],\n",
       " [2613, 25],\n",
       " [668, 240, 154, 8550],\n",
       " [1391, 104, 5655, 747, 25, 933, 14, 119, 815, 747, 240, 14, 119],\n",
       " [5656, 5657, 5658, 5659, 1293, 67, 96, 240, 2987, 1293, 8551],\n",
       " [8552, 96, 240],\n",
       " [4, 695, 240],\n",
       " [76, 485, 558, 52, 58, 282, 485, 67, 76, 485, 121, 240],\n",
       " [486, 179, 8553, 2077, 31, 2312, 8554, 8555, 2077, 2050, 240, 474],\n",
       " [504, 5660, 240, 1617, 1506, 3, 5661, 8556, 5662],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [3577, 2313, 240, 2078, 2314, 2071, 4315, 8557, 788],\n",
       " [8558, 240, 8559, 68, 1321],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [2069, 450, 16, 3016, 8560, 240, 5663, 875],\n",
       " [113, 695, 240],\n",
       " [3578, 5664, 5665, 8561, 602, 2614, 240, 60, 602, 2615, 3578, 8562],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [5656, 5657, 5658, 5659, 726, 505, 5666, 2987, 240, 603, 8563],\n",
       " [8564, 8565, 240, 3017, 3579, 1124, 8566, 8567, 855],\n",
       " [8568, 8569, 8570, 727, 3580, 1125, 8571, 2, 808, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [240, 2315, 4316, 206, 4317, 878, 166, 5667, 5668, 4318, 5669],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8572, 5670, 8573, 3, 271, 1507, 96, 77, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [566,\n",
       "  1079,\n",
       "  271,\n",
       "  106,\n",
       "  199,\n",
       "  470,\n",
       "  2616,\n",
       "  5671,\n",
       "  585,\n",
       "  240,\n",
       "  1909,\n",
       "  883,\n",
       "  2316,\n",
       "  74,\n",
       "  5672],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8574, 540, 412, 240],\n",
       " [725, 240, 634, 749, 750, 936, 504, 663, 885, 937],\n",
       " [8575, 8576, 459, 1912, 1171, 925, 152, 1027, 5673, 8577],\n",
       " [182, 42, 12, 3018, 3019, 459, 780, 12],\n",
       " [1913, 22, 2, 459],\n",
       " [913, 459, 5674, 780, 913, 459, 8578, 59, 2617, 3581, 813, 3582, 5675, 4319],\n",
       " [3583, 3584, 4320, 5676, 2561, 2562, 3585, 624, 459, 1914, 3586, 4321, 3587],\n",
       " [2079, 1764, 26, 1881, 1508],\n",
       " [147, 751, 5677, 459, 3588, 1605, 564, 3020, 8579, 2289],\n",
       " [3016, 8580, 2317, 2618, 8581, 8582, 913, 459, 8583],\n",
       " [3021, 3018, 3019, 3589, 2318, 980, 1915, 3022, 460, 12, 5678],\n",
       " [604, 1618, 459, 3590, 8584, 8585, 1509, 8586, 8587, 8588],\n",
       " [3583, 379, 117, 925, 3591, 459, 3592, 8589, 8590],\n",
       " [1765, 459, 8591, 1766, 270, 8592, 3593, 8593, 4322, 1185, 3023],\n",
       " [185, 2619, 5679, 1510, 8594, 5680, 459, 8595],\n",
       " [271, 101, 5681, 3024, 535, 1511, 460, 222, 175, 1243, 8596, 8597],\n",
       " [101, 3594, 3595, 2080, 1080, 5682, 3596, 3025, 5683, 459, 6],\n",
       " [3021, 3018, 3019, 3589, 2318, 980, 1915, 3022, 460, 12, 459],\n",
       " [752, 459, 567, 46, 222, 2, 752, 1767],\n",
       " [166, 426, 4323, 19, 5684, 459, 8598, 1768, 127, 122, 753, 1, 849],\n",
       " [8599, 5685, 54, 115, 526, 5686, 459],\n",
       " [1395,\n",
       "  5687,\n",
       "  8600,\n",
       "  8601,\n",
       "  970,\n",
       "  4324,\n",
       "  673,\n",
       "  3597,\n",
       "  459,\n",
       "  1916,\n",
       "  1,\n",
       "  8602,\n",
       "  1412,\n",
       "  8603,\n",
       "  12],\n",
       " [101, 16, 526, 459, 460, 2620, 8604, 8605, 2617],\n",
       " [8606, 8607, 459, 480, 5688, 1236, 801, 8608],\n",
       " [23, 1726, 3598, 23, 4325],\n",
       " [8609, 1322, 8610, 113],\n",
       " [459, 36, 297, 52, 3599, 4, 4326, 8611],\n",
       " [468, 5, 8612, 459, 39],\n",
       " [1170, 44, 4327, 8613, 1917, 460, 12, 459],\n",
       " [459, 99, 2032, 8614, 568, 719],\n",
       " [5684, 459, 540, 540, 444, 671, 624, 624, 2046, 1323, 4328, 122, 4329, 132],\n",
       " [8615, 459, 1173, 2078],\n",
       " [754, 1512, 380, 5689, 2293, 315, 4330, 5690, 85, 426],\n",
       " [2607, 53, 1028, 315, 5691, 181, 1408, 8616, 755, 5, 2319, 3600, 1186],\n",
       " [3026, 669, 1324, 315, 3027, 5692, 5693, 8617],\n",
       " [817, 12, 42, 938, 36, 4331, 315, 5694, 974, 2320, 2321, 8618, 8619],\n",
       " [1244, 173, 3028, 925, 8620, 315, 8621, 8622, 315, 5695, 8623, 8624, 1769],\n",
       " [1510, 2585, 315, 533, 448],\n",
       " [9, 8625, 8626, 8627, 315, 8628, 52, 376, 223, 2621, 1316],\n",
       " [3601, 3602, 63, 315, 1493, 53],\n",
       " [8629,\n",
       "  4332,\n",
       "  5696,\n",
       "  8630,\n",
       "  755,\n",
       "  8631,\n",
       "  101,\n",
       "  8632,\n",
       "  584,\n",
       "  2284,\n",
       "  4333,\n",
       "  3029,\n",
       "  315,\n",
       "  1325,\n",
       "  8633,\n",
       "  8634,\n",
       "  58],\n",
       " [76, 728, 729, 315, 1029, 754, 47, 1063, 1619, 1620, 1413, 163],\n",
       " [77, 616, 648, 696, 177, 1918, 729, 315, 4334, 3030, 4335, 8635, 8636, 4336],\n",
       " [315, 8637, 179],\n",
       " [1126, 315, 1012, 5697, 578],\n",
       " [8638, 3517, 4337, 315],\n",
       " [253, 315, 3031, 925, 2622, 5698],\n",
       " [175, 3603, 3, 23, 8639, 8640, 1919],\n",
       " [730, 1920, 8641, 8642, 315, 8643],\n",
       " [182, 42, 12, 3604, 925, 1614, 5699, 315],\n",
       " [8644, 8645, 778, 315],\n",
       " [8646, 206, 115, 8647, 3, 11, 1621, 8648, 66, 315, 1187, 3605, 675],\n",
       " [8649, 3032, 716, 315],\n",
       " [315, 5700],\n",
       " [754, 1512, 380, 5689, 2293, 315, 4330, 5690, 85, 426],\n",
       " [76, 728, 729, 315, 1029, 754, 130, 1063, 1619, 1620, 1413, 163],\n",
       " [5701, 2623, 605, 5702, 1877, 5703, 3606, 1402],\n",
       " [67,\n",
       "  697,\n",
       "  5704,\n",
       "  8650,\n",
       "  1245,\n",
       "  2988,\n",
       "  5705,\n",
       "  1770,\n",
       "  673,\n",
       "  8651,\n",
       "  119,\n",
       "  315,\n",
       "  8652,\n",
       "  8653],\n",
       " [7, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 1246],\n",
       " [782, 41, 381, 413, 606, 461, 1513, 4296],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [2624, 8654, 381, 1622, 2081, 1414, 461, 1513],\n",
       " [381, 413, 606, 461, 1513, 536, 676, 586, 757],\n",
       " [8655, 2322, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [7, 381, 413, 606, 461, 756, 536, 676, 586, 757, 56, 381, 1771],\n",
       " [381, 102, 4338, 461, 756, 586, 757, 381, 1307, 4338, 1623, 886],\n",
       " [381, 413, 844, 1414, 461, 1513, 8656],\n",
       " [4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 1188, 3035, 1246],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [118,\n",
       "  1725,\n",
       "  1,\n",
       "  1,\n",
       "  4341,\n",
       "  8657,\n",
       "  1189,\n",
       "  1772,\n",
       "  3036,\n",
       "  8658,\n",
       "  461,\n",
       "  4342,\n",
       "  74,\n",
       "  676,\n",
       "  757,\n",
       "  8659],\n",
       " [381, 2625, 606, 886, 3037, 1414, 461, 1513, 536, 676, 586, 757],\n",
       " [36, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 7, 397, 3038, 4343],\n",
       " [19, 8660, 5706, 381, 413, 606, 886, 3037, 461, 1513, 536, 676, 586, 757],\n",
       " [2323, 819, 5707, 381, 413, 606, 5708, 4344, 1246],\n",
       " [36, 381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 844, 1414, 461, 1513, 381, 4345, 5709, 5710, 413, 3039, 5711],\n",
       " [381, 413, 844, 1414, 461, 1513, 3607, 601, 5712],\n",
       " [521, 381, 413, 2081, 87, 8661, 1015, 1912],\n",
       " [641, 8662, 5713, 1224, 461, 5714, 114, 123, 8663, 8664, 5715],\n",
       " [381, 413, 606, 461, 756, 1246],\n",
       " [3608, 8665, 3040, 381, 1622, 2081, 1414, 461, 1513],\n",
       " [1246, 1503, 4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 5716],\n",
       " [1921, 381, 413, 2625, 606, 886, 3037, 1414, 8666],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [381, 1622, 2081, 1414, 461, 1513, 8667],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [381, 413, 2081, 461, 756, 8668, 566, 4346],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757],\n",
       " [1246, 1503, 4339, 461, 586, 3033, 969, 3034, 4340, 491, 52, 818, 5716],\n",
       " [381, 413, 606, 461, 756, 536, 676, 586, 757, 1246],\n",
       " [5717, 717, 3041, 1877, 381, 1622, 4338, 1623, 3037, 886, 3609, 8669, 721],\n",
       " [448, 382, 1326],\n",
       " [182, 42, 12, 8670, 1415, 3042, 5718, 382, 2626, 103, 3014, 3610, 8671],\n",
       " [676, 382, 3036, 1624, 5719, 1409, 586, 717, 3611, 5720, 4347],\n",
       " [8672, 586, 1770, 8673, 4348, 448, 5721, 8674, 382],\n",
       " [8675,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110,\n",
       "  8676],\n",
       " [8677, 160, 101, 427, 526, 939, 382, 23, 818, 1301, 55, 75],\n",
       " [5722, 8678, 17, 4349, 2084, 8679, 382],\n",
       " [382, 635, 451, 8680, 8681, 8682, 8683, 8684, 4350, 383, 7, 2326],\n",
       " [8685,\n",
       "  8686,\n",
       "  714,\n",
       "  139,\n",
       "  382,\n",
       "  66,\n",
       "  4348,\n",
       "  8687,\n",
       "  3612,\n",
       "  43,\n",
       "  2327,\n",
       "  8688,\n",
       "  2630,\n",
       "  1306,\n",
       "  23,\n",
       "  1742],\n",
       " [981, 922, 2269, 3043, 649, 382, 3023, 8689, 8690, 5654],\n",
       " [130,\n",
       "  5723,\n",
       "  8691,\n",
       "  2085,\n",
       "  1747,\n",
       "  3613,\n",
       "  1030,\n",
       "  241,\n",
       "  1922,\n",
       "  5724,\n",
       "  1006,\n",
       "  1486,\n",
       "  1224,\n",
       "  5725,\n",
       "  2328,\n",
       "  2328,\n",
       "  642,\n",
       "  382],\n",
       " [8692, 506, 50, 3044, 9, 5726, 50, 382, 2969, 598, 8693],\n",
       " [5727, 1626, 8694, 10, 1774, 1775, 8695, 382, 13, 8696],\n",
       " [8697, 8698, 8699, 670, 544, 8700, 677, 929, 2603, 382, 886, 31, 252],\n",
       " [448, 382, 1326],\n",
       " [3036, 546, 478, 1627, 382, 4344],\n",
       " [446, 807, 2086, 468, 382, 1005, 1327, 8701, 3045, 2086],\n",
       " [160, 1074, 8702, 8703, 3614, 1247, 3611, 2037, 5728, 382, 5721, 820],\n",
       " [8704,\n",
       "  533,\n",
       "  8705,\n",
       "  2083,\n",
       "  1773,\n",
       "  648,\n",
       "  231,\n",
       "  1625,\n",
       "  8706,\n",
       "  2631,\n",
       "  5729,\n",
       "  1,\n",
       "  8707,\n",
       "  8708,\n",
       "  382,\n",
       "  1021],\n",
       " [1416, 1, 50, 5730, 4266, 2946, 11, 1478, 2, 1302, 8709, 382, 3046],\n",
       " [448, 382, 1326],\n",
       " [617, 4351, 1031, 5731, 5732, 1500, 8710, 382, 8711, 8712, 8713],\n",
       " [8714, 33, 8715, 2065, 8716, 8717, 8718, 2050, 3615, 1110, 15, 8719, 68, 400],\n",
       " [8720,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110],\n",
       " [448, 382, 1326],\n",
       " [8721,\n",
       "  2324,\n",
       "  1,\n",
       "  2627,\n",
       "  2628,\n",
       "  2082,\n",
       "  698,\n",
       "  2083,\n",
       "  1773,\n",
       "  231,\n",
       "  1,\n",
       "  52,\n",
       "  84,\n",
       "  1625,\n",
       "  33,\n",
       "  382,\n",
       "  1,\n",
       "  295,\n",
       "  2325,\n",
       "  1595,\n",
       "  2629,\n",
       "  1110],\n",
       " [8722,\n",
       "  842,\n",
       "  382,\n",
       "  8723,\n",
       "  8724,\n",
       "  8725,\n",
       "  8726,\n",
       "  2325,\n",
       "  2087,\n",
       "  8727,\n",
       "  2324,\n",
       "  5733,\n",
       "  8728,\n",
       "  3616],\n",
       " [1628, 1915, 34, 2632, 483, 382, 427, 1032, 8729],\n",
       " [1033, 676, 3036, 5734],\n",
       " [382, 1502, 8730, 8731, 1328, 940, 3617, 3047, 8732, 1190, 941, 8733],\n",
       " [8734, 633, 4352, 15],\n",
       " [206, 1127, 24, 8735],\n",
       " [26, 789, 8736, 849, 8737],\n",
       " [43, 8738, 16, 581, 1034, 5735, 475, 633, 631, 928, 8739, 252, 5698],\n",
       " [633, 1300],\n",
       " [8740, 506, 35],\n",
       " [8741, 2329, 8742, 5736, 152, 5737, 853, 3618, 8743, 8744],\n",
       " [5738, 474, 1106, 106, 3048, 69, 8745, 633],\n",
       " [1191, 8746, 8747, 8748, 8749, 8750, 143, 3049, 8751, 8752],\n",
       " [35, 821, 633, 556, 5739, 5740],\n",
       " [8753, 8754, 153, 1913, 633],\n",
       " [633, 3619, 249, 1776, 8755, 4353],\n",
       " [1081, 2633, 779, 62, 8756, 1192, 425],\n",
       " [615, 2634, 30, 78, 699, 873, 222, 45, 633, 5741],\n",
       " [212, 790, 2635, 633, 722, 5742, 8757, 441],\n",
       " [1034, 35, 3620],\n",
       " [8758, 11, 2, 4354, 540, 1082, 669, 499, 8759],\n",
       " [182, 42, 12, 8760, 1311, 8761, 130, 633, 3621, 782, 4355, 357, 8762, 8763],\n",
       " [4356, 633, 2330, 1083, 514, 664, 14, 4357, 741, 76, 2331, 8764, 5743],\n",
       " [1322, 23, 3622, 4, 5744],\n",
       " [2332, 2333, 67, 633, 2334, 8765, 255, 478, 8766],\n",
       " [492, 633, 2088, 424, 84, 2, 8767, 8768, 161, 2],\n",
       " [168, 2, 39, 1629, 678, 940],\n",
       " [604, 8769, 3, 2636, 26],\n",
       " [8770, 4, 679, 1, 11, 633],\n",
       " [8771, 545, 26, 633, 813, 625, 378, 81, 5745, 1630, 514],\n",
       " [5746, 7, 5, 1329, 1923, 4358, 1248, 79, 8772, 8773, 1514],\n",
       " [8774, 3623, 633, 3624, 3050, 8775],\n",
       " [182, 42, 12, 1311, 972, 633, 3051, 8776],\n",
       " [615, 2634, 30, 78, 699, 873, 222, 45, 633, 5741, 2637],\n",
       " [8777, 3052, 67, 5, 8778, 973, 633],\n",
       " [8779, 8780, 878, 856, 2089, 604, 556, 633],\n",
       " [5747, 136, 2335, 8781, 69],\n",
       " [8782, 8783, 8784, 3625, 2336, 3626, 927, 39, 8, 633],\n",
       " [8785, 8786, 1515, 3053, 623, 42, 12],\n",
       " [506, 1631],\n",
       " [5748, 8787, 201, 5749, 5750, 283],\n",
       " [8788, 56, 4274, 1084, 8789, 1128, 4359, 8790, 6, 8791, 4360, 8792],\n",
       " [5751, 1, 401, 4361, 1398, 942, 4362],\n",
       " [250, 5752, 719, 401, 64, 5752, 641, 4363],\n",
       " [5753, 115, 401, 80, 138, 502, 8793, 4364, 99, 8794],\n",
       " [115, 879, 458, 298, 5754, 401, 4365, 5755, 36, 2337],\n",
       " [2090, 887, 1924, 6, 4366, 3627, 5756, 5757, 5758, 401, 2638, 248, 3054, 85],\n",
       " [2090, 887, 1924, 6, 4366, 3627, 5756, 5757, 5758, 401, 2638, 248, 3054, 85],\n",
       " [2091, 23, 804, 525, 206, 401, 525, 314],\n",
       " [8795, 8796, 280, 401, 198, 64, 8797, 3628, 3055, 5759, 47],\n",
       " [921, 8798, 925, 8799, 8800, 401, 4367, 758, 8801],\n",
       " [888, 8802, 8803, 87, 8804, 1, 3056, 8805, 8806, 401, 1325],\n",
       " [8807, 1777, 8808],\n",
       " [4, 232, 4368, 700, 1631, 401, 8809],\n",
       " [401, 1498, 8810, 8811, 8812, 77, 1751, 115, 1129, 93],\n",
       " [8813, 8814, 8815, 169, 8816, 401, 80, 2, 138, 5760],\n",
       " [6,\n",
       "  4369,\n",
       "  3629,\n",
       "  3548,\n",
       "  1314,\n",
       "  45,\n",
       "  2639,\n",
       "  2640,\n",
       "  857,\n",
       "  4370,\n",
       "  672,\n",
       "  3057,\n",
       "  1882,\n",
       "  4371,\n",
       "  4372,\n",
       "  3630,\n",
       "  2641],\n",
       " [195, 398, 3631, 4373, 401, 1925, 4290, 3058, 8817],\n",
       " [8818, 8819, 739, 35],\n",
       " [8820, 913, 2642, 1193, 3632, 4374, 4375, 8821, 8822, 8823, 8824, 691],\n",
       " [532, 4376, 889, 401, 80, 142, 532],\n",
       " [4, 581, 181, 3059, 1926, 24, 142, 401, 890, 575, 4, 8825],\n",
       " [4, 401, 1130, 350, 102],\n",
       " [401],\n",
       " [8826, 33, 465, 3633, 3060, 16, 401, 890, 2643],\n",
       " [8827, 401],\n",
       " [8828, 612, 569, 5, 431, 3634, 581, 3628, 2644, 401, 891, 35, 1927],\n",
       " [83, 80, 401, 1516, 8829, 8830, 8831],\n",
       " [8832, 521, 8833, 356, 8834, 401, 80, 5761, 8835, 8836],\n",
       " [8837, 8838, 217, 401],\n",
       " [401, 8839, 8840, 11, 4377, 8841, 414, 2645],\n",
       " [2090,\n",
       "  887,\n",
       "  1924,\n",
       "  129,\n",
       "  3635,\n",
       "  4366,\n",
       "  3627,\n",
       "  4378,\n",
       "  401,\n",
       "  2638,\n",
       "  8842,\n",
       "  1417,\n",
       "  1517,\n",
       "  2,\n",
       "  402],\n",
       " [401, 80, 5762, 166, 3636, 1418, 1083, 5762, 943, 3636, 8843, 1916, 8844],\n",
       " [8845, 8846, 63, 1085, 8847, 1631, 401, 892, 1632, 2957, 8848, 1185, 529],\n",
       " [650, 8849, 175, 153, 1060, 8850, 29, 401],\n",
       " [11, 26, 500, 3061, 2632, 401, 890, 622, 1500],\n",
       " [1633, 8851, 316, 4379, 8852, 8853, 8854],\n",
       " [2092, 3062, 8855, 3063, 3637, 9, 316],\n",
       " [8856, 2646, 564, 8857, 316],\n",
       " [1194, 8858, 3638, 52, 102, 316, 3532, 2982, 8859],\n",
       " [2338, 316, 2, 61, 514],\n",
       " [316, 8860, 24, 442, 206, 2647, 3064, 8861],\n",
       " [8862,\n",
       "  54,\n",
       "  79,\n",
       "  111,\n",
       "  90,\n",
       "  255,\n",
       "  893,\n",
       "  62,\n",
       "  665,\n",
       "  1928,\n",
       "  8863,\n",
       "  58,\n",
       "  1486,\n",
       "  316,\n",
       "  73,\n",
       "  3065],\n",
       " [2648, 2335, 1082, 316],\n",
       " [1929, 90, 316, 8, 84, 1382, 81, 9, 1419, 1195],\n",
       " [2649, 8864, 8865, 560, 1634, 560, 316, 1249],\n",
       " [8866, 2630, 246, 136, 4380, 316],\n",
       " [8867, 67, 84, 1032, 316],\n",
       " [153, 1778, 4381, 316],\n",
       " [696, 8868, 314, 8869, 316, 4, 55],\n",
       " [804, 4382, 1032, 1196, 3639, 5763, 8870, 11, 96, 822, 9, 316, 105],\n",
       " [1930, 316],\n",
       " [8871, 500, 5663, 1086, 316],\n",
       " [9, 1330, 2339, 814, 1884, 8872, 2338, 1518, 316],\n",
       " [5764, 8873, 223, 2, 1322, 2338, 316],\n",
       " [206, 316, 3066, 96, 3067, 1931],\n",
       " [8874, 2340, 3640, 153, 223, 5765, 316, 252, 105],\n",
       " [8875, 506, 4, 316, 398, 5766],\n",
       " [8876, 3068, 3604, 2650, 26, 4383, 131, 3069, 176, 603, 54, 466, 280, 316],\n",
       " [8877, 8878, 1622, 316],\n",
       " [8879, 26, 15, 90, 316, 398, 5767, 4328, 4384],\n",
       " [8880, 837, 271, 316, 546, 176],\n",
       " [8881, 316, 1087, 11, 26, 169],\n",
       " [1197, 55, 3070, 90, 316, 1322, 161],\n",
       " [8882, 33, 8883, 316, 95, 412, 677, 1779, 2651, 75, 599],\n",
       " [8884, 316, 8885, 3071, 168, 297, 8886, 223],\n",
       " [1930, 316, 631, 8887],\n",
       " [3641, 927, 316],\n",
       " [8888, 8889, 132, 8890, 271, 5768, 566, 3631, 316],\n",
       " [8891, 1198, 841, 560, 316],\n",
       " [56, 176, 412, 839, 661, 5769, 569, 151, 102, 4385, 316],\n",
       " [8892, 1322, 3642, 317, 525, 662],\n",
       " [8893, 84, 3, 317, 33, 116, 32],\n",
       " [317, 5770, 8894, 169, 808, 1330, 808, 3072, 5771],\n",
       " [132, 8895, 581, 403, 317],\n",
       " [3073, 3, 317, 1929, 5772, 4386, 236, 478, 35],\n",
       " [564, 584, 8896, 317, 759, 2652, 8897, 8898],\n",
       " [8899, 8900, 450, 15, 2, 317, 198, 15, 5773, 11, 3585, 1416, 2341],\n",
       " [631, 8, 856, 8901, 26, 691, 1519, 598, 317, 602, 5774, 90, 4387],\n",
       " [50, 3, 317, 8902, 3643, 175, 808, 8903, 35],\n",
       " [5775, 31, 891, 3644, 928, 1484, 2093, 760, 317, 2949, 4388, 662, 4389],\n",
       " [1635, 481, 2631, 3045, 56, 890, 317, 810, 1737],\n",
       " [1420, 317, 2653, 966, 2342, 8904],\n",
       " [4390, 8905, 317],\n",
       " [8906, 2654, 3, 317, 2949, 377, 117, 23, 255],\n",
       " [3, 2343, 891, 8907, 58, 317, 8908, 198, 8909, 8910],\n",
       " [100, 3, 317, 8911],\n",
       " [317, 3645, 661],\n",
       " [4286, 4391, 377, 8912, 317, 4330, 644, 844, 195, 499, 54, 102, 8913],\n",
       " [8914,\n",
       "  16,\n",
       "  4392,\n",
       "  4393,\n",
       "  8915,\n",
       "  1199,\n",
       "  33,\n",
       "  4394,\n",
       "  1395,\n",
       "  1,\n",
       "  317,\n",
       "  621,\n",
       "  3074,\n",
       "  578,\n",
       "  69],\n",
       " [5776, 5777, 1, 3643, 317],\n",
       " [579, 8916, 2580, 487, 1, 67, 79, 218, 317, 4395, 121, 46],\n",
       " [317, 2094],\n",
       " [152, 16, 317, 397, 162, 1167, 486, 5776],\n",
       " [8917, 2, 602, 317],\n",
       " [8918, 12, 317],\n",
       " [8919, 1742, 58, 84, 55, 2344, 4396, 5778, 317],\n",
       " [1024, 300, 2095, 579, 317, 378, 1250, 2065, 1421],\n",
       " [1200, 4397, 317, 2096, 2655, 5779, 8920, 4349, 5780, 8921, 8922],\n",
       " [67, 8923, 1131, 810, 84, 2, 470, 317],\n",
       " [4398, 67, 253, 216, 3075, 317],\n",
       " [3076, 3, 317, 378],\n",
       " [1085, 317, 105],\n",
       " [8924, 8925, 5781, 525, 5782, 806, 5783, 317, 170, 1236],\n",
       " [8926, 8927, 101, 3646, 2, 874, 8928, 1072, 452],\n",
       " [8929, 1932, 481, 3647, 5784, 5785, 661, 452, 5786, 1422],\n",
       " [4399,\n",
       "  431,\n",
       "  96,\n",
       "  596,\n",
       "  36,\n",
       "  8930,\n",
       "  8931,\n",
       "  452,\n",
       "  5787,\n",
       "  8932,\n",
       "  3077,\n",
       "  487,\n",
       "  944,\n",
       "  626,\n",
       "  4400],\n",
       " [8933, 452],\n",
       " [850, 2097],\n",
       " [8934, 1128, 1384, 3648, 31, 1132, 1780, 452, 131, 8935, 791, 57, 57],\n",
       " [2656, 4401, 858, 4402, 452, 1933],\n",
       " [5, 119, 8936, 8937, 5788, 452, 1251, 218, 3051],\n",
       " [3049, 1781, 4403, 1781, 448, 452, 118, 1176],\n",
       " [3078, 8938, 8939, 838, 894, 452, 15, 23],\n",
       " [4404, 8940, 2288, 8941, 8942, 452, 8943, 3626, 5789, 8944, 1181, 2657, 5790],\n",
       " [5791, 3649, 1252, 452, 2345, 2273, 3079],\n",
       " [99, 1079, 79, 888, 452, 8945, 2302, 2346, 4405, 7],\n",
       " [8946, 506, 63, 5, 452, 1407],\n",
       " [1782, 8947, 651, 8948, 452, 8949, 8950, 8951, 3650, 8952, 4406],\n",
       " [3049, 1781, 4403, 1781, 448, 452, 118, 1176],\n",
       " [2054, 31, 8953, 8954, 216, 412, 5792, 8955, 147, 3651, 625, 26, 2347],\n",
       " [8956, 376, 8957, 1001, 452],\n",
       " [8958, 452, 521, 809],\n",
       " [8959, 8960, 452, 8961],\n",
       " [8962, 452, 46, 69, 940, 9],\n",
       " [2658, 2291, 452, 241, 1201, 8963, 8964, 8965],\n",
       " [2, 1934, 969, 1913, 8966, 45, 3027, 8967, 50, 1932, 151, 452],\n",
       " [8968, 873, 1636, 87, 1423],\n",
       " [8969, 8970, 1067, 295, 1424, 1293, 452, 524, 8971],\n",
       " [84, 547, 3651, 3080, 219, 8972, 8973, 277, 452, 8974, 2348, 3081, 449],\n",
       " [5793, 8975, 5784, 67, 761, 4407, 83, 452, 297],\n",
       " [5793, 3647, 583, 58, 1175, 184, 5786, 2098, 4374, 452],\n",
       " [1121, 3652, 318, 452, 1599, 1, 2099, 504, 821],\n",
       " [2349, 742, 8976, 982, 162, 3082, 330, 1062, 8977, 1637, 3653, 1638],\n",
       " [8978, 895, 8979, 8980, 452, 5794, 8981],\n",
       " [3083, 452, 8982, 8983],\n",
       " [2659, 8984, 469, 934, 1247, 2350, 478, 314, 4385],\n",
       " [1133, 526, 529, 792],\n",
       " [5, 2351, 4408, 1007, 314, 885, 548],\n",
       " [548, 3654, 166, 895, 8985],\n",
       " [8986, 8987, 3, 489, 548, 549, 5795, 56, 669, 8988, 2977, 166, 1180, 1783],\n",
       " [548, 8989, 38, 8990, 8991, 4409, 793, 52, 8992, 24, 1418],\n",
       " [77, 1067, 5796, 548, 614, 106, 919, 3655, 236],\n",
       " [5797, 8993, 1331, 5797, 486, 5798, 42, 8994, 2, 841, 87],\n",
       " [4226],\n",
       " [856, 8995, 5799, 8996, 330, 1120, 8997, 3, 84, 1507],\n",
       " [548, 896, 8998, 700, 8999, 3507, 49, 1935, 3656, 1088, 2660],\n",
       " [548, 9000, 5800],\n",
       " [3084, 2661, 9001, 9002, 548],\n",
       " [4410, 5801, 1088, 1018, 511],\n",
       " [4411, 3085, 5798],\n",
       " [1639, 2060, 2577],\n",
       " [5, 9003, 1520, 9004, 745, 1332, 5802],\n",
       " [9005, 77, 3657, 548, 9006, 45, 9007, 5803, 5804, 5805, 9008, 1333],\n",
       " [64, 253, 3655, 3582, 3086, 548],\n",
       " [4412, 9009, 548, 9010],\n",
       " [548, 3654, 3087, 62],\n",
       " [9011, 127, 9012, 5806, 548, 35],\n",
       " [548, 2100, 5750],\n",
       " [548, 3654, 3087, 2, 680],\n",
       " [9013, 255, 255, 9014, 3088, 1077, 69, 1784, 548, 1253],\n",
       " [3087, 644, 9015],\n",
       " [16, 675, 5807, 2352, 1785],\n",
       " [35, 3582, 3086, 548],\n",
       " [9016, 3, 713, 9017, 2053],\n",
       " [77, 475, 5808, 9018, 548],\n",
       " [147, 921, 548],\n",
       " [301, 7, 9019, 548, 9020, 1334, 2652],\n",
       " [3075, 243, 1009, 5809, 548, 925, 5810],\n",
       " [945, 4413, 5807],\n",
       " [1008, 3655, 475, 5811, 548, 176],\n",
       " [353,\n",
       "  779,\n",
       "  15,\n",
       "  761,\n",
       "  4414,\n",
       "  168,\n",
       "  2,\n",
       "  5812,\n",
       "  548,\n",
       "  2034,\n",
       "  15,\n",
       "  467,\n",
       "  751,\n",
       "  2101,\n",
       "  2353,\n",
       "  3658],\n",
       " [3087, 1640, 918],\n",
       " [1389, 214],\n",
       " [1335, 9021, 1521, 730, 214, 1522, 1035, 4, 818, 739],\n",
       " [946, 9022, 541, 214, 1066, 1425],\n",
       " [1036, 4415, 250, 1757, 607, 214, 1641, 2354, 1520, 2102, 79, 546],\n",
       " [1336, 9023, 3026, 214, 9024, 5670, 9025, 1642],\n",
       " [214, 330, 1202, 1523, 9026, 9027, 184, 893, 617, 5813, 9028, 17],\n",
       " [812, 2, 214, 115, 155],\n",
       " [1035, 214],\n",
       " [8, 4416, 15, 3065, 2355, 214, 4222, 214],\n",
       " [483, 9029, 4417, 195, 712, 214, 583, 300, 1936],\n",
       " [9030, 880, 625, 2312, 1037, 69, 84, 560, 214, 2053, 1038],\n",
       " [9031, 9032, 283, 2046, 214, 1786, 4418, 1254, 731, 3659, 5814],\n",
       " [214, 1786, 1925, 11, 62, 2328, 105],\n",
       " [9033, 214, 545, 9034],\n",
       " [54, 486, 8, 1926, 2662, 11, 248, 214, 1786, 43, 4419, 9035],\n",
       " [603, 9036, 622, 481, 1089, 214],\n",
       " [1640, 282, 8, 792, 9037, 2103, 762, 9038, 214, 31, 1086],\n",
       " [24, 3660, 9039, 5815, 1337, 8, 214, 2575, 5815, 5816],\n",
       " [9040, 557, 1398, 730, 214],\n",
       " [924, 214],\n",
       " [1309, 9041, 3567, 214, 3639, 9042],\n",
       " [791, 5817, 214],\n",
       " [41, 9043, 121, 3089, 3661, 214, 302, 3601, 576, 5818, 9044],\n",
       " [29, 1787, 1203, 102, 9045, 891, 2356, 1019, 214],\n",
       " [1039, 9046, 214, 16],\n",
       " [9047, 9048, 679, 485],\n",
       " [2608, 5819, 100, 763, 214, 9049, 3090],\n",
       " [9050, 9051, 9052, 214, 9053, 9054, 496, 9055, 9056],\n",
       " [9057, 1191, 85, 166, 4420, 5, 617, 5820, 2104, 9, 111],\n",
       " [9058, 1134, 214],\n",
       " [9059, 3091, 5821, 2663, 9060, 9061, 673, 24, 4421, 214, 979],\n",
       " [3092, 2664, 214, 3662, 3663, 462, 9062, 4422, 79, 102, 3093, 4423],\n",
       " [1643, 9063, 9064, 1477, 148],\n",
       " [5822, 216, 2648, 1132, 2648, 1932, 2, 214, 9065],\n",
       " [1756, 3664, 2105, 2649, 5517, 886, 214],\n",
       " [9066, 180, 488, 67],\n",
       " [9067, 180, 3641, 9068, 499],\n",
       " [9069, 180, 5823],\n",
       " [2665, 716, 759, 159, 9070, 3637, 9071, 5824, 6, 3094, 9072],\n",
       " [9073, 632, 5825, 596, 9074, 9075, 180, 75, 9076, 9077],\n",
       " [4, 1788, 180, 3665],\n",
       " [9078, 9079, 1495, 170, 180, 1937, 4424],\n",
       " [5826, 3666, 5827, 520, 121, 5828],\n",
       " [54,\n",
       "  180,\n",
       "  529,\n",
       "  4425,\n",
       "  1762,\n",
       "  814,\n",
       "  2357,\n",
       "  696,\n",
       "  6,\n",
       "  1010,\n",
       "  1497,\n",
       "  3667,\n",
       "  9080,\n",
       "  9081,\n",
       "  5829],\n",
       " [1203, 530, 83, 180, 412, 1038],\n",
       " [9082, 931, 1236, 661, 680, 4426, 9083, 180, 9084, 3668, 2358, 2, 180],\n",
       " [180, 4427, 5830, 4427],\n",
       " [9085, 806, 560, 9086, 4428, 430, 3669, 9087, 525, 180, 625, 3095, 44],\n",
       " [9088, 9089, 170, 90, 180, 121, 967, 5831],\n",
       " [5832, 180, 1604, 162, 9090, 5],\n",
       " [180, 488, 24, 1016, 77, 3, 1524, 356, 9091, 79, 67],\n",
       " [525, 180, 80],\n",
       " [9092, 9, 180, 3670, 9093],\n",
       " [26, 56, 1204, 476, 1, 77, 662, 1245, 4429, 9094, 1783, 1204, 180, 5833],\n",
       " [5834, 5835, 845, 535, 3671, 9095, 587, 2284, 180, 5836, 5572],\n",
       " [4, 1937, 3605, 67, 1128, 4430, 180, 9096, 5837],\n",
       " [62, 43, 3093, 3, 2551, 2044, 930, 180, 243, 3, 2551],\n",
       " [9097, 50, 9, 180, 9098],\n",
       " [2295, 1426, 939, 180, 5838, 180, 9099, 2295, 1426, 9100, 9101],\n",
       " [9102, 9103, 2359, 180, 5839],\n",
       " [180, 1604, 3096, 9104, 5840],\n",
       " [9105, 9106, 9107, 9108, 9109, 9110, 9111, 814, 180, 442, 1067],\n",
       " [1338, 180, 2666],\n",
       " [1135, 1644, 557, 24, 9, 180, 1135, 4224, 9112],\n",
       " [9113, 2667, 9114, 180, 111, 106, 9115, 2106, 1136, 4431],\n",
       " [529, 466, 3044, 812, 9116, 180, 1604, 3672, 3551],\n",
       " [1226, 410, 430, 9117, 499, 12, 9118, 9119, 971, 1205, 852, 9120],\n",
       " [206, 3, 1200, 972, 6, 357, 180, 4432, 9121, 9122],\n",
       " [5841,\n",
       "  3097,\n",
       "  57,\n",
       "  9123,\n",
       "  9124,\n",
       "  57,\n",
       "  5841,\n",
       "  3097,\n",
       "  57,\n",
       "  1743,\n",
       "  9125,\n",
       "  5842,\n",
       "  1006,\n",
       "  2,\n",
       "  180,\n",
       "  5843],\n",
       " [76, 1135, 3098, 180, 9126, 2668],\n",
       " [9127, 760, 3070, 4433, 121, 492, 284, 9128, 2669, 3099],\n",
       " [9129, 6, 667, 2094, 284, 1738, 9130, 630, 55, 1927, 3673],\n",
       " [5844, 9131, 550, 3, 550, 1938, 284, 587, 3100, 1525, 2, 3101, 9132],\n",
       " [525, 9133, 23, 284, 3674],\n",
       " [1526, 284, 81],\n",
       " [4434, 9134, 90, 284, 9135],\n",
       " [9136, 545, 1185, 598, 9137, 1, 378, 608, 284, 511],\n",
       " [282, 143, 9138, 9139, 2299, 101, 3675, 425, 284, 353, 9140, 34, 1090, 5845],\n",
       " [29, 1255, 9141, 284, 280, 9142],\n",
       " [2094, 284, 1111],\n",
       " [9143, 5846, 844, 284, 2107, 85, 541],\n",
       " [779, 415, 716, 1427, 9144, 284, 9145, 3071, 2360, 5847, 1130],\n",
       " [9146, 67, 5848, 5849, 5850, 608, 280, 284, 9147],\n",
       " [2054, 9148, 1206, 284, 9149, 147, 2298, 918, 4435, 9150],\n",
       " [632, 84, 9151, 9152, 87, 41, 23, 284, 8, 9153, 2670, 3078, 3676],\n",
       " [9154, 23, 968, 377, 4436, 1645, 1137, 1255, 284],\n",
       " [16, 856, 284, 358, 24, 1062],\n",
       " [2361, 402, 284, 297, 4437, 1789, 1202, 597, 2569],\n",
       " [9155, 465, 2636, 502, 9156, 284, 5851, 483, 4438],\n",
       " [9157, 9158, 5852, 3677, 284, 297, 609, 9159, 9160, 117],\n",
       " [9161, 9162, 9163, 61, 4439, 551, 3102, 3102, 284, 297],\n",
       " [9164, 198, 2049, 284, 764, 9165, 250, 1109, 1387],\n",
       " [5853, 9166, 5854, 5855, 2362, 284, 147, 4440],\n",
       " [412,\n",
       "  543,\n",
       "  9167,\n",
       "  1939,\n",
       "  3678,\n",
       "  9168,\n",
       "  4241,\n",
       "  284,\n",
       "  9169,\n",
       "  5856,\n",
       "  9170,\n",
       "  9171,\n",
       "  1,\n",
       "  2363,\n",
       "  9172],\n",
       " [9173, 9174, 2942, 284, 3, 5857, 62, 102, 9175],\n",
       " [3679, 9176, 9177, 1726, 727, 3680, 3681, 4441, 15, 6, 9178, 3682, 284],\n",
       " [9179,\n",
       "  222,\n",
       "  838,\n",
       "  91,\n",
       "  1175,\n",
       "  284,\n",
       "  442,\n",
       "  55,\n",
       "  2108,\n",
       "  716,\n",
       "  280,\n",
       "  613,\n",
       "  1232,\n",
       "  1035,\n",
       "  449],\n",
       " [9180, 2, 284, 4442],\n",
       " [668, 6, 3, 284, 9181, 180, 9182, 183],\n",
       " [888, 4443, 284],\n",
       " [9183, 377, 9184, 284, 253, 26, 4444, 1428, 9185, 1199, 43, 5858, 783],\n",
       " [9186, 9187, 125, 3, 23, 9188, 284, 9189, 9190],\n",
       " [9191, 9192, 9193, 9194, 9195, 283, 2109, 284, 9196, 2671, 9197, 617],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [1245, 2085, 2, 31, 53, 427, 115, 17, 217, 83, 1940, 26],\n",
       " [9198, 9199, 58, 96, 9200, 5859, 1180, 17, 217],\n",
       " [1527,\n",
       "  3103,\n",
       "  9201,\n",
       "  9202,\n",
       "  133,\n",
       "  17,\n",
       "  2364,\n",
       "  217,\n",
       "  1497,\n",
       "  2672,\n",
       "  4445,\n",
       "  2365,\n",
       "  694,\n",
       "  243,\n",
       "  814,\n",
       "  33],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [1027, 1941, 9203, 2110, 463, 217, 133, 17, 217],\n",
       " [175, 9204, 4446, 217, 463, 133, 17, 4447, 9205, 5860, 9206, 5608, 426],\n",
       " [164, 2112, 1, 17, 3052, 9207, 217, 9208, 813, 5861, 9209, 4448],\n",
       " [5, 764, 463, 983, 984, 241, 133, 17, 217, 1138, 694, 897, 1139, 402],\n",
       " [5, 137, 314, 17, 217, 2674, 1193, 4395, 813],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 19, 1587],\n",
       " [2675,\n",
       "  9210,\n",
       "  1640,\n",
       "  475,\n",
       "  448,\n",
       "  4,\n",
       "  1338,\n",
       "  4449,\n",
       "  1300,\n",
       "  327,\n",
       "  90,\n",
       "  4450,\n",
       "  243,\n",
       "  17,\n",
       "  217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 85],\n",
       " [1773, 1027, 1941, 498, 5862, 133, 17, 463, 217, 2110, 85],\n",
       " [175, 2366, 9211, 9212, 4288, 463, 133, 17, 217, 498, 694, 9213, 426],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [5, 137, 314, 17, 217, 2674, 1193, 4395, 813],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 19, 1587],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [5, 241, 984, 1138, 694, 764, 463, 983, 133, 17, 217, 731, 2113, 402],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [24, 398, 1035, 9214, 17, 217],\n",
       " [984, 897, 1528, 5863, 1027, 1941, 9215, 9216, 463, 133, 17, 217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [984, 897, 1528, 2366, 2114, 1139, 2115, 217, 133, 17, 9217],\n",
       " [3103,\n",
       "  694,\n",
       "  29,\n",
       "  217,\n",
       "  2364,\n",
       "  2116,\n",
       "  3683,\n",
       "  1427,\n",
       "  1340,\n",
       "  515,\n",
       "  133,\n",
       "  17,\n",
       "  199,\n",
       "  4451,\n",
       "  1317],\n",
       " [1027, 1941, 2110, 2673, 1339, 2111, 2100, 133, 17, 217, 85, 426],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859],\n",
       " [9218, 3634, 9219, 4, 630, 17, 217],\n",
       " [5, 764, 463, 983, 984, 1138, 694, 1429, 1139, 133, 17, 217, 859, 85],\n",
       " [9220, 9221, 4, 4, 898, 618, 1169, 68, 60, 17, 217],\n",
       " [3104, 9222, 195, 4, 331, 17, 9223, 4, 898, 808, 31, 2676],\n",
       " [9224, 4452, 331, 17, 808, 1942, 80],\n",
       " [652, 77, 17, 331, 794],\n",
       " [9225, 9226, 652, 985, 891, 1943, 331, 40, 17, 986],\n",
       " [2677, 33, 4453, 577, 2367, 669, 652, 3047, 651, 17, 331, 2, 1009, 9227],\n",
       " [4454, 2117, 1223, 531, 2678, 794, 2679, 2, 1085, 1, 652, 17, 331, 125, 1404],\n",
       " [3105, 17, 331, 3521],\n",
       " [9228, 9229, 9230, 378, 1646, 17, 331],\n",
       " [667, 17, 331, 9231],\n",
       " [9232, 9233, 1341, 17, 331],\n",
       " [17, 331, 2368],\n",
       " [327, 695, 841, 1177, 24, 1422, 39, 9234, 17, 331, 5864],\n",
       " [9235, 9236, 3684, 819, 198, 17, 331, 1034, 9237],\n",
       " [17, 331, 41, 23, 119, 3685],\n",
       " [9238, 83, 331, 5865, 1109, 81, 791, 5866, 1647, 176],\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the sequence\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d296f03",
   "metadata": {
    "papermill": {
     "duration": 0.034848,
     "end_time": "2024-06-02T10:22:25.687339",
     "exception": false,
     "start_time": "2024-06-02T10:22:25.652491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Next, we see the maximum length of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0158e3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:25.759270Z",
     "iopub.status.busy": "2024-06-02T10:22:25.758916Z",
     "iopub.status.idle": "2024-06-02T10:22:26.048196Z",
     "shell.execute_reply": "2024-06-02T10:22:26.047323Z"
    },
    "papermill": {
     "duration": 0.328014,
     "end_time": "2024-06-02T10:22:26.050119",
     "exception": false,
     "start_time": "2024-06-02T10:22:25.722105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9500689655172414"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see sentence length\n",
    "word_count = [len(x0) for x0 in sequence]\n",
    "plt.hist(word_count,bins=max(word_count))\n",
    "plt.show()\n",
    "len([x0 for x0 in word_count if x0<=14])/len(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef13ced",
   "metadata": {
    "papermill": {
     "duration": 0.032894,
     "end_time": "2024-06-02T10:22:26.116724",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.083830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see most of texts' numbers of words are equal to or less than 14, so we set maximum length of a sentence as 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c1ce9",
   "metadata": {
    "papermill": {
     "duration": 0.033363,
     "end_time": "2024-06-02T10:22:26.182871",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.149508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We set the parameter â€˜paddingâ€™ as â€˜preâ€™, which implies that if the number of words of sentences is shorter than 14, the shortcomings at the front of sentences will be filled with 0. Of course, if the number of words of sentences is larger than 14, we will keep the last 14 words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62d29e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:26.250986Z",
     "iopub.status.busy": "2024-06-02T10:22:26.250214Z",
     "iopub.status.idle": "2024-06-02T10:22:26.299468Z",
     "shell.execute_reply": "2024-06-02T10:22:26.298565Z"
    },
    "papermill": {
     "duration": 0.085205,
     "end_time": "2024-06-02T10:22:26.301324",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.216119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  135, 1582, 3503],\n",
       "       [   0,    0,    0, ..., 8061, 8062, 1223],\n",
       "       [   0,    0,    0, ...,  575, 1474, 1055],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  403,  307,  664],\n",
       "       [   0,    0,    0, ...,  138, 1577, 1715],\n",
       "       [   0,    0,    0, ...,   10,  140, 2883]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=pad_sequences(sequence,maxlen=14,padding='pre')\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15656f46",
   "metadata": {
    "papermill": {
     "duration": 0.03288,
     "end_time": "2024-06-02T10:22:26.367086",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.334206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Then we split the training, validation and testing data. We use training data to train the model, and we use the validation data to select a trained model. Then we predict the classification of testing data using selected trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05ee1792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:26.436235Z",
     "iopub.status.busy": "2024-06-02T10:22:26.435956Z",
     "iopub.status.idle": "2024-06-02T10:22:26.440265Z",
     "shell.execute_reply": "2024-06-02T10:22:26.439353Z"
    },
    "papermill": {
     "duration": 0.04022,
     "end_time": "2024-06-02T10:22:26.442147",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.401927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = all_data[:len(df_train)]\n",
    "test_data = all_data[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e8521a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:26.508925Z",
     "iopub.status.busy": "2024-06-02T10:22:26.508612Z",
     "iopub.status.idle": "2024-06-02T10:22:26.515736Z",
     "shell.execute_reply": "2024-06-02T10:22:26.514916Z"
    },
    "papermill": {
     "duration": 0.042579,
     "end_time": "2024-06-02T10:22:26.517593",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.475014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "X_train,X_val,y_train,y_val=train_test_split(train_data,df_train['target'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54068bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:26.584461Z",
     "iopub.status.busy": "2024-06-02T10:22:26.584205Z",
     "iopub.status.idle": "2024-06-02T10:22:26.597467Z",
     "shell.execute_reply": "2024-06-02T10:22:26.596636Z"
    },
    "papermill": {
     "duration": 0.04917,
     "end_time": "2024-06-02T10:22:26.599432",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.550262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp0=[]\n",
    "for x0 in y_train:\n",
    "    if x0==1:\n",
    "        temp0.append([0,1])\n",
    "    else:\n",
    "        temp0.append([1,0])\n",
    "y_train=np.array(temp0)\n",
    "temp0=[]\n",
    "for x0 in y_val:\n",
    "    if x0==1:\n",
    "        temp0.append([0,1])\n",
    "    else:\n",
    "        temp0.append([1,0])\n",
    "y_val=np.array(temp0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e67ba0",
   "metadata": {
    "papermill": {
     "duration": 0.033006,
     "end_time": "2024-06-02T10:22:26.665580",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.632574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Then we build embedding weights matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a8f17",
   "metadata": {
    "papermill": {
     "duration": 0.033389,
     "end_time": "2024-06-02T10:22:26.731917",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.698528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We use pre-trained 200-dimension word-to-vector model to build the embedding weights matrix. It is named â€˜twitter.27Bâ€™ and all words are from twitter texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff4deda8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:22:26.799438Z",
     "iopub.status.busy": "2024-06-02T10:22:26.799155Z",
     "iopub.status.idle": "2024-06-02T10:30:06.037889Z",
     "shell.execute_reply": "2024-06-02T10:30:06.036584Z"
    },
    "papermill": {
     "duration": 459.274793,
     "end_time": "2024-06-02T10:30:06.039771",
     "exception": false,
     "start_time": "2024-06-02T10:22:26.764978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/glove/glove.twitter.27B.zip: 1.52GB [04:46, 5.31MB/s]                            \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1193513/1193514 [02:21<00:00, 8453.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 10.4 s, total: 3min 3s\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#download the word-to-vector\n",
    "w_2_v = torchtext.vocab.GloVe(name='twitter.27B',dim=200,cache='/kaggle/working/glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d413",
   "metadata": {
    "papermill": {
     "duration": 0.38053,
     "end_time": "2024-06-02T10:30:06.771610",
     "exception": false,
     "start_time": "2024-06-02T10:30:06.391080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We set the dimension of the embedding matrix as (20575+1)Ã—200. 200 is the dimension of word-to-vectors. 20575 is the number of different words. However, what is the remaining 1? It means the padding of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c0b7242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:07.521221Z",
     "iopub.status.busy": "2024-06-02T10:30:07.520740Z",
     "iopub.status.idle": "2024-06-02T10:30:07.828362Z",
     "shell.execute_reply": "2024-06-02T10:30:07.827494Z"
    },
    "papermill": {
     "duration": 0.658908,
     "end_time": "2024-06-02T10:30:07.830797",
     "exception": false,
     "start_time": "2024-06-02T10:30:07.171889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.12293   ,  0.20598   ,  0.056996  , ...,  0.45835999,\n",
       "         0.13722   ,  0.66996998],\n",
       "       [-0.015537  ,  0.11158   , -0.23599   , ..., -0.037577  ,\n",
       "        -0.13539   ,  0.45965001],\n",
       "       ...,\n",
       "       [ 0.60323   ,  0.38554999, -0.014256  , ...,  0.27281001,\n",
       "         0.35587999, -0.099306  ],\n",
       "       [ 0.36585999, -0.31378001,  0.73769999, ...,  0.01626   ,\n",
       "         0.1302    , -0.16039   ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((20575+1,200))\n",
    "for k,v in word_index.items():\n",
    "    embedding_matrix[v]=w_2_v[k]\n",
    "    \n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0607d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:08.519000Z",
     "iopub.status.busy": "2024-06-02T10:30:08.518621Z",
     "iopub.status.idle": "2024-06-02T10:30:08.524106Z",
     "shell.execute_reply": "2024-06-02T10:30:08.523285Z"
    },
    "papermill": {
     "duration": 0.348548,
     "end_time": "2024-06-02T10:30:08.525963",
     "exception": false,
     "start_time": "2024-06-02T10:30:08.177415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20576, 200)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the dimension\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eef865",
   "metadata": {
    "papermill": {
     "duration": 0.342899,
     "end_time": "2024-06-02T10:30:09.208561",
     "exception": false,
     "start_time": "2024-06-02T10:30:08.865662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Remove used data to save disk volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "971a59d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:09.939715Z",
     "iopub.status.busy": "2024-06-02T10:30:09.939354Z",
     "iopub.status.idle": "2024-06-02T10:30:10.830218Z",
     "shell.execute_reply": "2024-06-02T10:30:10.829357Z"
    },
    "papermill": {
     "duration": 1.236673,
     "end_time": "2024-06-02T10:30:10.832688",
     "exception": false,
     "start_time": "2024-06-02T10:30:09.596015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('/kaggle/working/glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7391f0a",
   "metadata": {
    "papermill": {
     "duration": 0.364244,
     "end_time": "2024-06-02T10:30:11.544249",
     "exception": false,
     "start_time": "2024-06-02T10:30:11.180005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Model Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee3830",
   "metadata": {
    "papermill": {
     "duration": 0.343861,
     "end_time": "2024-06-02T10:30:12.235935",
     "exception": false,
     "start_time": "2024-06-02T10:30:11.892074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We use BiLSTM model to do the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82775db",
   "metadata": {
    "papermill": {
     "duration": 0.341926,
     "end_time": "2024-06-02T10:30:12.922257",
     "exception": false,
     "start_time": "2024-06-02T10:30:12.580331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### A bidirectional LSTM (BiLSTM) layer is an RNN layer that learns bidirectional long-term dependencies between time steps of time-series or sequence data. These dependencies can be useful when you want the RNN to learn from the complete time series at each time step. (Citation: https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.bilstmlayer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8cca4",
   "metadata": {
    "papermill": {
     "duration": 0.386875,
     "end_time": "2024-06-02T10:30:13.655305",
     "exception": false,
     "start_time": "2024-06-02T10:30:13.268430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here, we set a 4-layer BiLSTM model, each layer is a bidirectional layer and has 128 nodes. The fully connected layerâ€™s activation is ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3405e090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:14.344841Z",
     "iopub.status.busy": "2024-06-02T10:30:14.344476Z",
     "iopub.status.idle": "2024-06-02T10:30:17.638005Z",
     "shell.execute_reply": "2024-06-02T10:30:17.637051Z"
    },
    "papermill": {
     "duration": 3.64192,
     "end_time": "2024-06-02T10:30:17.643354",
     "exception": false,
     "start_time": "2024-06-02T10:30:14.001434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 14, 200)           4115200   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 14, 256)           336896    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 14, 256)           394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 14, 256)           394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 256)               394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5701122 (21.75 MB)\n",
      "Trainable params: 1585922 (6.05 MB)\n",
      "Non-trainable params: 4115200 (15.70 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0=tf.keras.models.Sequential()\n",
    "model0.add(tf.keras.layers.Embedding(input_dim=20575+1,output_dim=200,input_length=14,weights=[embedding_matrix],trainable=False))\n",
    "model0.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,return_sequences=True,dropout=0.2,seed=None)))\n",
    "model0.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,return_sequences=True,dropout=0.2,seed=None)))\n",
    "model0.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,return_sequences=True,dropout=0.2,seed=None)))\n",
    "model0.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,return_sequences=False,dropout=0.2,seed=None)))\n",
    "model0.add(tf.keras.layers.Dense(128*2,activation=tf.nn.elu))\n",
    "model0.add(tf.keras.layers.Dense(2,activation=tf.nn.relu))\n",
    "\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe3706",
   "metadata": {
    "papermill": {
     "duration": 0.345242,
     "end_time": "2024-06-02T10:30:18.335467",
     "exception": false,
     "start_time": "2024-06-02T10:30:17.990225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### New a file to save trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e24c7c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:19.070357Z",
     "iopub.status.busy": "2024-06-02T10:30:19.069353Z",
     "iopub.status.idle": "2024-06-02T10:30:19.074959Z",
     "shell.execute_reply": "2024-06-02T10:30:19.074096Z"
    },
    "papermill": {
     "duration": 0.397937,
     "end_time": "2024-06-02T10:30:19.076976",
     "exception": false,
     "start_time": "2024-06-02T10:30:18.679039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree('/kaggle/working/')\n",
    "except:\n",
    "    None\n",
    "os.mkdir('/kaggle/working/model_save/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d1601",
   "metadata": {
    "papermill": {
     "duration": 0.344056,
     "end_time": "2024-06-02T10:30:19.772235",
     "exception": false,
     "start_time": "2024-06-02T10:30:19.428179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Set the callback. Save the model every 100 epochs and save the weights only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "012adfe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:20.461213Z",
     "iopub.status.busy": "2024-06-02T10:30:20.460557Z",
     "iopub.status.idle": "2024-06-02T10:30:20.465532Z",
     "shell.execute_reply": "2024-06-02T10:30:20.464616Z"
    },
    "papermill": {
     "duration": 0.354108,
     "end_time": "2024-06-02T10:30:20.467464",
     "exception": false,
     "start_time": "2024-06-02T10:30:20.113356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/model_save/model_{epoch:02d}.h5',save_weights_only=True,period=100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722dac2",
   "metadata": {
    "papermill": {
     "duration": 0.344743,
     "end_time": "2024-06-02T10:30:21.197381",
     "exception": false,
     "start_time": "2024-06-02T10:30:20.852638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Set the optimizer. We use Exponential Decay for the learning rate. The initial learning rate is 0.001, and it decays every 100 epochs and the decay rate is 0.995. The optimizer is RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0177822b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:21.910218Z",
     "iopub.status.busy": "2024-06-02T10:30:21.909349Z",
     "iopub.status.idle": "2024-06-02T10:30:22.463006Z",
     "shell.execute_reply": "2024-06-02T10:30:22.462201Z"
    },
    "papermill": {
     "duration": 0.901919,
     "end_time": "2024-06-02T10:30:22.465318",
     "exception": false,
     "start_time": "2024-06-02T10:30:21.563399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_schedule0=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=100,decay_rate=0.995)\n",
    "optimizer0=tf.keras.optimizers.RMSprop(learning_rate=lr_schedule0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386aa95a",
   "metadata": {
    "papermill": {
     "duration": 0.352041,
     "end_time": "2024-06-02T10:30:23.162145",
     "exception": false,
     "start_time": "2024-06-02T10:30:22.810104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Then we train the model for 5000 epochs. We set the batch size as 64 and set the metrics as accuracy and F1 score. The loss function is binary cross entropy loss. We use GPU instead of CPU to speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20cae95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T10:30:23.895005Z",
     "iopub.status.busy": "2024-06-02T10:30:23.894153Z",
     "iopub.status.idle": "2024-06-02T12:36:55.240743Z",
     "shell.execute_reply": "2024-06-02T12:36:55.239764Z"
    },
    "papermill": {
     "duration": 7591.736034,
     "end_time": "2024-06-02T12:36:55.242884",
     "exception": false,
     "start_time": "2024-06-02T10:30:23.506850",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717324265.500259      75 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 48s 69ms/step - loss: 4.4442 - accuracy: 0.4698 - f1_score: 0.3620 - val_loss: 0.6616 - val_accuracy: 0.7417 - val_f1_score: 0.4923\n",
      "Epoch 2/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.6757 - accuracy: 0.7357 - f1_score: 0.5904 - val_loss: 0.5357 - val_accuracy: 0.7811 - val_f1_score: 0.6370\n",
      "Epoch 3/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.6126 - accuracy: 0.7712 - f1_score: 0.6633 - val_loss: 0.9005 - val_accuracy: 0.8012 - val_f1_score: 0.6845\n",
      "Epoch 4/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.5654 - accuracy: 0.7714 - f1_score: 0.6962 - val_loss: 0.5104 - val_accuracy: 0.7863 - val_f1_score: 0.7063\n",
      "Epoch 5/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.5317 - accuracy: 0.7900 - f1_score: 0.7139 - val_loss: 1.0307 - val_accuracy: 0.8126 - val_f1_score: 0.7222\n",
      "Epoch 6/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.5063 - accuracy: 0.8011 - f1_score: 0.7283 - val_loss: 0.6380 - val_accuracy: 0.8043 - val_f1_score: 0.7343\n",
      "Epoch 7/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.5133 - accuracy: 0.7909 - f1_score: 0.7377 - val_loss: 0.7212 - val_accuracy: 0.7789 - val_f1_score: 0.7411\n",
      "Epoch 8/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.5241 - accuracy: 0.8059 - f1_score: 0.7443 - val_loss: 0.8045 - val_accuracy: 0.7771 - val_f1_score: 0.7473\n",
      "Epoch 9/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.5188 - accuracy: 0.8003 - f1_score: 0.7493 - val_loss: 0.5410 - val_accuracy: 0.7898 - val_f1_score: 0.7511\n",
      "Epoch 10/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4696 - accuracy: 0.8159 - f1_score: 0.7536 - val_loss: 0.7283 - val_accuracy: 0.7947 - val_f1_score: 0.7562\n",
      "Epoch 11/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4681 - accuracy: 0.8202 - f1_score: 0.7587 - val_loss: 0.6375 - val_accuracy: 0.7999 - val_f1_score: 0.7609\n",
      "Epoch 12/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4466 - accuracy: 0.8196 - f1_score: 0.7629 - val_loss: 0.7238 - val_accuracy: 0.7360 - val_f1_score: 0.7635\n",
      "Epoch 13/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4652 - accuracy: 0.8224 - f1_score: 0.7644 - val_loss: 0.6206 - val_accuracy: 0.8060 - val_f1_score: 0.7663\n",
      "Epoch 14/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4450 - accuracy: 0.8221 - f1_score: 0.7678 - val_loss: 0.6457 - val_accuracy: 0.8052 - val_f1_score: 0.7694\n",
      "Epoch 15/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.4351 - accuracy: 0.8346 - f1_score: 0.7710 - val_loss: 0.7278 - val_accuracy: 0.8152 - val_f1_score: 0.7729\n",
      "Epoch 16/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4145 - accuracy: 0.8437 - f1_score: 0.7746 - val_loss: 1.0188 - val_accuracy: 0.7833 - val_f1_score: 0.7761\n",
      "Epoch 17/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3985 - accuracy: 0.8489 - f1_score: 0.7776 - val_loss: 0.6032 - val_accuracy: 0.7995 - val_f1_score: 0.7791\n",
      "Epoch 18/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.3972 - accuracy: 0.8498 - f1_score: 0.7804 - val_loss: 0.9394 - val_accuracy: 0.7977 - val_f1_score: 0.7819\n",
      "Epoch 19/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.4028 - accuracy: 0.8358 - f1_score: 0.7832 - val_loss: 0.5466 - val_accuracy: 0.7968 - val_f1_score: 0.7837\n",
      "Epoch 20/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.3574 - accuracy: 0.8673 - f1_score: 0.7850 - val_loss: 0.5356 - val_accuracy: 0.8104 - val_f1_score: 0.7866\n",
      "Epoch 21/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.3837 - accuracy: 0.8619 - f1_score: 0.7879 - val_loss: 0.8985 - val_accuracy: 0.7789 - val_f1_score: 0.7889\n",
      "Epoch 22/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3790 - accuracy: 0.8598 - f1_score: 0.7900 - val_loss: 0.6163 - val_accuracy: 0.8078 - val_f1_score: 0.7911\n",
      "Epoch 23/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3518 - accuracy: 0.8677 - f1_score: 0.7922 - val_loss: 1.1303 - val_accuracy: 0.7820 - val_f1_score: 0.7933\n",
      "Epoch 24/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3547 - accuracy: 0.8784 - f1_score: 0.7944 - val_loss: 1.1755 - val_accuracy: 0.8082 - val_f1_score: 0.7956\n",
      "Epoch 25/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3588 - accuracy: 0.8733 - f1_score: 0.7967 - val_loss: 0.9986 - val_accuracy: 0.7912 - val_f1_score: 0.7976\n",
      "Epoch 26/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3449 - accuracy: 0.8752 - f1_score: 0.7987 - val_loss: 0.8553 - val_accuracy: 0.7658 - val_f1_score: 0.7994\n",
      "Epoch 27/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3489 - accuracy: 0.8911 - f1_score: 0.8004 - val_loss: 1.3105 - val_accuracy: 0.7618 - val_f1_score: 0.8013\n",
      "Epoch 28/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.9009 - f1_score: 0.8023 - val_loss: 1.1038 - val_accuracy: 0.8065 - val_f1_score: 0.8036\n",
      "Epoch 29/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3309 - accuracy: 0.8964 - f1_score: 0.8047 - val_loss: 0.8366 - val_accuracy: 0.7824 - val_f1_score: 0.8056\n",
      "Epoch 30/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2845 - accuracy: 0.9047 - f1_score: 0.8065 - val_loss: 1.4288 - val_accuracy: 0.7877 - val_f1_score: 0.8076\n",
      "Epoch 31/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.3126 - accuracy: 0.9088 - f1_score: 0.8087 - val_loss: 1.2431 - val_accuracy: 0.7798 - val_f1_score: 0.8095\n",
      "Epoch 32/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2830 - accuracy: 0.9137 - f1_score: 0.8105 - val_loss: 1.1626 - val_accuracy: 0.7745 - val_f1_score: 0.8114\n",
      "Epoch 33/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2970 - accuracy: 0.9197 - f1_score: 0.8124 - val_loss: 1.1957 - val_accuracy: 0.7986 - val_f1_score: 0.8134\n",
      "Epoch 34/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2892 - accuracy: 0.9315 - f1_score: 0.8145 - val_loss: 1.8388 - val_accuracy: 0.7811 - val_f1_score: 0.8156\n",
      "Epoch 35/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.2826 - accuracy: 0.9294 - f1_score: 0.8166 - val_loss: 1.7212 - val_accuracy: 0.7955 - val_f1_score: 0.8176\n",
      "Epoch 36/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2626 - accuracy: 0.9315 - f1_score: 0.8186 - val_loss: 1.5273 - val_accuracy: 0.7859 - val_f1_score: 0.8196\n",
      "Epoch 37/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2669 - accuracy: 0.9386 - f1_score: 0.8205 - val_loss: 1.5842 - val_accuracy: 0.8017 - val_f1_score: 0.8216\n",
      "Epoch 38/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2600 - accuracy: 0.9418 - f1_score: 0.8226 - val_loss: 2.0408 - val_accuracy: 0.7588 - val_f1_score: 0.8234\n",
      "Epoch 39/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.2485 - accuracy: 0.9362 - f1_score: 0.8242 - val_loss: 1.3259 - val_accuracy: 0.7947 - val_f1_score: 0.8250\n",
      "Epoch 40/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2414 - accuracy: 0.9458 - f1_score: 0.8260 - val_loss: 1.8176 - val_accuracy: 0.7793 - val_f1_score: 0.8269\n",
      "Epoch 41/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2185 - accuracy: 0.9506 - f1_score: 0.8277 - val_loss: 1.4881 - val_accuracy: 0.7780 - val_f1_score: 0.8286\n",
      "Epoch 42/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.2084 - accuracy: 0.9521 - f1_score: 0.8295 - val_loss: 1.9265 - val_accuracy: 0.7557 - val_f1_score: 0.8302\n",
      "Epoch 43/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2027 - accuracy: 0.9433 - f1_score: 0.8308 - val_loss: 1.4256 - val_accuracy: 0.7771 - val_f1_score: 0.8316\n",
      "Epoch 44/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2029 - accuracy: 0.9521 - f1_score: 0.8324 - val_loss: 1.5317 - val_accuracy: 0.7728 - val_f1_score: 0.8331\n",
      "Epoch 45/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1998 - accuracy: 0.9598 - f1_score: 0.8339 - val_loss: 2.1526 - val_accuracy: 0.7877 - val_f1_score: 0.8347\n",
      "Epoch 46/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.2040 - accuracy: 0.9523 - f1_score: 0.8354 - val_loss: 1.4891 - val_accuracy: 0.7933 - val_f1_score: 0.8361\n",
      "Epoch 47/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1954 - accuracy: 0.9568 - f1_score: 0.8369 - val_loss: 2.1038 - val_accuracy: 0.7894 - val_f1_score: 0.8376\n",
      "Epoch 48/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.2171 - accuracy: 0.9604 - f1_score: 0.8384 - val_loss: 2.2197 - val_accuracy: 0.7833 - val_f1_score: 0.8391\n",
      "Epoch 49/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1918 - accuracy: 0.9604 - f1_score: 0.8397 - val_loss: 2.3987 - val_accuracy: 0.7579 - val_f1_score: 0.8404\n",
      "Epoch 50/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1638 - accuracy: 0.9625 - f1_score: 0.8410 - val_loss: 2.0133 - val_accuracy: 0.7789 - val_f1_score: 0.8416\n",
      "Epoch 51/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1936 - accuracy: 0.9666 - f1_score: 0.8423 - val_loss: 1.8369 - val_accuracy: 0.7741 - val_f1_score: 0.8429\n",
      "Epoch 52/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1616 - accuracy: 0.9632 - f1_score: 0.8436 - val_loss: 2.0418 - val_accuracy: 0.7885 - val_f1_score: 0.8442\n",
      "Epoch 53/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1766 - accuracy: 0.9615 - f1_score: 0.8448 - val_loss: 1.8396 - val_accuracy: 0.7960 - val_f1_score: 0.8454\n",
      "Epoch 54/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1419 - accuracy: 0.9688 - f1_score: 0.8461 - val_loss: 2.2104 - val_accuracy: 0.7872 - val_f1_score: 0.8467\n",
      "Epoch 55/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.1539 - accuracy: 0.9660 - f1_score: 0.8473 - val_loss: 1.8253 - val_accuracy: 0.7977 - val_f1_score: 0.8479\n",
      "Epoch 56/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1596 - accuracy: 0.9658 - f1_score: 0.8485 - val_loss: 2.2256 - val_accuracy: 0.7938 - val_f1_score: 0.8491\n",
      "Epoch 57/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1659 - accuracy: 0.9677 - f1_score: 0.8496 - val_loss: 2.1006 - val_accuracy: 0.7863 - val_f1_score: 0.8502\n",
      "Epoch 58/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1528 - accuracy: 0.9702 - f1_score: 0.8507 - val_loss: 2.3598 - val_accuracy: 0.7754 - val_f1_score: 0.8512\n",
      "Epoch 59/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1672 - accuracy: 0.9702 - f1_score: 0.8518 - val_loss: 2.1942 - val_accuracy: 0.7938 - val_f1_score: 0.8523\n",
      "Epoch 60/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1570 - accuracy: 0.9702 - f1_score: 0.8529 - val_loss: 2.1764 - val_accuracy: 0.7872 - val_f1_score: 0.8533\n",
      "Epoch 61/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1502 - accuracy: 0.9700 - f1_score: 0.8538 - val_loss: 2.2087 - val_accuracy: 0.7925 - val_f1_score: 0.8543\n",
      "Epoch 62/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1464 - accuracy: 0.9717 - f1_score: 0.8549 - val_loss: 1.9747 - val_accuracy: 0.7842 - val_f1_score: 0.8553\n",
      "Epoch 63/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.1933 - accuracy: 0.9668 - f1_score: 0.8558 - val_loss: 2.1877 - val_accuracy: 0.7526 - val_f1_score: 0.8561\n",
      "Epoch 64/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1274 - accuracy: 0.9709 - f1_score: 0.8565 - val_loss: 2.0566 - val_accuracy: 0.8082 - val_f1_score: 0.8570\n",
      "Epoch 65/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1458 - accuracy: 0.9688 - f1_score: 0.8575 - val_loss: 2.0888 - val_accuracy: 0.7947 - val_f1_score: 0.8579\n",
      "Epoch 66/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1171 - accuracy: 0.9724 - f1_score: 0.8584 - val_loss: 2.3419 - val_accuracy: 0.7947 - val_f1_score: 0.8588\n",
      "Epoch 67/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1395 - accuracy: 0.9700 - f1_score: 0.8593 - val_loss: 2.1217 - val_accuracy: 0.7977 - val_f1_score: 0.8597\n",
      "Epoch 68/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1091 - accuracy: 0.9717 - f1_score: 0.8601 - val_loss: 2.3002 - val_accuracy: 0.7999 - val_f1_score: 0.8606\n",
      "Epoch 69/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1184 - accuracy: 0.9722 - f1_score: 0.8610 - val_loss: 2.2357 - val_accuracy: 0.7916 - val_f1_score: 0.8614\n",
      "Epoch 70/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1365 - accuracy: 0.9728 - f1_score: 0.8618 - val_loss: 2.4718 - val_accuracy: 0.7903 - val_f1_score: 0.8622\n",
      "Epoch 71/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1423 - accuracy: 0.9745 - f1_score: 0.8626 - val_loss: 2.2511 - val_accuracy: 0.7933 - val_f1_score: 0.8630\n",
      "Epoch 72/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1226 - accuracy: 0.9752 - f1_score: 0.8633 - val_loss: 2.2499 - val_accuracy: 0.7863 - val_f1_score: 0.8637\n",
      "Epoch 73/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1241 - accuracy: 0.9733 - f1_score: 0.8641 - val_loss: 2.1786 - val_accuracy: 0.7820 - val_f1_score: 0.8644\n",
      "Epoch 74/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1078 - accuracy: 0.9758 - f1_score: 0.8648 - val_loss: 2.1619 - val_accuracy: 0.7842 - val_f1_score: 0.8651\n",
      "Epoch 75/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0921 - accuracy: 0.9769 - f1_score: 0.8655 - val_loss: 2.3462 - val_accuracy: 0.7828 - val_f1_score: 0.8658\n",
      "Epoch 76/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.1133 - accuracy: 0.9737 - f1_score: 0.8662 - val_loss: 2.1883 - val_accuracy: 0.7890 - val_f1_score: 0.8665\n",
      "Epoch 77/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1061 - accuracy: 0.9765 - f1_score: 0.8668 - val_loss: 2.4543 - val_accuracy: 0.7947 - val_f1_score: 0.8672\n",
      "Epoch 78/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1100 - accuracy: 0.9764 - f1_score: 0.8675 - val_loss: 2.4057 - val_accuracy: 0.7885 - val_f1_score: 0.8678\n",
      "Epoch 79/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1264 - accuracy: 0.9748 - f1_score: 0.8682 - val_loss: 2.2116 - val_accuracy: 0.7815 - val_f1_score: 0.8684\n",
      "Epoch 80/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1007 - accuracy: 0.9765 - f1_score: 0.8688 - val_loss: 2.4483 - val_accuracy: 0.7780 - val_f1_score: 0.8690\n",
      "Epoch 81/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0984 - accuracy: 0.9777 - f1_score: 0.8693 - val_loss: 2.2785 - val_accuracy: 0.7842 - val_f1_score: 0.8696\n",
      "Epoch 82/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.1203 - accuracy: 0.9767 - f1_score: 0.8699 - val_loss: 2.6427 - val_accuracy: 0.7732 - val_f1_score: 0.8702\n",
      "Epoch 83/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0920 - accuracy: 0.9765 - f1_score: 0.8705 - val_loss: 2.5754 - val_accuracy: 0.7842 - val_f1_score: 0.8708\n",
      "Epoch 84/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1027 - accuracy: 0.9775 - f1_score: 0.8710 - val_loss: 2.4971 - val_accuracy: 0.7907 - val_f1_score: 0.8713\n",
      "Epoch 85/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0832 - accuracy: 0.9790 - f1_score: 0.8716 - val_loss: 2.3948 - val_accuracy: 0.7999 - val_f1_score: 0.8719\n",
      "Epoch 86/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1059 - accuracy: 0.9777 - f1_score: 0.8722 - val_loss: 2.4461 - val_accuracy: 0.7877 - val_f1_score: 0.8725\n",
      "Epoch 87/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1019 - accuracy: 0.9775 - f1_score: 0.8728 - val_loss: 2.6264 - val_accuracy: 0.7754 - val_f1_score: 0.8730\n",
      "Epoch 88/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1027 - accuracy: 0.9777 - f1_score: 0.8733 - val_loss: 2.4251 - val_accuracy: 0.7977 - val_f1_score: 0.8735\n",
      "Epoch 89/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0933 - accuracy: 0.9782 - f1_score: 0.8738 - val_loss: 2.4984 - val_accuracy: 0.7916 - val_f1_score: 0.8741\n",
      "Epoch 90/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.1303 - accuracy: 0.9771 - f1_score: 0.8743 - val_loss: 2.4600 - val_accuracy: 0.8025 - val_f1_score: 0.8746\n",
      "Epoch 91/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0968 - accuracy: 0.9782 - f1_score: 0.8749 - val_loss: 2.4454 - val_accuracy: 0.7763 - val_f1_score: 0.8751\n",
      "Epoch 92/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1026 - accuracy: 0.9784 - f1_score: 0.8753 - val_loss: 2.3976 - val_accuracy: 0.7999 - val_f1_score: 0.8756\n",
      "Epoch 93/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0889 - accuracy: 0.9773 - f1_score: 0.8758 - val_loss: 2.4018 - val_accuracy: 0.7855 - val_f1_score: 0.8761\n",
      "Epoch 94/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0882 - accuracy: 0.9779 - f1_score: 0.8763 - val_loss: 2.4320 - val_accuracy: 0.7758 - val_f1_score: 0.8765\n",
      "Epoch 95/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1001 - accuracy: 0.9794 - f1_score: 0.8767 - val_loss: 2.3641 - val_accuracy: 0.7938 - val_f1_score: 0.8770\n",
      "Epoch 96/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0989 - accuracy: 0.9775 - f1_score: 0.8772 - val_loss: 2.3980 - val_accuracy: 0.7977 - val_f1_score: 0.8774\n",
      "Epoch 97/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0992 - accuracy: 0.9801 - f1_score: 0.8777 - val_loss: 2.5538 - val_accuracy: 0.7763 - val_f1_score: 0.8779\n",
      "Epoch 98/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1035 - accuracy: 0.9784 - f1_score: 0.8781 - val_loss: 2.3705 - val_accuracy: 0.8039 - val_f1_score: 0.8783\n",
      "Epoch 99/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0916 - accuracy: 0.9790 - f1_score: 0.8785 - val_loss: 2.3432 - val_accuracy: 0.7894 - val_f1_score: 0.8787\n",
      "Epoch 100/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0830 - accuracy: 0.9805 - f1_score: 0.8790 - val_loss: 2.4083 - val_accuracy: 0.7968 - val_f1_score: 0.8792\n",
      "Epoch 101/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0921 - accuracy: 0.9792 - f1_score: 0.8794 - val_loss: 2.3336 - val_accuracy: 0.7990 - val_f1_score: 0.8796\n",
      "Epoch 102/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0860 - accuracy: 0.9782 - f1_score: 0.8798 - val_loss: 2.5812 - val_accuracy: 0.7719 - val_f1_score: 0.8800\n",
      "Epoch 103/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0715 - accuracy: 0.9801 - f1_score: 0.8802 - val_loss: 2.4418 - val_accuracy: 0.7907 - val_f1_score: 0.8804\n",
      "Epoch 104/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0838 - accuracy: 0.9792 - f1_score: 0.8806 - val_loss: 2.3470 - val_accuracy: 0.7951 - val_f1_score: 0.8808\n",
      "Epoch 105/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0774 - accuracy: 0.9809 - f1_score: 0.8810 - val_loss: 2.4710 - val_accuracy: 0.7855 - val_f1_score: 0.8812\n",
      "Epoch 106/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0893 - accuracy: 0.9790 - f1_score: 0.8814 - val_loss: 2.4601 - val_accuracy: 0.8025 - val_f1_score: 0.8816\n",
      "Epoch 107/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0816 - accuracy: 0.9812 - f1_score: 0.8818 - val_loss: 2.3380 - val_accuracy: 0.7977 - val_f1_score: 0.8820\n",
      "Epoch 108/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0819 - accuracy: 0.9799 - f1_score: 0.8822 - val_loss: 2.2738 - val_accuracy: 0.7929 - val_f1_score: 0.8824\n",
      "Epoch 109/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0827 - accuracy: 0.9809 - f1_score: 0.8825 - val_loss: 2.4022 - val_accuracy: 0.8004 - val_f1_score: 0.8827\n",
      "Epoch 110/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0836 - accuracy: 0.9803 - f1_score: 0.8829 - val_loss: 2.3758 - val_accuracy: 0.7868 - val_f1_score: 0.8831\n",
      "Epoch 111/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0824 - accuracy: 0.9818 - f1_score: 0.8833 - val_loss: 2.6726 - val_accuracy: 0.7863 - val_f1_score: 0.8834\n",
      "Epoch 112/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0907 - accuracy: 0.9790 - f1_score: 0.8836 - val_loss: 2.4150 - val_accuracy: 0.7982 - val_f1_score: 0.8838\n",
      "Epoch 113/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0752 - accuracy: 0.9805 - f1_score: 0.8840 - val_loss: 2.5249 - val_accuracy: 0.7903 - val_f1_score: 0.8841\n",
      "Epoch 114/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0855 - accuracy: 0.9795 - f1_score: 0.8843 - val_loss: 2.1005 - val_accuracy: 0.7947 - val_f1_score: 0.8845\n",
      "Epoch 115/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0655 - accuracy: 0.9818 - f1_score: 0.8846 - val_loss: 2.5744 - val_accuracy: 0.7824 - val_f1_score: 0.8848\n",
      "Epoch 116/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0940 - accuracy: 0.9795 - f1_score: 0.8849 - val_loss: 2.4260 - val_accuracy: 0.8008 - val_f1_score: 0.8851\n",
      "Epoch 117/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0809 - accuracy: 0.9788 - f1_score: 0.8853 - val_loss: 2.5125 - val_accuracy: 0.7999 - val_f1_score: 0.8854\n",
      "Epoch 118/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0817 - accuracy: 0.9788 - f1_score: 0.8856 - val_loss: 2.4412 - val_accuracy: 0.7973 - val_f1_score: 0.8858\n",
      "Epoch 119/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0752 - accuracy: 0.9803 - f1_score: 0.8859 - val_loss: 2.6397 - val_accuracy: 0.7811 - val_f1_score: 0.8861\n",
      "Epoch 120/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0729 - accuracy: 0.9803 - f1_score: 0.8862 - val_loss: 2.5554 - val_accuracy: 0.7999 - val_f1_score: 0.8864\n",
      "Epoch 121/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0771 - accuracy: 0.9801 - f1_score: 0.8865 - val_loss: 2.4496 - val_accuracy: 0.8069 - val_f1_score: 0.8867\n",
      "Epoch 122/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0760 - accuracy: 0.9799 - f1_score: 0.8869 - val_loss: 2.4437 - val_accuracy: 0.8021 - val_f1_score: 0.8870\n",
      "Epoch 123/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0822 - accuracy: 0.9788 - f1_score: 0.8872 - val_loss: 2.4870 - val_accuracy: 0.7925 - val_f1_score: 0.8873\n",
      "Epoch 124/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0867 - accuracy: 0.9786 - f1_score: 0.8874 - val_loss: 2.5192 - val_accuracy: 0.7942 - val_f1_score: 0.8876\n",
      "Epoch 125/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0744 - accuracy: 0.9807 - f1_score: 0.8877 - val_loss: 2.4810 - val_accuracy: 0.7977 - val_f1_score: 0.8879\n",
      "Epoch 126/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0818 - accuracy: 0.9788 - f1_score: 0.8880 - val_loss: 2.5108 - val_accuracy: 0.7828 - val_f1_score: 0.8881\n",
      "Epoch 127/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0745 - accuracy: 0.9807 - f1_score: 0.8882 - val_loss: 2.4441 - val_accuracy: 0.7942 - val_f1_score: 0.8884\n",
      "Epoch 128/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0792 - accuracy: 0.9803 - f1_score: 0.8885 - val_loss: 2.5090 - val_accuracy: 0.7868 - val_f1_score: 0.8886\n",
      "Epoch 129/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0688 - accuracy: 0.9809 - f1_score: 0.8888 - val_loss: 2.4457 - val_accuracy: 0.7951 - val_f1_score: 0.8889\n",
      "Epoch 130/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0788 - accuracy: 0.9820 - f1_score: 0.8890 - val_loss: 2.6353 - val_accuracy: 0.7898 - val_f1_score: 0.8892\n",
      "Epoch 131/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0782 - accuracy: 0.9810 - f1_score: 0.8893 - val_loss: 2.5593 - val_accuracy: 0.7907 - val_f1_score: 0.8894\n",
      "Epoch 132/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0811 - accuracy: 0.9803 - f1_score: 0.8895 - val_loss: 2.4640 - val_accuracy: 0.7960 - val_f1_score: 0.8897\n",
      "Epoch 133/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0808 - accuracy: 0.9807 - f1_score: 0.8898 - val_loss: 2.8232 - val_accuracy: 0.7715 - val_f1_score: 0.8899\n",
      "Epoch 134/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0770 - accuracy: 0.9788 - f1_score: 0.8900 - val_loss: 2.5568 - val_accuracy: 0.7802 - val_f1_score: 0.8901\n",
      "Epoch 135/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0680 - accuracy: 0.9809 - f1_score: 0.8902 - val_loss: 2.5292 - val_accuracy: 0.7846 - val_f1_score: 0.8903\n",
      "Epoch 136/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0792 - accuracy: 0.9814 - f1_score: 0.8904 - val_loss: 2.5110 - val_accuracy: 0.7872 - val_f1_score: 0.8905\n",
      "Epoch 137/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0692 - accuracy: 0.9824 - f1_score: 0.8907 - val_loss: 2.5829 - val_accuracy: 0.7920 - val_f1_score: 0.8908\n",
      "Epoch 138/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0665 - accuracy: 0.9795 - f1_score: 0.8909 - val_loss: 2.4653 - val_accuracy: 0.7986 - val_f1_score: 0.8910\n",
      "Epoch 139/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0652 - accuracy: 0.9825 - f1_score: 0.8911 - val_loss: 2.5926 - val_accuracy: 0.7837 - val_f1_score: 0.8912\n",
      "Epoch 140/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0645 - accuracy: 0.9818 - f1_score: 0.8914 - val_loss: 2.4180 - val_accuracy: 0.7960 - val_f1_score: 0.8915\n",
      "Epoch 141/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0764 - accuracy: 0.9801 - f1_score: 0.8916 - val_loss: 2.4143 - val_accuracy: 0.7920 - val_f1_score: 0.8917\n",
      "Epoch 142/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0684 - accuracy: 0.9810 - f1_score: 0.8918 - val_loss: 2.4023 - val_accuracy: 0.8012 - val_f1_score: 0.8919\n",
      "Epoch 143/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0568 - accuracy: 0.9810 - f1_score: 0.8920 - val_loss: 2.5494 - val_accuracy: 0.8004 - val_f1_score: 0.8922\n",
      "Epoch 144/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0759 - accuracy: 0.9812 - f1_score: 0.8923 - val_loss: 2.4424 - val_accuracy: 0.7990 - val_f1_score: 0.8924\n",
      "Epoch 145/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0670 - accuracy: 0.9816 - f1_score: 0.8925 - val_loss: 2.6063 - val_accuracy: 0.7990 - val_f1_score: 0.8926\n",
      "Epoch 146/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0664 - accuracy: 0.9824 - f1_score: 0.8927 - val_loss: 2.5025 - val_accuracy: 0.7995 - val_f1_score: 0.8928\n",
      "Epoch 147/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0681 - accuracy: 0.9814 - f1_score: 0.8929 - val_loss: 2.4030 - val_accuracy: 0.7982 - val_f1_score: 0.8931\n",
      "Epoch 148/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0721 - accuracy: 0.9820 - f1_score: 0.8932 - val_loss: 2.5175 - val_accuracy: 0.7964 - val_f1_score: 0.8933\n",
      "Epoch 149/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0649 - accuracy: 0.9809 - f1_score: 0.8934 - val_loss: 2.7142 - val_accuracy: 0.7898 - val_f1_score: 0.8935\n",
      "Epoch 150/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0832 - accuracy: 0.9794 - f1_score: 0.8936 - val_loss: 2.4347 - val_accuracy: 0.8012 - val_f1_score: 0.8937\n",
      "Epoch 151/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0722 - accuracy: 0.9809 - f1_score: 0.8938 - val_loss: 2.3326 - val_accuracy: 0.8060 - val_f1_score: 0.8939\n",
      "Epoch 152/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0729 - accuracy: 0.9795 - f1_score: 0.8940 - val_loss: 2.3637 - val_accuracy: 0.7947 - val_f1_score: 0.8941\n",
      "Epoch 153/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0669 - accuracy: 0.9818 - f1_score: 0.8942 - val_loss: 2.5244 - val_accuracy: 0.8065 - val_f1_score: 0.8943\n",
      "Epoch 154/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0709 - accuracy: 0.9816 - f1_score: 0.8944 - val_loss: 2.4812 - val_accuracy: 0.8021 - val_f1_score: 0.8945\n",
      "Epoch 155/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0685 - accuracy: 0.9805 - f1_score: 0.8946 - val_loss: 2.5009 - val_accuracy: 0.7999 - val_f1_score: 0.8947\n",
      "Epoch 156/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0647 - accuracy: 0.9824 - f1_score: 0.8948 - val_loss: 2.6760 - val_accuracy: 0.7907 - val_f1_score: 0.8949\n",
      "Epoch 157/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0746 - accuracy: 0.9805 - f1_score: 0.8950 - val_loss: 2.5605 - val_accuracy: 0.8004 - val_f1_score: 0.8951\n",
      "Epoch 158/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0655 - accuracy: 0.9814 - f1_score: 0.8952 - val_loss: 2.7820 - val_accuracy: 0.7907 - val_f1_score: 0.8953\n",
      "Epoch 159/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0694 - accuracy: 0.9816 - f1_score: 0.8953 - val_loss: 2.5311 - val_accuracy: 0.7942 - val_f1_score: 0.8954\n",
      "Epoch 160/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0751 - accuracy: 0.9816 - f1_score: 0.8955 - val_loss: 2.6174 - val_accuracy: 0.7920 - val_f1_score: 0.8956\n",
      "Epoch 161/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0681 - accuracy: 0.9829 - f1_score: 0.8957 - val_loss: 2.6540 - val_accuracy: 0.7903 - val_f1_score: 0.8958\n",
      "Epoch 162/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0702 - accuracy: 0.9795 - f1_score: 0.8959 - val_loss: 2.4628 - val_accuracy: 0.8065 - val_f1_score: 0.8959\n",
      "Epoch 163/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0649 - accuracy: 0.9818 - f1_score: 0.8960 - val_loss: 2.7788 - val_accuracy: 0.7802 - val_f1_score: 0.8961\n",
      "Epoch 164/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0760 - accuracy: 0.9795 - f1_score: 0.8962 - val_loss: 2.4698 - val_accuracy: 0.8056 - val_f1_score: 0.8963\n",
      "Epoch 165/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0666 - accuracy: 0.9825 - f1_score: 0.8964 - val_loss: 2.5658 - val_accuracy: 0.7977 - val_f1_score: 0.8965\n",
      "Epoch 166/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0764 - accuracy: 0.9805 - f1_score: 0.8965 - val_loss: 2.5361 - val_accuracy: 0.7999 - val_f1_score: 0.8966\n",
      "Epoch 167/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0654 - accuracy: 0.9822 - f1_score: 0.8967 - val_loss: 2.5573 - val_accuracy: 0.7898 - val_f1_score: 0.8968\n",
      "Epoch 168/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0648 - accuracy: 0.9816 - f1_score: 0.8969 - val_loss: 2.5701 - val_accuracy: 0.7982 - val_f1_score: 0.8970\n",
      "Epoch 169/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0822 - accuracy: 0.9799 - f1_score: 0.8970 - val_loss: 2.5295 - val_accuracy: 0.7964 - val_f1_score: 0.8971\n",
      "Epoch 170/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0644 - accuracy: 0.9812 - f1_score: 0.8972 - val_loss: 2.7063 - val_accuracy: 0.7933 - val_f1_score: 0.8973\n",
      "Epoch 171/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0693 - accuracy: 0.9810 - f1_score: 0.8974 - val_loss: 2.4854 - val_accuracy: 0.8039 - val_f1_score: 0.8974\n",
      "Epoch 172/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0659 - accuracy: 0.9816 - f1_score: 0.8975 - val_loss: 2.6754 - val_accuracy: 0.7933 - val_f1_score: 0.8976\n",
      "Epoch 173/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0652 - accuracy: 0.9822 - f1_score: 0.8977 - val_loss: 2.6358 - val_accuracy: 0.7973 - val_f1_score: 0.8977\n",
      "Epoch 174/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0608 - accuracy: 0.9824 - f1_score: 0.8978 - val_loss: 2.7270 - val_accuracy: 0.7968 - val_f1_score: 0.8979\n",
      "Epoch 175/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0689 - accuracy: 0.9812 - f1_score: 0.8980 - val_loss: 2.6882 - val_accuracy: 0.7951 - val_f1_score: 0.8981\n",
      "Epoch 176/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0707 - accuracy: 0.9807 - f1_score: 0.8981 - val_loss: 2.5877 - val_accuracy: 0.8047 - val_f1_score: 0.8982\n",
      "Epoch 177/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0682 - accuracy: 0.9829 - f1_score: 0.8983 - val_loss: 2.7174 - val_accuracy: 0.7977 - val_f1_score: 0.8984\n",
      "Epoch 178/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0765 - accuracy: 0.9801 - f1_score: 0.8984 - val_loss: 2.5551 - val_accuracy: 0.8021 - val_f1_score: 0.8985\n",
      "Epoch 179/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0751 - accuracy: 0.9814 - f1_score: 0.8986 - val_loss: 2.5997 - val_accuracy: 0.7955 - val_f1_score: 0.8987\n",
      "Epoch 180/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0619 - accuracy: 0.9827 - f1_score: 0.8987 - val_loss: 2.5507 - val_accuracy: 0.7973 - val_f1_score: 0.8988\n",
      "Epoch 181/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0603 - accuracy: 0.9829 - f1_score: 0.8989 - val_loss: 2.4465 - val_accuracy: 0.8043 - val_f1_score: 0.8990\n",
      "Epoch 182/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0672 - accuracy: 0.9816 - f1_score: 0.8990 - val_loss: 2.6239 - val_accuracy: 0.7986 - val_f1_score: 0.8991\n",
      "Epoch 183/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0653 - accuracy: 0.9829 - f1_score: 0.8992 - val_loss: 2.7129 - val_accuracy: 0.7828 - val_f1_score: 0.8992\n",
      "Epoch 184/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9820 - f1_score: 0.8993 - val_loss: 2.5960 - val_accuracy: 0.7925 - val_f1_score: 0.8994\n",
      "Epoch 185/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0657 - accuracy: 0.9820 - f1_score: 0.8994 - val_loss: 2.4831 - val_accuracy: 0.8012 - val_f1_score: 0.8995\n",
      "Epoch 186/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0651 - accuracy: 0.9833 - f1_score: 0.8996 - val_loss: 2.5149 - val_accuracy: 0.8056 - val_f1_score: 0.8997\n",
      "Epoch 187/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9822 - f1_score: 0.8997 - val_loss: 2.5825 - val_accuracy: 0.7955 - val_f1_score: 0.8998\n",
      "Epoch 188/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9825 - f1_score: 0.8999 - val_loss: 2.4626 - val_accuracy: 0.7968 - val_f1_score: 0.8999\n",
      "Epoch 189/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0629 - accuracy: 0.9835 - f1_score: 0.9000 - val_loss: 2.6161 - val_accuracy: 0.8004 - val_f1_score: 0.9001\n",
      "Epoch 190/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0649 - accuracy: 0.9822 - f1_score: 0.9001 - val_loss: 2.6026 - val_accuracy: 0.7982 - val_f1_score: 0.9002\n",
      "Epoch 191/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0619 - accuracy: 0.9812 - f1_score: 0.9003 - val_loss: 2.5125 - val_accuracy: 0.8052 - val_f1_score: 0.9003\n",
      "Epoch 192/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0609 - accuracy: 0.9824 - f1_score: 0.9004 - val_loss: 2.7379 - val_accuracy: 0.7929 - val_f1_score: 0.9005\n",
      "Epoch 193/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0729 - accuracy: 0.9814 - f1_score: 0.9005 - val_loss: 2.6759 - val_accuracy: 0.7929 - val_f1_score: 0.9006\n",
      "Epoch 194/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0644 - accuracy: 0.9816 - f1_score: 0.9007 - val_loss: 2.7736 - val_accuracy: 0.7920 - val_f1_score: 0.9007\n",
      "Epoch 195/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0703 - accuracy: 0.9810 - f1_score: 0.9008 - val_loss: 2.5464 - val_accuracy: 0.7990 - val_f1_score: 0.9008\n",
      "Epoch 196/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0674 - accuracy: 0.9822 - f1_score: 0.9009 - val_loss: 2.8346 - val_accuracy: 0.7824 - val_f1_score: 0.9009\n",
      "Epoch 197/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9824 - f1_score: 0.9010 - val_loss: 2.5877 - val_accuracy: 0.7977 - val_f1_score: 0.9011\n",
      "Epoch 198/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0616 - accuracy: 0.9831 - f1_score: 0.9011 - val_loss: 2.6045 - val_accuracy: 0.8008 - val_f1_score: 0.9012\n",
      "Epoch 199/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9822 - f1_score: 0.9012 - val_loss: 2.5180 - val_accuracy: 0.7982 - val_f1_score: 0.9013\n",
      "Epoch 200/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9824 - f1_score: 0.9014 - val_loss: 2.6599 - val_accuracy: 0.7877 - val_f1_score: 0.9014\n",
      "Epoch 201/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9824 - f1_score: 0.9015 - val_loss: 2.5414 - val_accuracy: 0.7960 - val_f1_score: 0.9015\n",
      "Epoch 202/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9827 - f1_score: 0.9016 - val_loss: 2.7454 - val_accuracy: 0.7890 - val_f1_score: 0.9016\n",
      "Epoch 203/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0544 - accuracy: 0.9822 - f1_score: 0.9017 - val_loss: 2.7081 - val_accuracy: 0.7907 - val_f1_score: 0.9017\n",
      "Epoch 204/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9833 - f1_score: 0.9018 - val_loss: 2.6478 - val_accuracy: 0.7898 - val_f1_score: 0.9019\n",
      "Epoch 205/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0640 - accuracy: 0.9814 - f1_score: 0.9019 - val_loss: 2.6826 - val_accuracy: 0.7938 - val_f1_score: 0.9020\n",
      "Epoch 206/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0571 - accuracy: 0.9831 - f1_score: 0.9020 - val_loss: 2.7389 - val_accuracy: 0.8004 - val_f1_score: 0.9021\n",
      "Epoch 207/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0569 - accuracy: 0.9837 - f1_score: 0.9021 - val_loss: 2.6487 - val_accuracy: 0.8017 - val_f1_score: 0.9022\n",
      "Epoch 208/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0692 - accuracy: 0.9820 - f1_score: 0.9023 - val_loss: 2.6093 - val_accuracy: 0.7990 - val_f1_score: 0.9023\n",
      "Epoch 209/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0633 - accuracy: 0.9814 - f1_score: 0.9024 - val_loss: 2.6222 - val_accuracy: 0.7968 - val_f1_score: 0.9024\n",
      "Epoch 210/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0637 - accuracy: 0.9812 - f1_score: 0.9025 - val_loss: 2.6249 - val_accuracy: 0.7986 - val_f1_score: 0.9025\n",
      "Epoch 211/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0571 - accuracy: 0.9833 - f1_score: 0.9026 - val_loss: 2.7443 - val_accuracy: 0.7916 - val_f1_score: 0.9026\n",
      "Epoch 212/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0635 - accuracy: 0.9818 - f1_score: 0.9027 - val_loss: 2.6409 - val_accuracy: 0.7955 - val_f1_score: 0.9027\n",
      "Epoch 213/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0621 - accuracy: 0.9820 - f1_score: 0.9028 - val_loss: 2.5577 - val_accuracy: 0.8004 - val_f1_score: 0.9028\n",
      "Epoch 214/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0657 - accuracy: 0.9831 - f1_score: 0.9029 - val_loss: 2.6140 - val_accuracy: 0.7968 - val_f1_score: 0.9029\n",
      "Epoch 215/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0662 - accuracy: 0.9816 - f1_score: 0.9030 - val_loss: 2.5827 - val_accuracy: 0.8030 - val_f1_score: 0.9030\n",
      "Epoch 216/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0605 - accuracy: 0.9844 - f1_score: 0.9031 - val_loss: 2.6194 - val_accuracy: 0.7964 - val_f1_score: 0.9032\n",
      "Epoch 217/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0626 - accuracy: 0.9822 - f1_score: 0.9032 - val_loss: 2.5827 - val_accuracy: 0.8043 - val_f1_score: 0.9033\n",
      "Epoch 218/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0652 - accuracy: 0.9825 - f1_score: 0.9033 - val_loss: 2.6401 - val_accuracy: 0.8039 - val_f1_score: 0.9034\n",
      "Epoch 219/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0680 - accuracy: 0.9827 - f1_score: 0.9034 - val_loss: 2.6093 - val_accuracy: 0.8087 - val_f1_score: 0.9035\n",
      "Epoch 220/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0696 - accuracy: 0.9825 - f1_score: 0.9035 - val_loss: 2.5840 - val_accuracy: 0.8065 - val_f1_score: 0.9036\n",
      "Epoch 221/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0604 - accuracy: 0.9829 - f1_score: 0.9037 - val_loss: 2.5386 - val_accuracy: 0.8039 - val_f1_score: 0.9037\n",
      "Epoch 222/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0653 - accuracy: 0.9829 - f1_score: 0.9038 - val_loss: 2.5718 - val_accuracy: 0.8025 - val_f1_score: 0.9038\n",
      "Epoch 223/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0675 - accuracy: 0.9814 - f1_score: 0.9039 - val_loss: 2.5235 - val_accuracy: 0.8017 - val_f1_score: 0.9039\n",
      "Epoch 224/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0633 - accuracy: 0.9829 - f1_score: 0.9040 - val_loss: 2.5928 - val_accuracy: 0.7986 - val_f1_score: 0.9040\n",
      "Epoch 225/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0609 - accuracy: 0.9824 - f1_score: 0.9041 - val_loss: 2.6347 - val_accuracy: 0.7999 - val_f1_score: 0.9041\n",
      "Epoch 226/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0636 - accuracy: 0.9835 - f1_score: 0.9042 - val_loss: 2.6804 - val_accuracy: 0.7947 - val_f1_score: 0.9042\n",
      "Epoch 227/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0655 - accuracy: 0.9827 - f1_score: 0.9043 - val_loss: 2.7092 - val_accuracy: 0.7898 - val_f1_score: 0.9043\n",
      "Epoch 228/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0637 - accuracy: 0.9835 - f1_score: 0.9043 - val_loss: 2.5638 - val_accuracy: 0.7990 - val_f1_score: 0.9044\n",
      "Epoch 229/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0625 - accuracy: 0.9827 - f1_score: 0.9044 - val_loss: 2.5842 - val_accuracy: 0.7973 - val_f1_score: 0.9045\n",
      "Epoch 230/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0650 - accuracy: 0.9829 - f1_score: 0.9045 - val_loss: 2.6105 - val_accuracy: 0.7960 - val_f1_score: 0.9046\n",
      "Epoch 231/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0638 - accuracy: 0.9816 - f1_score: 0.9046 - val_loss: 2.7103 - val_accuracy: 0.7903 - val_f1_score: 0.9047\n",
      "Epoch 232/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0677 - accuracy: 0.9827 - f1_score: 0.9047 - val_loss: 2.6822 - val_accuracy: 0.7977 - val_f1_score: 0.9047\n",
      "Epoch 233/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0642 - accuracy: 0.9831 - f1_score: 0.9048 - val_loss: 2.7392 - val_accuracy: 0.7933 - val_f1_score: 0.9048\n",
      "Epoch 234/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0576 - accuracy: 0.9822 - f1_score: 0.9049 - val_loss: 2.6792 - val_accuracy: 0.7999 - val_f1_score: 0.9049\n",
      "Epoch 235/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0697 - accuracy: 0.9833 - f1_score: 0.9050 - val_loss: 2.8024 - val_accuracy: 0.7850 - val_f1_score: 0.9050\n",
      "Epoch 236/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0657 - accuracy: 0.9833 - f1_score: 0.9050 - val_loss: 2.6556 - val_accuracy: 0.8021 - val_f1_score: 0.9051\n",
      "Epoch 237/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0671 - accuracy: 0.9822 - f1_score: 0.9051 - val_loss: 2.6077 - val_accuracy: 0.7990 - val_f1_score: 0.9052\n",
      "Epoch 238/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0646 - accuracy: 0.9824 - f1_score: 0.9052 - val_loss: 2.5698 - val_accuracy: 0.8008 - val_f1_score: 0.9053\n",
      "Epoch 239/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0603 - accuracy: 0.9833 - f1_score: 0.9053 - val_loss: 2.6487 - val_accuracy: 0.7947 - val_f1_score: 0.9053\n",
      "Epoch 240/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0638 - accuracy: 0.9829 - f1_score: 0.9054 - val_loss: 2.5617 - val_accuracy: 0.7999 - val_f1_score: 0.9054\n",
      "Epoch 241/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0629 - accuracy: 0.9822 - f1_score: 0.9055 - val_loss: 2.6613 - val_accuracy: 0.7907 - val_f1_score: 0.9055\n",
      "Epoch 242/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0647 - accuracy: 0.9835 - f1_score: 0.9055 - val_loss: 2.6743 - val_accuracy: 0.7907 - val_f1_score: 0.9056\n",
      "Epoch 243/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9831 - f1_score: 0.9056 - val_loss: 2.7514 - val_accuracy: 0.7955 - val_f1_score: 0.9057\n",
      "Epoch 244/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0645 - accuracy: 0.9829 - f1_score: 0.9057 - val_loss: 2.6771 - val_accuracy: 0.7968 - val_f1_score: 0.9057\n",
      "Epoch 245/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0628 - accuracy: 0.9835 - f1_score: 0.9058 - val_loss: 2.7016 - val_accuracy: 0.7872 - val_f1_score: 0.9058\n",
      "Epoch 246/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0631 - accuracy: 0.9827 - f1_score: 0.9059 - val_loss: 2.7379 - val_accuracy: 0.7920 - val_f1_score: 0.9059\n",
      "Epoch 247/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9059 - val_loss: 2.6414 - val_accuracy: 0.8047 - val_f1_score: 0.9060\n",
      "Epoch 248/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0608 - accuracy: 0.9839 - f1_score: 0.9060 - val_loss: 2.6371 - val_accuracy: 0.7898 - val_f1_score: 0.9061\n",
      "Epoch 249/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0657 - accuracy: 0.9812 - f1_score: 0.9061 - val_loss: 2.7461 - val_accuracy: 0.7868 - val_f1_score: 0.9061\n",
      "Epoch 250/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0675 - accuracy: 0.9840 - f1_score: 0.9062 - val_loss: 2.7451 - val_accuracy: 0.7942 - val_f1_score: 0.9062\n",
      "Epoch 251/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0645 - accuracy: 0.9822 - f1_score: 0.9062 - val_loss: 2.6618 - val_accuracy: 0.7925 - val_f1_score: 0.9063\n",
      "Epoch 252/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0638 - accuracy: 0.9844 - f1_score: 0.9063 - val_loss: 2.5232 - val_accuracy: 0.7955 - val_f1_score: 0.9063\n",
      "Epoch 253/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0605 - accuracy: 0.9844 - f1_score: 0.9064 - val_loss: 2.5963 - val_accuracy: 0.7942 - val_f1_score: 0.9064\n",
      "Epoch 254/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0601 - accuracy: 0.9824 - f1_score: 0.9064 - val_loss: 2.6110 - val_accuracy: 0.8034 - val_f1_score: 0.9065\n",
      "Epoch 255/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9065 - val_loss: 2.5167 - val_accuracy: 0.8074 - val_f1_score: 0.9066\n",
      "Epoch 256/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0567 - accuracy: 0.9846 - f1_score: 0.9066 - val_loss: 2.6178 - val_accuracy: 0.8004 - val_f1_score: 0.9067\n",
      "Epoch 257/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0548 - accuracy: 0.9833 - f1_score: 0.9067 - val_loss: 2.6742 - val_accuracy: 0.7964 - val_f1_score: 0.9067\n",
      "Epoch 258/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9824 - f1_score: 0.9068 - val_loss: 2.6165 - val_accuracy: 0.7990 - val_f1_score: 0.9068\n",
      "Epoch 259/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9833 - f1_score: 0.9068 - val_loss: 2.6226 - val_accuracy: 0.7982 - val_f1_score: 0.9069\n",
      "Epoch 260/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0575 - accuracy: 0.9833 - f1_score: 0.9069 - val_loss: 2.6816 - val_accuracy: 0.7925 - val_f1_score: 0.9070\n",
      "Epoch 261/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9835 - f1_score: 0.9070 - val_loss: 2.7146 - val_accuracy: 0.7912 - val_f1_score: 0.9070\n",
      "Epoch 262/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0606 - accuracy: 0.9824 - f1_score: 0.9071 - val_loss: 2.5957 - val_accuracy: 0.8030 - val_f1_score: 0.9071\n",
      "Epoch 263/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9829 - f1_score: 0.9071 - val_loss: 2.8163 - val_accuracy: 0.7907 - val_f1_score: 0.9072\n",
      "Epoch 264/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0597 - accuracy: 0.9842 - f1_score: 0.9072 - val_loss: 2.7297 - val_accuracy: 0.7947 - val_f1_score: 0.9072\n",
      "Epoch 265/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9824 - f1_score: 0.9073 - val_loss: 2.6554 - val_accuracy: 0.7951 - val_f1_score: 0.9073\n",
      "Epoch 266/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0599 - accuracy: 0.9827 - f1_score: 0.9073 - val_loss: 2.6750 - val_accuracy: 0.7912 - val_f1_score: 0.9074\n",
      "Epoch 267/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0658 - accuracy: 0.9833 - f1_score: 0.9074 - val_loss: 2.7629 - val_accuracy: 0.7868 - val_f1_score: 0.9074\n",
      "Epoch 268/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0732 - accuracy: 0.9822 - f1_score: 0.9074 - val_loss: 2.5799 - val_accuracy: 0.7973 - val_f1_score: 0.9075\n",
      "Epoch 269/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0567 - accuracy: 0.9837 - f1_score: 0.9075 - val_loss: 2.7149 - val_accuracy: 0.7920 - val_f1_score: 0.9075\n",
      "Epoch 270/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9818 - f1_score: 0.9076 - val_loss: 2.5370 - val_accuracy: 0.7964 - val_f1_score: 0.9076\n",
      "Epoch 271/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0606 - accuracy: 0.9824 - f1_score: 0.9076 - val_loss: 2.5075 - val_accuracy: 0.8008 - val_f1_score: 0.9077\n",
      "Epoch 272/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0630 - accuracy: 0.9833 - f1_score: 0.9077 - val_loss: 2.5688 - val_accuracy: 0.8074 - val_f1_score: 0.9077\n",
      "Epoch 273/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0595 - accuracy: 0.9829 - f1_score: 0.9078 - val_loss: 2.7121 - val_accuracy: 0.7933 - val_f1_score: 0.9078\n",
      "Epoch 274/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0622 - accuracy: 0.9829 - f1_score: 0.9078 - val_loss: 2.5816 - val_accuracy: 0.8017 - val_f1_score: 0.9079\n",
      "Epoch 275/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9829 - f1_score: 0.9079 - val_loss: 2.5307 - val_accuracy: 0.8021 - val_f1_score: 0.9080\n",
      "Epoch 276/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0595 - accuracy: 0.9818 - f1_score: 0.9080 - val_loss: 2.7239 - val_accuracy: 0.7925 - val_f1_score: 0.9080\n",
      "Epoch 277/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0599 - accuracy: 0.9816 - f1_score: 0.9080 - val_loss: 2.6140 - val_accuracy: 0.7990 - val_f1_score: 0.9081\n",
      "Epoch 278/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0569 - accuracy: 0.9846 - f1_score: 0.9081 - val_loss: 2.6980 - val_accuracy: 0.7947 - val_f1_score: 0.9081\n",
      "Epoch 279/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0600 - accuracy: 0.9831 - f1_score: 0.9082 - val_loss: 2.6542 - val_accuracy: 0.7947 - val_f1_score: 0.9082\n",
      "Epoch 280/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9833 - f1_score: 0.9082 - val_loss: 2.6642 - val_accuracy: 0.8004 - val_f1_score: 0.9083\n",
      "Epoch 281/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0572 - accuracy: 0.9818 - f1_score: 0.9083 - val_loss: 2.6472 - val_accuracy: 0.7990 - val_f1_score: 0.9083\n",
      "Epoch 282/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0594 - accuracy: 0.9833 - f1_score: 0.9084 - val_loss: 2.6433 - val_accuracy: 0.7973 - val_f1_score: 0.9084\n",
      "Epoch 283/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9827 - f1_score: 0.9084 - val_loss: 2.5778 - val_accuracy: 0.8021 - val_f1_score: 0.9085\n",
      "Epoch 284/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9833 - f1_score: 0.9085 - val_loss: 2.6320 - val_accuracy: 0.8017 - val_f1_score: 0.9085\n",
      "Epoch 285/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0570 - accuracy: 0.9837 - f1_score: 0.9085 - val_loss: 2.6211 - val_accuracy: 0.7982 - val_f1_score: 0.9086\n",
      "Epoch 286/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0570 - accuracy: 0.9812 - f1_score: 0.9086 - val_loss: 2.6706 - val_accuracy: 0.7973 - val_f1_score: 0.9086\n",
      "Epoch 287/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0566 - accuracy: 0.9833 - f1_score: 0.9087 - val_loss: 2.6889 - val_accuracy: 0.7964 - val_f1_score: 0.9087\n",
      "Epoch 288/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0592 - accuracy: 0.9835 - f1_score: 0.9087 - val_loss: 2.6265 - val_accuracy: 0.7977 - val_f1_score: 0.9088\n",
      "Epoch 289/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0539 - accuracy: 0.9842 - f1_score: 0.9088 - val_loss: 2.7131 - val_accuracy: 0.7986 - val_f1_score: 0.9088\n",
      "Epoch 290/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0592 - accuracy: 0.9829 - f1_score: 0.9088 - val_loss: 2.6806 - val_accuracy: 0.7986 - val_f1_score: 0.9089\n",
      "Epoch 291/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0568 - accuracy: 0.9818 - f1_score: 0.9089 - val_loss: 2.7238 - val_accuracy: 0.7933 - val_f1_score: 0.9089\n",
      "Epoch 292/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0630 - accuracy: 0.9822 - f1_score: 0.9090 - val_loss: 2.6818 - val_accuracy: 0.7977 - val_f1_score: 0.9090\n",
      "Epoch 293/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0593 - accuracy: 0.9835 - f1_score: 0.9090 - val_loss: 2.6152 - val_accuracy: 0.8039 - val_f1_score: 0.9090\n",
      "Epoch 294/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9829 - f1_score: 0.9091 - val_loss: 2.6240 - val_accuracy: 0.8012 - val_f1_score: 0.9091\n",
      "Epoch 295/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9831 - f1_score: 0.9091 - val_loss: 2.7851 - val_accuracy: 0.7885 - val_f1_score: 0.9092\n",
      "Epoch 296/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9092 - val_loss: 2.6765 - val_accuracy: 0.7942 - val_f1_score: 0.9092\n",
      "Epoch 297/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0564 - accuracy: 0.9840 - f1_score: 0.9092 - val_loss: 2.5718 - val_accuracy: 0.8052 - val_f1_score: 0.9093\n",
      "Epoch 298/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0603 - accuracy: 0.9842 - f1_score: 0.9093 - val_loss: 2.6199 - val_accuracy: 0.7951 - val_f1_score: 0.9093\n",
      "Epoch 299/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0590 - accuracy: 0.9835 - f1_score: 0.9094 - val_loss: 2.6668 - val_accuracy: 0.7942 - val_f1_score: 0.9094\n",
      "Epoch 300/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9831 - f1_score: 0.9094 - val_loss: 2.7080 - val_accuracy: 0.7995 - val_f1_score: 0.9094\n",
      "Epoch 301/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9095 - val_loss: 2.6686 - val_accuracy: 0.7982 - val_f1_score: 0.9095\n",
      "Epoch 302/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0612 - accuracy: 0.9833 - f1_score: 0.9095 - val_loss: 2.7051 - val_accuracy: 0.7968 - val_f1_score: 0.9096\n",
      "Epoch 303/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0675 - accuracy: 0.9837 - f1_score: 0.9096 - val_loss: 2.7580 - val_accuracy: 0.7929 - val_f1_score: 0.9096\n",
      "Epoch 304/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0618 - accuracy: 0.9827 - f1_score: 0.9096 - val_loss: 2.6114 - val_accuracy: 0.7933 - val_f1_score: 0.9097\n",
      "Epoch 305/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0619 - accuracy: 0.9833 - f1_score: 0.9097 - val_loss: 2.6819 - val_accuracy: 0.7868 - val_f1_score: 0.9097\n",
      "Epoch 306/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9835 - f1_score: 0.9097 - val_loss: 2.6967 - val_accuracy: 0.7850 - val_f1_score: 0.9097\n",
      "Epoch 307/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9842 - f1_score: 0.9098 - val_loss: 2.7047 - val_accuracy: 0.7938 - val_f1_score: 0.9098\n",
      "Epoch 308/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9840 - f1_score: 0.9098 - val_loss: 2.7181 - val_accuracy: 0.7933 - val_f1_score: 0.9098\n",
      "Epoch 309/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9099 - val_loss: 2.7366 - val_accuracy: 0.7933 - val_f1_score: 0.9099\n",
      "Epoch 310/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9842 - f1_score: 0.9099 - val_loss: 2.8397 - val_accuracy: 0.7846 - val_f1_score: 0.9099\n",
      "Epoch 311/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9100 - val_loss: 2.7831 - val_accuracy: 0.7885 - val_f1_score: 0.9100\n",
      "Epoch 312/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0566 - accuracy: 0.9835 - f1_score: 0.9100 - val_loss: 2.7932 - val_accuracy: 0.7855 - val_f1_score: 0.9100\n",
      "Epoch 313/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9100 - val_loss: 2.6791 - val_accuracy: 0.7968 - val_f1_score: 0.9101\n",
      "Epoch 314/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0571 - accuracy: 0.9837 - f1_score: 0.9101 - val_loss: 2.7022 - val_accuracy: 0.7968 - val_f1_score: 0.9101\n",
      "Epoch 315/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0572 - accuracy: 0.9827 - f1_score: 0.9101 - val_loss: 2.7018 - val_accuracy: 0.7968 - val_f1_score: 0.9102\n",
      "Epoch 316/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0575 - accuracy: 0.9829 - f1_score: 0.9102 - val_loss: 2.6536 - val_accuracy: 0.7955 - val_f1_score: 0.9102\n",
      "Epoch 317/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0565 - accuracy: 0.9820 - f1_score: 0.9102 - val_loss: 2.6882 - val_accuracy: 0.7907 - val_f1_score: 0.9103\n",
      "Epoch 318/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0566 - accuracy: 0.9842 - f1_score: 0.9103 - val_loss: 2.6656 - val_accuracy: 0.7925 - val_f1_score: 0.9103\n",
      "Epoch 319/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0592 - accuracy: 0.9835 - f1_score: 0.9103 - val_loss: 2.6934 - val_accuracy: 0.7907 - val_f1_score: 0.9103\n",
      "Epoch 320/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0595 - accuracy: 0.9829 - f1_score: 0.9104 - val_loss: 2.6431 - val_accuracy: 0.7973 - val_f1_score: 0.9104\n",
      "Epoch 321/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9104 - val_loss: 2.7692 - val_accuracy: 0.7920 - val_f1_score: 0.9104\n",
      "Epoch 322/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0592 - accuracy: 0.9820 - f1_score: 0.9105 - val_loss: 2.7480 - val_accuracy: 0.7947 - val_f1_score: 0.9105\n",
      "Epoch 323/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9105 - val_loss: 2.7647 - val_accuracy: 0.7894 - val_f1_score: 0.9105\n",
      "Epoch 324/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9829 - f1_score: 0.9105 - val_loss: 2.7068 - val_accuracy: 0.7973 - val_f1_score: 0.9106\n",
      "Epoch 325/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0595 - accuracy: 0.9844 - f1_score: 0.9106 - val_loss: 2.6200 - val_accuracy: 0.8039 - val_f1_score: 0.9106\n",
      "Epoch 326/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9106 - val_loss: 2.6763 - val_accuracy: 0.8025 - val_f1_score: 0.9107\n",
      "Epoch 327/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0601 - accuracy: 0.9837 - f1_score: 0.9107 - val_loss: 2.7731 - val_accuracy: 0.7916 - val_f1_score: 0.9107\n",
      "Epoch 328/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9107 - val_loss: 2.7422 - val_accuracy: 0.7916 - val_f1_score: 0.9108\n",
      "Epoch 329/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0590 - accuracy: 0.9842 - f1_score: 0.9108 - val_loss: 2.7176 - val_accuracy: 0.7973 - val_f1_score: 0.9108\n",
      "Epoch 330/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9108 - val_loss: 2.8080 - val_accuracy: 0.7933 - val_f1_score: 0.9108\n",
      "Epoch 331/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0598 - accuracy: 0.9829 - f1_score: 0.9109 - val_loss: 2.7019 - val_accuracy: 0.8030 - val_f1_score: 0.9109\n",
      "Epoch 332/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0569 - accuracy: 0.9833 - f1_score: 0.9109 - val_loss: 2.7300 - val_accuracy: 0.7942 - val_f1_score: 0.9109\n",
      "Epoch 333/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0536 - accuracy: 0.9844 - f1_score: 0.9110 - val_loss: 2.6933 - val_accuracy: 0.8034 - val_f1_score: 0.9110\n",
      "Epoch 334/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9831 - f1_score: 0.9110 - val_loss: 2.7503 - val_accuracy: 0.7898 - val_f1_score: 0.9110\n",
      "Epoch 335/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0571 - accuracy: 0.9820 - f1_score: 0.9110 - val_loss: 2.7067 - val_accuracy: 0.7938 - val_f1_score: 0.9111\n",
      "Epoch 336/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9111 - val_loss: 2.7113 - val_accuracy: 0.7933 - val_f1_score: 0.9111\n",
      "Epoch 337/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0628 - accuracy: 0.9833 - f1_score: 0.9111 - val_loss: 2.7249 - val_accuracy: 0.7933 - val_f1_score: 0.9111\n",
      "Epoch 338/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9112 - val_loss: 2.7397 - val_accuracy: 0.7955 - val_f1_score: 0.9112\n",
      "Epoch 339/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9112 - val_loss: 2.7660 - val_accuracy: 0.7960 - val_f1_score: 0.9112\n",
      "Epoch 340/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9829 - f1_score: 0.9113 - val_loss: 2.7197 - val_accuracy: 0.7968 - val_f1_score: 0.9113\n",
      "Epoch 341/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9113 - val_loss: 2.7166 - val_accuracy: 0.7990 - val_f1_score: 0.9113\n",
      "Epoch 342/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0595 - accuracy: 0.9839 - f1_score: 0.9113 - val_loss: 2.7163 - val_accuracy: 0.7977 - val_f1_score: 0.9114\n",
      "Epoch 343/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9831 - f1_score: 0.9114 - val_loss: 2.7081 - val_accuracy: 0.7990 - val_f1_score: 0.9114\n",
      "Epoch 344/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0567 - accuracy: 0.9835 - f1_score: 0.9114 - val_loss: 2.7209 - val_accuracy: 0.7982 - val_f1_score: 0.9114\n",
      "Epoch 345/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9115 - val_loss: 2.7390 - val_accuracy: 0.7995 - val_f1_score: 0.9115\n",
      "Epoch 346/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0591 - accuracy: 0.9837 - f1_score: 0.9115 - val_loss: 2.7468 - val_accuracy: 0.7977 - val_f1_score: 0.9115\n",
      "Epoch 347/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9116 - val_loss: 2.7092 - val_accuracy: 0.7951 - val_f1_score: 0.9116\n",
      "Epoch 348/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9829 - f1_score: 0.9116 - val_loss: 2.7415 - val_accuracy: 0.7973 - val_f1_score: 0.9116\n",
      "Epoch 349/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9840 - f1_score: 0.9116 - val_loss: 2.7301 - val_accuracy: 0.7964 - val_f1_score: 0.9117\n",
      "Epoch 350/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9117 - val_loss: 2.7356 - val_accuracy: 0.7925 - val_f1_score: 0.9117\n",
      "Epoch 351/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9827 - f1_score: 0.9117 - val_loss: 2.7639 - val_accuracy: 0.7885 - val_f1_score: 0.9117\n",
      "Epoch 352/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9840 - f1_score: 0.9117 - val_loss: 2.8009 - val_accuracy: 0.7846 - val_f1_score: 0.9118\n",
      "Epoch 353/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9118 - val_loss: 2.7099 - val_accuracy: 0.7968 - val_f1_score: 0.9118\n",
      "Epoch 354/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9118 - val_loss: 2.7697 - val_accuracy: 0.7912 - val_f1_score: 0.9118\n",
      "Epoch 355/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9839 - f1_score: 0.9119 - val_loss: 2.7749 - val_accuracy: 0.7929 - val_f1_score: 0.9119\n",
      "Epoch 356/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9850 - f1_score: 0.9119 - val_loss: 2.7239 - val_accuracy: 0.7968 - val_f1_score: 0.9119\n",
      "Epoch 357/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9846 - f1_score: 0.9119 - val_loss: 2.7531 - val_accuracy: 0.7903 - val_f1_score: 0.9119\n",
      "Epoch 358/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9844 - f1_score: 0.9120 - val_loss: 2.8137 - val_accuracy: 0.7833 - val_f1_score: 0.9120\n",
      "Epoch 359/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0589 - accuracy: 0.9835 - f1_score: 0.9120 - val_loss: 2.7717 - val_accuracy: 0.7916 - val_f1_score: 0.9120\n",
      "Epoch 360/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0590 - accuracy: 0.9839 - f1_score: 0.9120 - val_loss: 2.7062 - val_accuracy: 0.7982 - val_f1_score: 0.9121\n",
      "Epoch 361/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0625 - accuracy: 0.9827 - f1_score: 0.9121 - val_loss: 2.7269 - val_accuracy: 0.7955 - val_f1_score: 0.9121\n",
      "Epoch 362/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0645 - accuracy: 0.9824 - f1_score: 0.9121 - val_loss: 2.6365 - val_accuracy: 0.7995 - val_f1_score: 0.9121\n",
      "Epoch 363/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0562 - accuracy: 0.9822 - f1_score: 0.9121 - val_loss: 2.6724 - val_accuracy: 0.8025 - val_f1_score: 0.9122\n",
      "Epoch 364/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9122 - val_loss: 2.7594 - val_accuracy: 0.7898 - val_f1_score: 0.9122\n",
      "Epoch 365/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9122 - val_loss: 2.7224 - val_accuracy: 0.7968 - val_f1_score: 0.9122\n",
      "Epoch 366/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0594 - accuracy: 0.9837 - f1_score: 0.9123 - val_loss: 2.6827 - val_accuracy: 0.7947 - val_f1_score: 0.9123\n",
      "Epoch 367/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0567 - accuracy: 0.9835 - f1_score: 0.9123 - val_loss: 2.7065 - val_accuracy: 0.7938 - val_f1_score: 0.9123\n",
      "Epoch 368/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9842 - f1_score: 0.9123 - val_loss: 2.7660 - val_accuracy: 0.7916 - val_f1_score: 0.9123\n",
      "Epoch 369/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9840 - f1_score: 0.9124 - val_loss: 2.6475 - val_accuracy: 0.7995 - val_f1_score: 0.9124\n",
      "Epoch 370/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9833 - f1_score: 0.9124 - val_loss: 2.8045 - val_accuracy: 0.7885 - val_f1_score: 0.9124\n",
      "Epoch 371/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9842 - f1_score: 0.9124 - val_loss: 2.6704 - val_accuracy: 0.7968 - val_f1_score: 0.9124\n",
      "Epoch 372/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9125 - val_loss: 2.6355 - val_accuracy: 0.7986 - val_f1_score: 0.9125\n",
      "Epoch 373/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9839 - f1_score: 0.9125 - val_loss: 2.6549 - val_accuracy: 0.7929 - val_f1_score: 0.9125\n",
      "Epoch 374/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0567 - accuracy: 0.9839 - f1_score: 0.9125 - val_loss: 2.6945 - val_accuracy: 0.7942 - val_f1_score: 0.9125\n",
      "Epoch 375/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9126 - val_loss: 2.6536 - val_accuracy: 0.7968 - val_f1_score: 0.9126\n",
      "Epoch 376/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0597 - accuracy: 0.9831 - f1_score: 0.9126 - val_loss: 2.6427 - val_accuracy: 0.7973 - val_f1_score: 0.9126\n",
      "Epoch 377/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9126 - val_loss: 2.6466 - val_accuracy: 0.7964 - val_f1_score: 0.9127\n",
      "Epoch 378/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9827 - f1_score: 0.9127 - val_loss: 2.7312 - val_accuracy: 0.7912 - val_f1_score: 0.9127\n",
      "Epoch 379/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0590 - accuracy: 0.9835 - f1_score: 0.9127 - val_loss: 2.6761 - val_accuracy: 0.7947 - val_f1_score: 0.9127\n",
      "Epoch 380/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9829 - f1_score: 0.9127 - val_loss: 2.6610 - val_accuracy: 0.7973 - val_f1_score: 0.9128\n",
      "Epoch 381/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9829 - f1_score: 0.9128 - val_loss: 2.7470 - val_accuracy: 0.7933 - val_f1_score: 0.9128\n",
      "Epoch 382/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9825 - f1_score: 0.9128 - val_loss: 2.7113 - val_accuracy: 0.7947 - val_f1_score: 0.9128\n",
      "Epoch 383/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9829 - f1_score: 0.9128 - val_loss: 2.7222 - val_accuracy: 0.7925 - val_f1_score: 0.9128\n",
      "Epoch 384/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9129 - val_loss: 2.7139 - val_accuracy: 0.7933 - val_f1_score: 0.9129\n",
      "Epoch 385/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9129 - val_loss: 2.7279 - val_accuracy: 0.7942 - val_f1_score: 0.9129\n",
      "Epoch 386/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9129 - val_loss: 2.7172 - val_accuracy: 0.7960 - val_f1_score: 0.9129\n",
      "Epoch 387/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9130 - val_loss: 2.8000 - val_accuracy: 0.7881 - val_f1_score: 0.9130\n",
      "Epoch 388/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9130 - val_loss: 2.7481 - val_accuracy: 0.7938 - val_f1_score: 0.9130\n",
      "Epoch 389/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0592 - accuracy: 0.9831 - f1_score: 0.9130 - val_loss: 2.8023 - val_accuracy: 0.7881 - val_f1_score: 0.9130\n",
      "Epoch 390/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0600 - accuracy: 0.9833 - f1_score: 0.9130 - val_loss: 2.7431 - val_accuracy: 0.7903 - val_f1_score: 0.9131\n",
      "Epoch 391/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9131 - val_loss: 2.7243 - val_accuracy: 0.7982 - val_f1_score: 0.9131\n",
      "Epoch 392/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0566 - accuracy: 0.9829 - f1_score: 0.9131 - val_loss: 2.8029 - val_accuracy: 0.7920 - val_f1_score: 0.9131\n",
      "Epoch 393/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0602 - accuracy: 0.9822 - f1_score: 0.9131 - val_loss: 2.6453 - val_accuracy: 0.7951 - val_f1_score: 0.9131\n",
      "Epoch 394/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9132 - val_loss: 2.6844 - val_accuracy: 0.7933 - val_f1_score: 0.9132\n",
      "Epoch 395/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0589 - accuracy: 0.9840 - f1_score: 0.9132 - val_loss: 2.6707 - val_accuracy: 0.7955 - val_f1_score: 0.9132\n",
      "Epoch 396/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0597 - accuracy: 0.9824 - f1_score: 0.9132 - val_loss: 2.7093 - val_accuracy: 0.7903 - val_f1_score: 0.9132\n",
      "Epoch 397/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9837 - f1_score: 0.9132 - val_loss: 2.6295 - val_accuracy: 0.8012 - val_f1_score: 0.9133\n",
      "Epoch 398/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9840 - f1_score: 0.9133 - val_loss: 2.7347 - val_accuracy: 0.7916 - val_f1_score: 0.9133\n",
      "Epoch 399/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9133 - val_loss: 2.7030 - val_accuracy: 0.7982 - val_f1_score: 0.9133\n",
      "Epoch 400/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0562 - accuracy: 0.9824 - f1_score: 0.9133 - val_loss: 2.7405 - val_accuracy: 0.7920 - val_f1_score: 0.9133\n",
      "Epoch 401/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0595 - accuracy: 0.9837 - f1_score: 0.9134 - val_loss: 2.6999 - val_accuracy: 0.7982 - val_f1_score: 0.9134\n",
      "Epoch 402/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9134 - val_loss: 2.6836 - val_accuracy: 0.7955 - val_f1_score: 0.9134\n",
      "Epoch 403/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0592 - accuracy: 0.9835 - f1_score: 0.9134 - val_loss: 2.7048 - val_accuracy: 0.7938 - val_f1_score: 0.9134\n",
      "Epoch 404/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0593 - accuracy: 0.9829 - f1_score: 0.9135 - val_loss: 2.7173 - val_accuracy: 0.7925 - val_f1_score: 0.9135\n",
      "Epoch 405/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9135 - val_loss: 2.7404 - val_accuracy: 0.7907 - val_f1_score: 0.9135\n",
      "Epoch 406/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9135 - val_loss: 2.7120 - val_accuracy: 0.7942 - val_f1_score: 0.9135\n",
      "Epoch 407/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0595 - accuracy: 0.9831 - f1_score: 0.9135 - val_loss: 2.7350 - val_accuracy: 0.7885 - val_f1_score: 0.9135\n",
      "Epoch 408/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9136 - val_loss: 2.7089 - val_accuracy: 0.7964 - val_f1_score: 0.9136\n",
      "Epoch 409/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9833 - f1_score: 0.9136 - val_loss: 2.6332 - val_accuracy: 0.7982 - val_f1_score: 0.9136\n",
      "Epoch 410/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0619 - accuracy: 0.9831 - f1_score: 0.9136 - val_loss: 2.7425 - val_accuracy: 0.7929 - val_f1_score: 0.9136\n",
      "Epoch 411/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9136 - val_loss: 2.7573 - val_accuracy: 0.7964 - val_f1_score: 0.9137\n",
      "Epoch 412/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9137 - val_loss: 2.7447 - val_accuracy: 0.7973 - val_f1_score: 0.9137\n",
      "Epoch 413/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9839 - f1_score: 0.9137 - val_loss: 2.6542 - val_accuracy: 0.8043 - val_f1_score: 0.9137\n",
      "Epoch 414/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9137 - val_loss: 2.7581 - val_accuracy: 0.8021 - val_f1_score: 0.9138\n",
      "Epoch 415/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9138 - val_loss: 2.7213 - val_accuracy: 0.8025 - val_f1_score: 0.9138\n",
      "Epoch 416/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9839 - f1_score: 0.9138 - val_loss: 2.6608 - val_accuracy: 0.8030 - val_f1_score: 0.9138\n",
      "Epoch 417/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9138 - val_loss: 2.7160 - val_accuracy: 0.7986 - val_f1_score: 0.9139\n",
      "Epoch 418/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9139 - val_loss: 2.7378 - val_accuracy: 0.7960 - val_f1_score: 0.9139\n",
      "Epoch 419/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9139 - val_loss: 2.7132 - val_accuracy: 0.7973 - val_f1_score: 0.9139\n",
      "Epoch 420/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9139 - val_loss: 2.7383 - val_accuracy: 0.7951 - val_f1_score: 0.9139\n",
      "Epoch 421/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0595 - accuracy: 0.9835 - f1_score: 0.9140 - val_loss: 2.7259 - val_accuracy: 0.7977 - val_f1_score: 0.9140\n",
      "Epoch 422/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9825 - f1_score: 0.9140 - val_loss: 2.6678 - val_accuracy: 0.8030 - val_f1_score: 0.9140\n",
      "Epoch 423/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0567 - accuracy: 0.9839 - f1_score: 0.9140 - val_loss: 2.6798 - val_accuracy: 0.8017 - val_f1_score: 0.9140\n",
      "Epoch 424/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9140 - val_loss: 2.7093 - val_accuracy: 0.7982 - val_f1_score: 0.9141\n",
      "Epoch 425/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9141 - val_loss: 2.7627 - val_accuracy: 0.7982 - val_f1_score: 0.9141\n",
      "Epoch 426/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9141 - val_loss: 2.7411 - val_accuracy: 0.7960 - val_f1_score: 0.9141\n",
      "Epoch 427/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0565 - accuracy: 0.9844 - f1_score: 0.9141 - val_loss: 2.7492 - val_accuracy: 0.7951 - val_f1_score: 0.9142\n",
      "Epoch 428/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9839 - f1_score: 0.9142 - val_loss: 2.7816 - val_accuracy: 0.7973 - val_f1_score: 0.9142\n",
      "Epoch 429/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9837 - f1_score: 0.9142 - val_loss: 2.7495 - val_accuracy: 0.7955 - val_f1_score: 0.9142\n",
      "Epoch 430/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9850 - f1_score: 0.9142 - val_loss: 2.7316 - val_accuracy: 0.7973 - val_f1_score: 0.9142\n",
      "Epoch 431/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9848 - f1_score: 0.9143 - val_loss: 2.7379 - val_accuracy: 0.7982 - val_f1_score: 0.9143\n",
      "Epoch 432/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0621 - accuracy: 0.9820 - f1_score: 0.9143 - val_loss: 2.7255 - val_accuracy: 0.7964 - val_f1_score: 0.9143\n",
      "Epoch 433/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9143 - val_loss: 2.7090 - val_accuracy: 0.7968 - val_f1_score: 0.9143\n",
      "Epoch 434/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0616 - accuracy: 0.9842 - f1_score: 0.9143 - val_loss: 2.7389 - val_accuracy: 0.7977 - val_f1_score: 0.9143\n",
      "Epoch 435/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9825 - f1_score: 0.9144 - val_loss: 2.7505 - val_accuracy: 0.7964 - val_f1_score: 0.9144\n",
      "Epoch 436/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0613 - accuracy: 0.9833 - f1_score: 0.9144 - val_loss: 2.7288 - val_accuracy: 0.7995 - val_f1_score: 0.9144\n",
      "Epoch 437/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9144 - val_loss: 2.7705 - val_accuracy: 0.7951 - val_f1_score: 0.9144\n",
      "Epoch 438/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9144 - val_loss: 2.7507 - val_accuracy: 0.7982 - val_f1_score: 0.9145\n",
      "Epoch 439/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9145 - val_loss: 2.8071 - val_accuracy: 0.7933 - val_f1_score: 0.9145\n",
      "Epoch 440/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0617 - accuracy: 0.9833 - f1_score: 0.9145 - val_loss: 2.8222 - val_accuracy: 0.7907 - val_f1_score: 0.9145\n",
      "Epoch 441/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9145 - val_loss: 2.8001 - val_accuracy: 0.7947 - val_f1_score: 0.9145\n",
      "Epoch 442/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9145 - val_loss: 2.8047 - val_accuracy: 0.7925 - val_f1_score: 0.9146\n",
      "Epoch 443/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0621 - accuracy: 0.9831 - f1_score: 0.9146 - val_loss: 2.7705 - val_accuracy: 0.7951 - val_f1_score: 0.9146\n",
      "Epoch 444/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9831 - f1_score: 0.9146 - val_loss: 2.7982 - val_accuracy: 0.7964 - val_f1_score: 0.9146\n",
      "Epoch 445/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9146 - val_loss: 2.8164 - val_accuracy: 0.7929 - val_f1_score: 0.9146\n",
      "Epoch 446/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0616 - accuracy: 0.9840 - f1_score: 0.9146 - val_loss: 2.7476 - val_accuracy: 0.7955 - val_f1_score: 0.9146\n",
      "Epoch 447/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0591 - accuracy: 0.9833 - f1_score: 0.9147 - val_loss: 2.7952 - val_accuracy: 0.7942 - val_f1_score: 0.9147\n",
      "Epoch 448/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9147 - val_loss: 2.7852 - val_accuracy: 0.7960 - val_f1_score: 0.9147\n",
      "Epoch 449/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9839 - f1_score: 0.9147 - val_loss: 2.8230 - val_accuracy: 0.7942 - val_f1_score: 0.9147\n",
      "Epoch 450/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9825 - f1_score: 0.9147 - val_loss: 2.7000 - val_accuracy: 0.7999 - val_f1_score: 0.9147\n",
      "Epoch 451/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9148 - val_loss: 2.7999 - val_accuracy: 0.7960 - val_f1_score: 0.9148\n",
      "Epoch 452/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9148 - val_loss: 2.7280 - val_accuracy: 0.7986 - val_f1_score: 0.9148\n",
      "Epoch 453/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9829 - f1_score: 0.9148 - val_loss: 2.8160 - val_accuracy: 0.7938 - val_f1_score: 0.9148\n",
      "Epoch 454/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9148 - val_loss: 2.7771 - val_accuracy: 0.7960 - val_f1_score: 0.9148\n",
      "Epoch 455/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9844 - f1_score: 0.9149 - val_loss: 2.8263 - val_accuracy: 0.7881 - val_f1_score: 0.9149\n",
      "Epoch 456/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0591 - accuracy: 0.9831 - f1_score: 0.9149 - val_loss: 2.7820 - val_accuracy: 0.7894 - val_f1_score: 0.9149\n",
      "Epoch 457/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0590 - accuracy: 0.9818 - f1_score: 0.9149 - val_loss: 2.7411 - val_accuracy: 0.7955 - val_f1_score: 0.9149\n",
      "Epoch 458/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9149 - val_loss: 2.7146 - val_accuracy: 0.7977 - val_f1_score: 0.9149\n",
      "Epoch 459/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9149 - val_loss: 2.7163 - val_accuracy: 0.7960 - val_f1_score: 0.9149\n",
      "Epoch 460/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9150 - val_loss: 2.7409 - val_accuracy: 0.7947 - val_f1_score: 0.9150\n",
      "Epoch 461/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9859 - f1_score: 0.9150 - val_loss: 2.7358 - val_accuracy: 0.7973 - val_f1_score: 0.9150\n",
      "Epoch 462/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9150 - val_loss: 2.7020 - val_accuracy: 0.8021 - val_f1_score: 0.9150\n",
      "Epoch 463/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0564 - accuracy: 0.9842 - f1_score: 0.9150 - val_loss: 2.6767 - val_accuracy: 0.8012 - val_f1_score: 0.9150\n",
      "Epoch 464/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9825 - f1_score: 0.9151 - val_loss: 2.7150 - val_accuracy: 0.7990 - val_f1_score: 0.9151\n",
      "Epoch 465/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0619 - accuracy: 0.9827 - f1_score: 0.9151 - val_loss: 2.6887 - val_accuracy: 0.7990 - val_f1_score: 0.9151\n",
      "Epoch 466/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9151 - val_loss: 2.6938 - val_accuracy: 0.8012 - val_f1_score: 0.9151\n",
      "Epoch 467/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9151 - val_loss: 2.6799 - val_accuracy: 0.8017 - val_f1_score: 0.9151\n",
      "Epoch 468/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9152 - val_loss: 2.7139 - val_accuracy: 0.7982 - val_f1_score: 0.9152\n",
      "Epoch 469/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9152 - val_loss: 2.7024 - val_accuracy: 0.7982 - val_f1_score: 0.9152\n",
      "Epoch 470/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9152 - val_loss: 2.7161 - val_accuracy: 0.7995 - val_f1_score: 0.9152\n",
      "Epoch 471/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9152 - val_loss: 2.7919 - val_accuracy: 0.7999 - val_f1_score: 0.9153\n",
      "Epoch 472/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9153 - val_loss: 2.7956 - val_accuracy: 0.7990 - val_f1_score: 0.9153\n",
      "Epoch 473/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9824 - f1_score: 0.9153 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9153\n",
      "Epoch 474/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9153 - val_loss: 2.7899 - val_accuracy: 0.7929 - val_f1_score: 0.9153\n",
      "Epoch 475/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9153 - val_loss: 2.7668 - val_accuracy: 0.7968 - val_f1_score: 0.9153\n",
      "Epoch 476/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9154 - val_loss: 2.7118 - val_accuracy: 0.7995 - val_f1_score: 0.9154\n",
      "Epoch 477/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9154 - val_loss: 2.7303 - val_accuracy: 0.7977 - val_f1_score: 0.9154\n",
      "Epoch 478/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9154 - val_loss: 2.7575 - val_accuracy: 0.7960 - val_f1_score: 0.9154\n",
      "Epoch 479/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9154 - val_loss: 2.7890 - val_accuracy: 0.7925 - val_f1_score: 0.9154\n",
      "Epoch 480/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9835 - f1_score: 0.9154 - val_loss: 2.7711 - val_accuracy: 0.7929 - val_f1_score: 0.9155\n",
      "Epoch 481/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0586 - accuracy: 0.9827 - f1_score: 0.9155 - val_loss: 2.7551 - val_accuracy: 0.7960 - val_f1_score: 0.9155\n",
      "Epoch 482/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9155 - val_loss: 2.7273 - val_accuracy: 0.7960 - val_f1_score: 0.9155\n",
      "Epoch 483/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9824 - f1_score: 0.9155 - val_loss: 2.7349 - val_accuracy: 0.7929 - val_f1_score: 0.9155\n",
      "Epoch 484/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9155 - val_loss: 2.7510 - val_accuracy: 0.7942 - val_f1_score: 0.9155\n",
      "Epoch 485/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9155 - val_loss: 2.7663 - val_accuracy: 0.7938 - val_f1_score: 0.9156\n",
      "Epoch 486/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9156 - val_loss: 2.7593 - val_accuracy: 0.7960 - val_f1_score: 0.9156\n",
      "Epoch 487/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9156 - val_loss: 2.7278 - val_accuracy: 0.7964 - val_f1_score: 0.9156\n",
      "Epoch 488/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9156 - val_loss: 2.7487 - val_accuracy: 0.7995 - val_f1_score: 0.9156\n",
      "Epoch 489/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9156 - val_loss: 2.7672 - val_accuracy: 0.7964 - val_f1_score: 0.9156\n",
      "Epoch 490/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9156 - val_loss: 2.7394 - val_accuracy: 0.7951 - val_f1_score: 0.9157\n",
      "Epoch 491/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9157 - val_loss: 2.7390 - val_accuracy: 0.7977 - val_f1_score: 0.9157\n",
      "Epoch 492/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9157 - val_loss: 2.7755 - val_accuracy: 0.7955 - val_f1_score: 0.9157\n",
      "Epoch 493/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9157 - val_loss: 2.7415 - val_accuracy: 0.7951 - val_f1_score: 0.9157\n",
      "Epoch 494/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9157 - val_loss: 2.7506 - val_accuracy: 0.7968 - val_f1_score: 0.9157\n",
      "Epoch 495/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9158 - val_loss: 2.7633 - val_accuracy: 0.7982 - val_f1_score: 0.9158\n",
      "Epoch 496/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9158 - val_loss: 2.7507 - val_accuracy: 0.7947 - val_f1_score: 0.9158\n",
      "Epoch 497/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9158 - val_loss: 2.7558 - val_accuracy: 0.7942 - val_f1_score: 0.9158\n",
      "Epoch 498/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9158 - val_loss: 2.8037 - val_accuracy: 0.7938 - val_f1_score: 0.9158\n",
      "Epoch 499/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9158 - val_loss: 2.7392 - val_accuracy: 0.7947 - val_f1_score: 0.9158\n",
      "Epoch 500/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9159 - val_loss: 2.7819 - val_accuracy: 0.7995 - val_f1_score: 0.9159\n",
      "Epoch 501/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9159 - val_loss: 2.7272 - val_accuracy: 0.7938 - val_f1_score: 0.9159\n",
      "Epoch 502/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9159 - val_loss: 2.7878 - val_accuracy: 0.7920 - val_f1_score: 0.9159\n",
      "Epoch 503/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9159 - val_loss: 2.7714 - val_accuracy: 0.7960 - val_f1_score: 0.9159\n",
      "Epoch 504/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9159 - val_loss: 2.7756 - val_accuracy: 0.7912 - val_f1_score: 0.9159\n",
      "Epoch 505/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9824 - f1_score: 0.9159 - val_loss: 2.7429 - val_accuracy: 0.7960 - val_f1_score: 0.9160\n",
      "Epoch 506/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9160 - val_loss: 2.7837 - val_accuracy: 0.7947 - val_f1_score: 0.9160\n",
      "Epoch 507/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0573 - accuracy: 0.9829 - f1_score: 0.9160 - val_loss: 2.6986 - val_accuracy: 0.7960 - val_f1_score: 0.9160\n",
      "Epoch 508/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9825 - f1_score: 0.9160 - val_loss: 2.6828 - val_accuracy: 0.7986 - val_f1_score: 0.9160\n",
      "Epoch 509/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9160 - val_loss: 2.6959 - val_accuracy: 0.7933 - val_f1_score: 0.9160\n",
      "Epoch 510/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9160 - val_loss: 2.7699 - val_accuracy: 0.7947 - val_f1_score: 0.9160\n",
      "Epoch 511/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9161 - val_loss: 2.7575 - val_accuracy: 0.7912 - val_f1_score: 0.9161\n",
      "Epoch 512/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0593 - accuracy: 0.9829 - f1_score: 0.9161 - val_loss: 2.7290 - val_accuracy: 0.7968 - val_f1_score: 0.9161\n",
      "Epoch 513/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9839 - f1_score: 0.9161 - val_loss: 2.6678 - val_accuracy: 0.7960 - val_f1_score: 0.9161\n",
      "Epoch 514/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9161 - val_loss: 2.6388 - val_accuracy: 0.7973 - val_f1_score: 0.9161\n",
      "Epoch 515/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9161 - val_loss: 2.6700 - val_accuracy: 0.8021 - val_f1_score: 0.9161\n",
      "Epoch 516/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9162 - val_loss: 2.7183 - val_accuracy: 0.7964 - val_f1_score: 0.9162\n",
      "Epoch 517/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9162 - val_loss: 2.7061 - val_accuracy: 0.7977 - val_f1_score: 0.9162\n",
      "Epoch 518/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9162 - val_loss: 2.7409 - val_accuracy: 0.7968 - val_f1_score: 0.9162\n",
      "Epoch 519/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0575 - accuracy: 0.9839 - f1_score: 0.9162 - val_loss: 2.7709 - val_accuracy: 0.7955 - val_f1_score: 0.9162\n",
      "Epoch 520/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9829 - f1_score: 0.9162 - val_loss: 2.7913 - val_accuracy: 0.7912 - val_f1_score: 0.9162\n",
      "Epoch 521/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9162 - val_loss: 2.7243 - val_accuracy: 0.7951 - val_f1_score: 0.9163\n",
      "Epoch 522/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9163 - val_loss: 2.7177 - val_accuracy: 0.7955 - val_f1_score: 0.9163\n",
      "Epoch 523/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9827 - f1_score: 0.9163 - val_loss: 2.6625 - val_accuracy: 0.8004 - val_f1_score: 0.9163\n",
      "Epoch 524/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9163 - val_loss: 2.7457 - val_accuracy: 0.7995 - val_f1_score: 0.9163\n",
      "Epoch 525/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9846 - f1_score: 0.9163 - val_loss: 2.6822 - val_accuracy: 0.7999 - val_f1_score: 0.9163\n",
      "Epoch 526/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9163 - val_loss: 2.7341 - val_accuracy: 0.7955 - val_f1_score: 0.9164\n",
      "Epoch 527/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0593 - accuracy: 0.9833 - f1_score: 0.9164 - val_loss: 2.7467 - val_accuracy: 0.7955 - val_f1_score: 0.9164\n",
      "Epoch 528/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9164 - val_loss: 2.7353 - val_accuracy: 0.7982 - val_f1_score: 0.9164\n",
      "Epoch 529/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9164 - val_loss: 2.7122 - val_accuracy: 0.7999 - val_f1_score: 0.9164\n",
      "Epoch 530/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9164 - val_loss: 2.6759 - val_accuracy: 0.7986 - val_f1_score: 0.9164\n",
      "Epoch 531/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9164 - val_loss: 2.7196 - val_accuracy: 0.7982 - val_f1_score: 0.9164\n",
      "Epoch 532/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0566 - accuracy: 0.9842 - f1_score: 0.9165 - val_loss: 2.7075 - val_accuracy: 0.7990 - val_f1_score: 0.9165\n",
      "Epoch 533/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9165 - val_loss: 2.7183 - val_accuracy: 0.8017 - val_f1_score: 0.9165\n",
      "Epoch 534/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0592 - accuracy: 0.9829 - f1_score: 0.9165 - val_loss: 2.7465 - val_accuracy: 0.7951 - val_f1_score: 0.9165\n",
      "Epoch 535/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9165 - val_loss: 2.7112 - val_accuracy: 0.7995 - val_f1_score: 0.9165\n",
      "Epoch 536/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9165 - val_loss: 2.7121 - val_accuracy: 0.8004 - val_f1_score: 0.9165\n",
      "Epoch 537/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9165 - val_loss: 2.7734 - val_accuracy: 0.7929 - val_f1_score: 0.9166\n",
      "Epoch 538/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0624 - accuracy: 0.9825 - f1_score: 0.9166 - val_loss: 2.7628 - val_accuracy: 0.7942 - val_f1_score: 0.9166\n",
      "Epoch 539/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9166 - val_loss: 2.7112 - val_accuracy: 0.7982 - val_f1_score: 0.9166\n",
      "Epoch 540/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9831 - f1_score: 0.9166 - val_loss: 2.7353 - val_accuracy: 0.7951 - val_f1_score: 0.9166\n",
      "Epoch 541/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9166 - val_loss: 2.7636 - val_accuracy: 0.7942 - val_f1_score: 0.9166\n",
      "Epoch 542/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9166 - val_loss: 2.7564 - val_accuracy: 0.7925 - val_f1_score: 0.9166\n",
      "Epoch 543/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9837 - f1_score: 0.9166 - val_loss: 2.6860 - val_accuracy: 0.7986 - val_f1_score: 0.9167\n",
      "Epoch 544/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9167 - val_loss: 2.6651 - val_accuracy: 0.7955 - val_f1_score: 0.9167\n",
      "Epoch 545/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9167 - val_loss: 2.6665 - val_accuracy: 0.7912 - val_f1_score: 0.9167\n",
      "Epoch 546/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9167 - val_loss: 2.6665 - val_accuracy: 0.7955 - val_f1_score: 0.9167\n",
      "Epoch 547/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9167 - val_loss: 2.8141 - val_accuracy: 0.7933 - val_f1_score: 0.9167\n",
      "Epoch 548/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9167 - val_loss: 2.7211 - val_accuracy: 0.7951 - val_f1_score: 0.9167\n",
      "Epoch 549/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9167 - val_loss: 2.6852 - val_accuracy: 0.7955 - val_f1_score: 0.9168\n",
      "Epoch 550/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9168 - val_loss: 2.6902 - val_accuracy: 0.7951 - val_f1_score: 0.9168\n",
      "Epoch 551/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0566 - accuracy: 0.9827 - f1_score: 0.9168 - val_loss: 2.6797 - val_accuracy: 0.7947 - val_f1_score: 0.9168\n",
      "Epoch 552/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9168 - val_loss: 2.6858 - val_accuracy: 0.7968 - val_f1_score: 0.9168\n",
      "Epoch 553/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9825 - f1_score: 0.9168 - val_loss: 2.7252 - val_accuracy: 0.7947 - val_f1_score: 0.9168\n",
      "Epoch 554/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9168 - val_loss: 2.6535 - val_accuracy: 0.8008 - val_f1_score: 0.9168\n",
      "Epoch 555/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9168 - val_loss: 2.6996 - val_accuracy: 0.7947 - val_f1_score: 0.9168\n",
      "Epoch 556/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9169 - val_loss: 2.6868 - val_accuracy: 0.7964 - val_f1_score: 0.9169\n",
      "Epoch 557/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9842 - f1_score: 0.9169 - val_loss: 2.7383 - val_accuracy: 0.7920 - val_f1_score: 0.9169\n",
      "Epoch 558/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9837 - f1_score: 0.9169 - val_loss: 2.7313 - val_accuracy: 0.7947 - val_f1_score: 0.9169\n",
      "Epoch 559/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9829 - f1_score: 0.9169 - val_loss: 2.6753 - val_accuracy: 0.7964 - val_f1_score: 0.9169\n",
      "Epoch 560/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9169 - val_loss: 2.6867 - val_accuracy: 0.7968 - val_f1_score: 0.9169\n",
      "Epoch 561/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9169 - val_loss: 2.6860 - val_accuracy: 0.7960 - val_f1_score: 0.9169\n",
      "Epoch 562/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9169 - val_loss: 2.6975 - val_accuracy: 0.7977 - val_f1_score: 0.9170\n",
      "Epoch 563/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9170 - val_loss: 2.6914 - val_accuracy: 0.7968 - val_f1_score: 0.9170\n",
      "Epoch 564/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9825 - f1_score: 0.9170 - val_loss: 2.6869 - val_accuracy: 0.7951 - val_f1_score: 0.9170\n",
      "Epoch 565/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9170 - val_loss: 2.6922 - val_accuracy: 0.7938 - val_f1_score: 0.9170\n",
      "Epoch 566/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9170 - val_loss: 2.7424 - val_accuracy: 0.7973 - val_f1_score: 0.9170\n",
      "Epoch 567/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0592 - accuracy: 0.9827 - f1_score: 0.9170 - val_loss: 2.6826 - val_accuracy: 0.7986 - val_f1_score: 0.9170\n",
      "Epoch 568/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9170 - val_loss: 2.6943 - val_accuracy: 0.7973 - val_f1_score: 0.9170\n",
      "Epoch 569/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9171 - val_loss: 2.7116 - val_accuracy: 0.7986 - val_f1_score: 0.9171\n",
      "Epoch 570/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9825 - f1_score: 0.9171 - val_loss: 2.7255 - val_accuracy: 0.7955 - val_f1_score: 0.9171\n",
      "Epoch 571/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9171 - val_loss: 2.7561 - val_accuracy: 0.7903 - val_f1_score: 0.9171\n",
      "Epoch 572/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9171 - val_loss: 2.7343 - val_accuracy: 0.7947 - val_f1_score: 0.9171\n",
      "Epoch 573/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9171 - val_loss: 2.7624 - val_accuracy: 0.7894 - val_f1_score: 0.9171\n",
      "Epoch 574/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9171 - val_loss: 2.7371 - val_accuracy: 0.7942 - val_f1_score: 0.9171\n",
      "Epoch 575/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0576 - accuracy: 0.9837 - f1_score: 0.9171 - val_loss: 2.7465 - val_accuracy: 0.7964 - val_f1_score: 0.9171\n",
      "Epoch 576/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9172 - val_loss: 2.7521 - val_accuracy: 0.7955 - val_f1_score: 0.9172\n",
      "Epoch 577/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9172 - val_loss: 2.7150 - val_accuracy: 0.7999 - val_f1_score: 0.9172\n",
      "Epoch 578/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9172 - val_loss: 2.7425 - val_accuracy: 0.7955 - val_f1_score: 0.9172\n",
      "Epoch 579/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9172 - val_loss: 2.7109 - val_accuracy: 0.7942 - val_f1_score: 0.9172\n",
      "Epoch 580/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9172 - val_loss: 2.7263 - val_accuracy: 0.7968 - val_f1_score: 0.9172\n",
      "Epoch 581/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9172 - val_loss: 2.7071 - val_accuracy: 0.7986 - val_f1_score: 0.9172\n",
      "Epoch 582/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9816 - f1_score: 0.9172 - val_loss: 2.7129 - val_accuracy: 0.7977 - val_f1_score: 0.9172\n",
      "Epoch 583/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9827 - f1_score: 0.9173 - val_loss: 2.6854 - val_accuracy: 0.7995 - val_f1_score: 0.9173\n",
      "Epoch 584/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9824 - f1_score: 0.9173 - val_loss: 2.6915 - val_accuracy: 0.7995 - val_f1_score: 0.9173\n",
      "Epoch 585/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0595 - accuracy: 0.9829 - f1_score: 0.9173 - val_loss: 2.6882 - val_accuracy: 0.7986 - val_f1_score: 0.9173\n",
      "Epoch 586/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9173 - val_loss: 2.7147 - val_accuracy: 0.7977 - val_f1_score: 0.9173\n",
      "Epoch 587/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9173 - val_loss: 2.6773 - val_accuracy: 0.7982 - val_f1_score: 0.9173\n",
      "Epoch 588/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9173 - val_loss: 2.7158 - val_accuracy: 0.7955 - val_f1_score: 0.9173\n",
      "Epoch 589/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9173 - val_loss: 2.7123 - val_accuracy: 0.7942 - val_f1_score: 0.9174\n",
      "Epoch 590/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9174 - val_loss: 2.7425 - val_accuracy: 0.7916 - val_f1_score: 0.9174\n",
      "Epoch 591/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9855 - f1_score: 0.9174 - val_loss: 2.7351 - val_accuracy: 0.7942 - val_f1_score: 0.9174\n",
      "Epoch 592/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9855 - f1_score: 0.9174 - val_loss: 2.7316 - val_accuracy: 0.7942 - val_f1_score: 0.9174\n",
      "Epoch 593/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9174 - val_loss: 2.7682 - val_accuracy: 0.7947 - val_f1_score: 0.9174\n",
      "Epoch 594/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9174 - val_loss: 2.7052 - val_accuracy: 0.7960 - val_f1_score: 0.9174\n",
      "Epoch 595/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9174 - val_loss: 2.7075 - val_accuracy: 0.7960 - val_f1_score: 0.9174\n",
      "Epoch 596/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9174 - val_loss: 2.7107 - val_accuracy: 0.7947 - val_f1_score: 0.9175\n",
      "Epoch 597/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9175 - val_loss: 2.7050 - val_accuracy: 0.7973 - val_f1_score: 0.9175\n",
      "Epoch 598/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9846 - f1_score: 0.9175 - val_loss: 2.7016 - val_accuracy: 0.7951 - val_f1_score: 0.9175\n",
      "Epoch 599/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9175 - val_loss: 2.7031 - val_accuracy: 0.7977 - val_f1_score: 0.9175\n",
      "Epoch 600/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0563 - accuracy: 0.9835 - f1_score: 0.9175 - val_loss: 2.7107 - val_accuracy: 0.7955 - val_f1_score: 0.9175\n",
      "Epoch 601/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9175 - val_loss: 2.7580 - val_accuracy: 0.7938 - val_f1_score: 0.9175\n",
      "Epoch 602/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9175 - val_loss: 2.7598 - val_accuracy: 0.7912 - val_f1_score: 0.9175\n",
      "Epoch 603/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9175 - val_loss: 2.7343 - val_accuracy: 0.7933 - val_f1_score: 0.9175\n",
      "Epoch 604/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9175 - val_loss: 2.7248 - val_accuracy: 0.7929 - val_f1_score: 0.9176\n",
      "Epoch 605/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9831 - f1_score: 0.9176 - val_loss: 2.7324 - val_accuracy: 0.7942 - val_f1_score: 0.9176\n",
      "Epoch 606/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0598 - accuracy: 0.9837 - f1_score: 0.9176 - val_loss: 2.6217 - val_accuracy: 0.8004 - val_f1_score: 0.9176\n",
      "Epoch 607/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9176 - val_loss: 2.6474 - val_accuracy: 0.8034 - val_f1_score: 0.9176\n",
      "Epoch 608/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9176 - val_loss: 2.6490 - val_accuracy: 0.8030 - val_f1_score: 0.9176\n",
      "Epoch 609/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9176 - val_loss: 2.6771 - val_accuracy: 0.7973 - val_f1_score: 0.9176\n",
      "Epoch 610/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9176 - val_loss: 2.6491 - val_accuracy: 0.7986 - val_f1_score: 0.9176\n",
      "Epoch 611/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9177 - val_loss: 2.6950 - val_accuracy: 0.7986 - val_f1_score: 0.9177\n",
      "Epoch 612/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9177 - val_loss: 2.6866 - val_accuracy: 0.7964 - val_f1_score: 0.9177\n",
      "Epoch 613/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9850 - f1_score: 0.9177 - val_loss: 2.6965 - val_accuracy: 0.7947 - val_f1_score: 0.9177\n",
      "Epoch 614/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0596 - accuracy: 0.9835 - f1_score: 0.9177 - val_loss: 2.6775 - val_accuracy: 0.7968 - val_f1_score: 0.9177\n",
      "Epoch 615/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0638 - accuracy: 0.9829 - f1_score: 0.9177 - val_loss: 2.6813 - val_accuracy: 0.7955 - val_f1_score: 0.9177\n",
      "Epoch 616/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9177 - val_loss: 2.6604 - val_accuracy: 0.8034 - val_f1_score: 0.9177\n",
      "Epoch 617/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0604 - accuracy: 0.9839 - f1_score: 0.9177 - val_loss: 2.6785 - val_accuracy: 0.8021 - val_f1_score: 0.9177\n",
      "Epoch 618/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9178 - val_loss: 2.7036 - val_accuracy: 0.7964 - val_f1_score: 0.9178\n",
      "Epoch 619/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0614 - accuracy: 0.9835 - f1_score: 0.9178 - val_loss: 2.6948 - val_accuracy: 0.7982 - val_f1_score: 0.9178\n",
      "Epoch 620/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0576 - accuracy: 0.9833 - f1_score: 0.9178 - val_loss: 2.6706 - val_accuracy: 0.7964 - val_f1_score: 0.9178\n",
      "Epoch 621/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9178 - val_loss: 2.7080 - val_accuracy: 0.7995 - val_f1_score: 0.9178\n",
      "Epoch 622/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9833 - f1_score: 0.9178 - val_loss: 2.7364 - val_accuracy: 0.7964 - val_f1_score: 0.9178\n",
      "Epoch 623/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9178 - val_loss: 2.7310 - val_accuracy: 0.7955 - val_f1_score: 0.9178\n",
      "Epoch 624/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9178 - val_loss: 2.7353 - val_accuracy: 0.7999 - val_f1_score: 0.9178\n",
      "Epoch 625/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9178 - val_loss: 2.7521 - val_accuracy: 0.7973 - val_f1_score: 0.9179\n",
      "Epoch 626/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9179 - val_loss: 2.7009 - val_accuracy: 0.7960 - val_f1_score: 0.9179\n",
      "Epoch 627/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9179 - val_loss: 2.6608 - val_accuracy: 0.7982 - val_f1_score: 0.9179\n",
      "Epoch 628/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9179 - val_loss: 2.6635 - val_accuracy: 0.8004 - val_f1_score: 0.9179\n",
      "Epoch 629/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9179 - val_loss: 2.6688 - val_accuracy: 0.7968 - val_f1_score: 0.9179\n",
      "Epoch 630/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9179 - val_loss: 2.6828 - val_accuracy: 0.7960 - val_f1_score: 0.9179\n",
      "Epoch 631/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0591 - accuracy: 0.9824 - f1_score: 0.9179 - val_loss: 2.7292 - val_accuracy: 0.7947 - val_f1_score: 0.9179\n",
      "Epoch 632/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9179 - val_loss: 2.7122 - val_accuracy: 0.7968 - val_f1_score: 0.9179\n",
      "Epoch 633/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0589 - accuracy: 0.9835 - f1_score: 0.9180 - val_loss: 2.6365 - val_accuracy: 0.7977 - val_f1_score: 0.9180\n",
      "Epoch 634/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9180 - val_loss: 2.6344 - val_accuracy: 0.7982 - val_f1_score: 0.9180\n",
      "Epoch 635/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9835 - f1_score: 0.9180 - val_loss: 2.6715 - val_accuracy: 0.7964 - val_f1_score: 0.9180\n",
      "Epoch 636/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9180 - val_loss: 2.6828 - val_accuracy: 0.7960 - val_f1_score: 0.9180\n",
      "Epoch 637/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9180 - val_loss: 2.7139 - val_accuracy: 0.7960 - val_f1_score: 0.9180\n",
      "Epoch 638/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9842 - f1_score: 0.9180 - val_loss: 2.7682 - val_accuracy: 0.7933 - val_f1_score: 0.9180\n",
      "Epoch 639/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9180 - val_loss: 2.7671 - val_accuracy: 0.7929 - val_f1_score: 0.9180\n",
      "Epoch 640/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9180 - val_loss: 2.7413 - val_accuracy: 0.7929 - val_f1_score: 0.9180\n",
      "Epoch 641/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9181 - val_loss: 2.7174 - val_accuracy: 0.7942 - val_f1_score: 0.9181\n",
      "Epoch 642/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9181 - val_loss: 2.7317 - val_accuracy: 0.7942 - val_f1_score: 0.9181\n",
      "Epoch 643/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9181 - val_loss: 2.6949 - val_accuracy: 0.7955 - val_f1_score: 0.9181\n",
      "Epoch 644/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9824 - f1_score: 0.9181 - val_loss: 2.6641 - val_accuracy: 0.7968 - val_f1_score: 0.9181\n",
      "Epoch 645/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9181 - val_loss: 2.7252 - val_accuracy: 0.7960 - val_f1_score: 0.9181\n",
      "Epoch 646/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9181 - val_loss: 2.6692 - val_accuracy: 0.7964 - val_f1_score: 0.9181\n",
      "Epoch 647/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9181 - val_loss: 2.7219 - val_accuracy: 0.7942 - val_f1_score: 0.9181\n",
      "Epoch 648/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9825 - f1_score: 0.9181 - val_loss: 2.7126 - val_accuracy: 0.7938 - val_f1_score: 0.9181\n",
      "Epoch 649/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9181 - val_loss: 2.7181 - val_accuracy: 0.7933 - val_f1_score: 0.9181\n",
      "Epoch 650/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9182 - val_loss: 2.7234 - val_accuracy: 0.7955 - val_f1_score: 0.9182\n",
      "Epoch 651/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9182 - val_loss: 2.7129 - val_accuracy: 0.7964 - val_f1_score: 0.9182\n",
      "Epoch 652/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9182 - val_loss: 2.7225 - val_accuracy: 0.7986 - val_f1_score: 0.9182\n",
      "Epoch 653/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9182 - val_loss: 2.6949 - val_accuracy: 0.8008 - val_f1_score: 0.9182\n",
      "Epoch 654/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9182 - val_loss: 2.6939 - val_accuracy: 0.8004 - val_f1_score: 0.9182\n",
      "Epoch 655/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0607 - accuracy: 0.9831 - f1_score: 0.9182 - val_loss: 2.7122 - val_accuracy: 0.7964 - val_f1_score: 0.9182\n",
      "Epoch 656/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0601 - accuracy: 0.9833 - f1_score: 0.9182 - val_loss: 2.7174 - val_accuracy: 0.7929 - val_f1_score: 0.9182\n",
      "Epoch 657/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9182 - val_loss: 2.7115 - val_accuracy: 0.7951 - val_f1_score: 0.9182\n",
      "Epoch 658/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9183 - val_loss: 2.6904 - val_accuracy: 0.7986 - val_f1_score: 0.9183\n",
      "Epoch 659/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9831 - f1_score: 0.9183 - val_loss: 2.6915 - val_accuracy: 0.7977 - val_f1_score: 0.9183\n",
      "Epoch 660/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9183 - val_loss: 2.6934 - val_accuracy: 0.7986 - val_f1_score: 0.9183\n",
      "Epoch 661/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9831 - f1_score: 0.9183 - val_loss: 2.7291 - val_accuracy: 0.7968 - val_f1_score: 0.9183\n",
      "Epoch 662/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0610 - accuracy: 0.9840 - f1_score: 0.9183 - val_loss: 2.7077 - val_accuracy: 0.7964 - val_f1_score: 0.9183\n",
      "Epoch 663/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9831 - f1_score: 0.9183 - val_loss: 2.7057 - val_accuracy: 0.7982 - val_f1_score: 0.9183\n",
      "Epoch 664/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9183 - val_loss: 2.7495 - val_accuracy: 0.7938 - val_f1_score: 0.9183\n",
      "Epoch 665/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9827 - f1_score: 0.9183 - val_loss: 2.7259 - val_accuracy: 0.7955 - val_f1_score: 0.9183\n",
      "Epoch 666/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9833 - f1_score: 0.9183 - val_loss: 2.7223 - val_accuracy: 0.7973 - val_f1_score: 0.9183\n",
      "Epoch 667/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9184 - val_loss: 2.7216 - val_accuracy: 0.7929 - val_f1_score: 0.9184\n",
      "Epoch 668/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9837 - f1_score: 0.9184 - val_loss: 2.7335 - val_accuracy: 0.7929 - val_f1_score: 0.9184\n",
      "Epoch 669/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9184 - val_loss: 2.7486 - val_accuracy: 0.7925 - val_f1_score: 0.9184\n",
      "Epoch 670/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9184 - val_loss: 2.7419 - val_accuracy: 0.7920 - val_f1_score: 0.9184\n",
      "Epoch 671/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9850 - f1_score: 0.9184 - val_loss: 2.7311 - val_accuracy: 0.7912 - val_f1_score: 0.9184\n",
      "Epoch 672/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9184 - val_loss: 2.7376 - val_accuracy: 0.7955 - val_f1_score: 0.9184\n",
      "Epoch 673/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9825 - f1_score: 0.9184 - val_loss: 2.7589 - val_accuracy: 0.7925 - val_f1_score: 0.9184\n",
      "Epoch 674/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9184 - val_loss: 2.7497 - val_accuracy: 0.7912 - val_f1_score: 0.9184\n",
      "Epoch 675/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9825 - f1_score: 0.9184 - val_loss: 2.7511 - val_accuracy: 0.7920 - val_f1_score: 0.9184\n",
      "Epoch 676/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9184 - val_loss: 2.7661 - val_accuracy: 0.7903 - val_f1_score: 0.9184\n",
      "Epoch 677/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9184 - val_loss: 2.7533 - val_accuracy: 0.7907 - val_f1_score: 0.9185\n",
      "Epoch 678/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9185 - val_loss: 2.7566 - val_accuracy: 0.7938 - val_f1_score: 0.9185\n",
      "Epoch 679/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9835 - f1_score: 0.9185 - val_loss: 2.7260 - val_accuracy: 0.7955 - val_f1_score: 0.9185\n",
      "Epoch 680/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0600 - accuracy: 0.9839 - f1_score: 0.9185 - val_loss: 2.7250 - val_accuracy: 0.7929 - val_f1_score: 0.9185\n",
      "Epoch 681/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9185 - val_loss: 2.7722 - val_accuracy: 0.7894 - val_f1_score: 0.9185\n",
      "Epoch 682/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9185 - val_loss: 2.7802 - val_accuracy: 0.7890 - val_f1_score: 0.9185\n",
      "Epoch 683/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9854 - f1_score: 0.9185 - val_loss: 2.7537 - val_accuracy: 0.7912 - val_f1_score: 0.9185\n",
      "Epoch 684/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9185 - val_loss: 2.7358 - val_accuracy: 0.7920 - val_f1_score: 0.9185\n",
      "Epoch 685/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9848 - f1_score: 0.9185 - val_loss: 2.7082 - val_accuracy: 0.7947 - val_f1_score: 0.9185\n",
      "Epoch 686/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9185 - val_loss: 2.7390 - val_accuracy: 0.7916 - val_f1_score: 0.9185\n",
      "Epoch 687/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9185 - val_loss: 2.7596 - val_accuracy: 0.7907 - val_f1_score: 0.9185\n",
      "Epoch 688/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9186 - val_loss: 2.7395 - val_accuracy: 0.7907 - val_f1_score: 0.9186\n",
      "Epoch 689/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0622 - accuracy: 0.9846 - f1_score: 0.9186 - val_loss: 2.7345 - val_accuracy: 0.7903 - val_f1_score: 0.9186\n",
      "Epoch 690/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9186 - val_loss: 2.7600 - val_accuracy: 0.7898 - val_f1_score: 0.9186\n",
      "Epoch 691/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0592 - accuracy: 0.9831 - f1_score: 0.9186 - val_loss: 2.7704 - val_accuracy: 0.7885 - val_f1_score: 0.9186\n",
      "Epoch 692/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9186 - val_loss: 2.7687 - val_accuracy: 0.7894 - val_f1_score: 0.9186\n",
      "Epoch 693/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9824 - f1_score: 0.9186 - val_loss: 2.7364 - val_accuracy: 0.7912 - val_f1_score: 0.9186\n",
      "Epoch 694/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9186 - val_loss: 2.7380 - val_accuracy: 0.7894 - val_f1_score: 0.9186\n",
      "Epoch 695/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9186 - val_loss: 2.7465 - val_accuracy: 0.7916 - val_f1_score: 0.9186\n",
      "Epoch 696/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9186 - val_loss: 2.7244 - val_accuracy: 0.7951 - val_f1_score: 0.9186\n",
      "Epoch 697/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9824 - f1_score: 0.9186 - val_loss: 2.7580 - val_accuracy: 0.7929 - val_f1_score: 0.9186\n",
      "Epoch 698/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9186 - val_loss: 2.7892 - val_accuracy: 0.7912 - val_f1_score: 0.9186\n",
      "Epoch 699/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9186 - val_loss: 2.7595 - val_accuracy: 0.7916 - val_f1_score: 0.9186\n",
      "Epoch 700/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9186 - val_loss: 2.7627 - val_accuracy: 0.7907 - val_f1_score: 0.9187\n",
      "Epoch 701/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9187 - val_loss: 2.7151 - val_accuracy: 0.7925 - val_f1_score: 0.9187\n",
      "Epoch 702/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9187 - val_loss: 2.7519 - val_accuracy: 0.7929 - val_f1_score: 0.9187\n",
      "Epoch 703/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9187 - val_loss: 2.7435 - val_accuracy: 0.7933 - val_f1_score: 0.9187\n",
      "Epoch 704/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9187 - val_loss: 2.7406 - val_accuracy: 0.7933 - val_f1_score: 0.9187\n",
      "Epoch 705/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9187 - val_loss: 2.7486 - val_accuracy: 0.7903 - val_f1_score: 0.9187\n",
      "Epoch 706/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9187 - val_loss: 2.7519 - val_accuracy: 0.7907 - val_f1_score: 0.9187\n",
      "Epoch 707/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9187 - val_loss: 2.7393 - val_accuracy: 0.7929 - val_f1_score: 0.9187\n",
      "Epoch 708/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9187 - val_loss: 2.7264 - val_accuracy: 0.7951 - val_f1_score: 0.9187\n",
      "Epoch 709/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9187 - val_loss: 2.7352 - val_accuracy: 0.7938 - val_f1_score: 0.9187\n",
      "Epoch 710/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9187 - val_loss: 2.7085 - val_accuracy: 0.7968 - val_f1_score: 0.9187\n",
      "Epoch 711/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9188 - val_loss: 2.7276 - val_accuracy: 0.7973 - val_f1_score: 0.9188\n",
      "Epoch 712/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9188 - val_loss: 2.7218 - val_accuracy: 0.7942 - val_f1_score: 0.9188\n",
      "Epoch 713/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9829 - f1_score: 0.9188 - val_loss: 2.7173 - val_accuracy: 0.7938 - val_f1_score: 0.9188\n",
      "Epoch 714/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9844 - f1_score: 0.9188 - val_loss: 2.7257 - val_accuracy: 0.7951 - val_f1_score: 0.9188\n",
      "Epoch 715/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9188 - val_loss: 2.7374 - val_accuracy: 0.7942 - val_f1_score: 0.9188\n",
      "Epoch 716/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0565 - accuracy: 0.9839 - f1_score: 0.9188 - val_loss: 2.7347 - val_accuracy: 0.7916 - val_f1_score: 0.9188\n",
      "Epoch 717/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9188 - val_loss: 2.7350 - val_accuracy: 0.7916 - val_f1_score: 0.9188\n",
      "Epoch 718/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9188 - val_loss: 2.7509 - val_accuracy: 0.7925 - val_f1_score: 0.9188\n",
      "Epoch 719/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9188 - val_loss: 2.7467 - val_accuracy: 0.7968 - val_f1_score: 0.9188\n",
      "Epoch 720/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9188 - val_loss: 2.7493 - val_accuracy: 0.7938 - val_f1_score: 0.9188\n",
      "Epoch 721/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9188 - val_loss: 2.7281 - val_accuracy: 0.7951 - val_f1_score: 0.9188\n",
      "Epoch 722/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9189 - val_loss: 2.7216 - val_accuracy: 0.7973 - val_f1_score: 0.9189\n",
      "Epoch 723/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9189 - val_loss: 2.6777 - val_accuracy: 0.7960 - val_f1_score: 0.9189\n",
      "Epoch 724/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9189 - val_loss: 2.6727 - val_accuracy: 0.7947 - val_f1_score: 0.9189\n",
      "Epoch 725/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0615 - accuracy: 0.9831 - f1_score: 0.9189 - val_loss: 2.6797 - val_accuracy: 0.7938 - val_f1_score: 0.9189\n",
      "Epoch 726/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9189 - val_loss: 2.6959 - val_accuracy: 0.7955 - val_f1_score: 0.9189\n",
      "Epoch 727/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9189 - val_loss: 2.7251 - val_accuracy: 0.7960 - val_f1_score: 0.9189\n",
      "Epoch 728/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9189 - val_loss: 2.7165 - val_accuracy: 0.7933 - val_f1_score: 0.9189\n",
      "Epoch 729/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9825 - f1_score: 0.9189 - val_loss: 2.7386 - val_accuracy: 0.7925 - val_f1_score: 0.9189\n",
      "Epoch 730/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9189 - val_loss: 2.7028 - val_accuracy: 0.7925 - val_f1_score: 0.9189\n",
      "Epoch 731/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9833 - f1_score: 0.9189 - val_loss: 2.6750 - val_accuracy: 0.7960 - val_f1_score: 0.9189\n",
      "Epoch 732/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9848 - f1_score: 0.9189 - val_loss: 2.6885 - val_accuracy: 0.7933 - val_f1_score: 0.9189\n",
      "Epoch 733/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0608 - accuracy: 0.9831 - f1_score: 0.9189 - val_loss: 2.6914 - val_accuracy: 0.7933 - val_f1_score: 0.9190\n",
      "Epoch 734/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9190 - val_loss: 2.6924 - val_accuracy: 0.7929 - val_f1_score: 0.9190\n",
      "Epoch 735/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9844 - f1_score: 0.9190 - val_loss: 2.7017 - val_accuracy: 0.7938 - val_f1_score: 0.9190\n",
      "Epoch 736/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9190 - val_loss: 2.7096 - val_accuracy: 0.7933 - val_f1_score: 0.9190\n",
      "Epoch 737/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9190 - val_loss: 2.7054 - val_accuracy: 0.7929 - val_f1_score: 0.9190\n",
      "Epoch 738/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9190 - val_loss: 2.7105 - val_accuracy: 0.7929 - val_f1_score: 0.9190\n",
      "Epoch 739/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9190 - val_loss: 2.7109 - val_accuracy: 0.7920 - val_f1_score: 0.9190\n",
      "Epoch 740/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9190 - val_loss: 2.7109 - val_accuracy: 0.7942 - val_f1_score: 0.9190\n",
      "Epoch 741/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9190 - val_loss: 2.6999 - val_accuracy: 0.7942 - val_f1_score: 0.9190\n",
      "Epoch 742/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9839 - f1_score: 0.9190 - val_loss: 2.7026 - val_accuracy: 0.7960 - val_f1_score: 0.9190\n",
      "Epoch 743/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9190 - val_loss: 2.7067 - val_accuracy: 0.7973 - val_f1_score: 0.9190\n",
      "Epoch 744/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9190 - val_loss: 2.7202 - val_accuracy: 0.7982 - val_f1_score: 0.9190\n",
      "Epoch 745/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9191 - val_loss: 2.7247 - val_accuracy: 0.7995 - val_f1_score: 0.9191\n",
      "Epoch 746/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9191 - val_loss: 2.7137 - val_accuracy: 0.7982 - val_f1_score: 0.9191\n",
      "Epoch 747/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9191 - val_loss: 2.7274 - val_accuracy: 0.7990 - val_f1_score: 0.9191\n",
      "Epoch 748/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9842 - f1_score: 0.9191 - val_loss: 2.7357 - val_accuracy: 0.7999 - val_f1_score: 0.9191\n",
      "Epoch 749/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9191 - val_loss: 2.7036 - val_accuracy: 0.7960 - val_f1_score: 0.9191\n",
      "Epoch 750/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9191 - val_loss: 2.7273 - val_accuracy: 0.7986 - val_f1_score: 0.9191\n",
      "Epoch 751/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9191 - val_loss: 2.7164 - val_accuracy: 0.7995 - val_f1_score: 0.9191\n",
      "Epoch 752/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0566 - accuracy: 0.9846 - f1_score: 0.9191 - val_loss: 2.7040 - val_accuracy: 0.7955 - val_f1_score: 0.9191\n",
      "Epoch 753/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9837 - f1_score: 0.9191 - val_loss: 2.7000 - val_accuracy: 0.7968 - val_f1_score: 0.9191\n",
      "Epoch 754/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9191 - val_loss: 2.7160 - val_accuracy: 0.7973 - val_f1_score: 0.9191\n",
      "Epoch 755/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9848 - f1_score: 0.9192 - val_loss: 2.7280 - val_accuracy: 0.7995 - val_f1_score: 0.9192\n",
      "Epoch 756/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9192 - val_loss: 2.7216 - val_accuracy: 0.7964 - val_f1_score: 0.9192\n",
      "Epoch 757/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9839 - f1_score: 0.9192 - val_loss: 2.7144 - val_accuracy: 0.7960 - val_f1_score: 0.9192\n",
      "Epoch 758/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9192 - val_loss: 2.7174 - val_accuracy: 0.7947 - val_f1_score: 0.9192\n",
      "Epoch 759/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9192 - val_loss: 2.7271 - val_accuracy: 0.7973 - val_f1_score: 0.9192\n",
      "Epoch 760/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9192 - val_loss: 2.7333 - val_accuracy: 0.7938 - val_f1_score: 0.9192\n",
      "Epoch 761/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9192 - val_loss: 2.7454 - val_accuracy: 0.7960 - val_f1_score: 0.9192\n",
      "Epoch 762/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9192 - val_loss: 2.7256 - val_accuracy: 0.7955 - val_f1_score: 0.9192\n",
      "Epoch 763/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9192 - val_loss: 2.7061 - val_accuracy: 0.7973 - val_f1_score: 0.9192\n",
      "Epoch 764/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9192 - val_loss: 2.7208 - val_accuracy: 0.7968 - val_f1_score: 0.9192\n",
      "Epoch 765/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9192 - val_loss: 2.7296 - val_accuracy: 0.7960 - val_f1_score: 0.9192\n",
      "Epoch 766/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0642 - accuracy: 0.9837 - f1_score: 0.9193 - val_loss: 2.7069 - val_accuracy: 0.7960 - val_f1_score: 0.9193\n",
      "Epoch 767/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9835 - f1_score: 0.9193 - val_loss: 2.7036 - val_accuracy: 0.7947 - val_f1_score: 0.9193\n",
      "Epoch 768/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9193 - val_loss: 2.7323 - val_accuracy: 0.7960 - val_f1_score: 0.9193\n",
      "Epoch 769/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9193 - val_loss: 2.6965 - val_accuracy: 0.7973 - val_f1_score: 0.9193\n",
      "Epoch 770/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9193 - val_loss: 2.6979 - val_accuracy: 0.7964 - val_f1_score: 0.9193\n",
      "Epoch 771/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9193 - val_loss: 2.7262 - val_accuracy: 0.7968 - val_f1_score: 0.9193\n",
      "Epoch 772/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9193 - val_loss: 2.7283 - val_accuracy: 0.7982 - val_f1_score: 0.9193\n",
      "Epoch 773/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9193 - val_loss: 2.7358 - val_accuracy: 0.7977 - val_f1_score: 0.9193\n",
      "Epoch 774/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9193 - val_loss: 2.7354 - val_accuracy: 0.7955 - val_f1_score: 0.9193\n",
      "Epoch 775/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9193 - val_loss: 2.7538 - val_accuracy: 0.7938 - val_f1_score: 0.9193\n",
      "Epoch 776/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9193 - val_loss: 2.7381 - val_accuracy: 0.7925 - val_f1_score: 0.9193\n",
      "Epoch 777/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9193 - val_loss: 2.7357 - val_accuracy: 0.7942 - val_f1_score: 0.9193\n",
      "Epoch 778/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9194 - val_loss: 2.7307 - val_accuracy: 0.7929 - val_f1_score: 0.9194\n",
      "Epoch 779/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9194 - val_loss: 2.7188 - val_accuracy: 0.7964 - val_f1_score: 0.9194\n",
      "Epoch 780/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9194 - val_loss: 2.7377 - val_accuracy: 0.7920 - val_f1_score: 0.9194\n",
      "Epoch 781/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9194 - val_loss: 2.7403 - val_accuracy: 0.7916 - val_f1_score: 0.9194\n",
      "Epoch 782/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9194 - val_loss: 2.7681 - val_accuracy: 0.7903 - val_f1_score: 0.9194\n",
      "Epoch 783/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9194 - val_loss: 2.7497 - val_accuracy: 0.7916 - val_f1_score: 0.9194\n",
      "Epoch 784/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9194 - val_loss: 2.7200 - val_accuracy: 0.7973 - val_f1_score: 0.9194\n",
      "Epoch 785/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9194 - val_loss: 2.7226 - val_accuracy: 0.7973 - val_f1_score: 0.9194\n",
      "Epoch 786/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0602 - accuracy: 0.9839 - f1_score: 0.9194 - val_loss: 2.7385 - val_accuracy: 0.7947 - val_f1_score: 0.9194\n",
      "Epoch 787/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9854 - f1_score: 0.9194 - val_loss: 2.7612 - val_accuracy: 0.7938 - val_f1_score: 0.9194\n",
      "Epoch 788/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9194 - val_loss: 2.7631 - val_accuracy: 0.7933 - val_f1_score: 0.9194\n",
      "Epoch 789/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9194 - val_loss: 2.7351 - val_accuracy: 0.7973 - val_f1_score: 0.9194\n",
      "Epoch 790/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9837 - f1_score: 0.9194 - val_loss: 2.7107 - val_accuracy: 0.7951 - val_f1_score: 0.9195\n",
      "Epoch 791/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9825 - f1_score: 0.9195 - val_loss: 2.7218 - val_accuracy: 0.7960 - val_f1_score: 0.9195\n",
      "Epoch 792/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9195 - val_loss: 2.7215 - val_accuracy: 0.7968 - val_f1_score: 0.9195\n",
      "Epoch 793/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9195 - val_loss: 2.7277 - val_accuracy: 0.7960 - val_f1_score: 0.9195\n",
      "Epoch 794/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9824 - f1_score: 0.9195 - val_loss: 2.7021 - val_accuracy: 0.7968 - val_f1_score: 0.9195\n",
      "Epoch 795/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9831 - f1_score: 0.9195 - val_loss: 2.7216 - val_accuracy: 0.7973 - val_f1_score: 0.9195\n",
      "Epoch 796/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9195 - val_loss: 2.7278 - val_accuracy: 0.7951 - val_f1_score: 0.9195\n",
      "Epoch 797/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9195 - val_loss: 2.6746 - val_accuracy: 0.7977 - val_f1_score: 0.9195\n",
      "Epoch 798/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9195 - val_loss: 2.6943 - val_accuracy: 0.7973 - val_f1_score: 0.9195\n",
      "Epoch 799/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9195 - val_loss: 2.7030 - val_accuracy: 0.7986 - val_f1_score: 0.9195\n",
      "Epoch 800/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9195 - val_loss: 2.7117 - val_accuracy: 0.7968 - val_f1_score: 0.9195\n",
      "Epoch 801/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9195 - val_loss: 2.7189 - val_accuracy: 0.7986 - val_f1_score: 0.9195\n",
      "Epoch 802/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9195 - val_loss: 2.7123 - val_accuracy: 0.7951 - val_f1_score: 0.9195\n",
      "Epoch 803/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9196 - val_loss: 2.6971 - val_accuracy: 0.7968 - val_f1_score: 0.9196\n",
      "Epoch 804/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9196 - val_loss: 2.6970 - val_accuracy: 0.7977 - val_f1_score: 0.9196\n",
      "Epoch 805/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9827 - f1_score: 0.9196 - val_loss: 2.6966 - val_accuracy: 0.8004 - val_f1_score: 0.9196\n",
      "Epoch 806/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0609 - accuracy: 0.9842 - f1_score: 0.9196 - val_loss: 2.6963 - val_accuracy: 0.8012 - val_f1_score: 0.9196\n",
      "Epoch 807/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0611 - accuracy: 0.9835 - f1_score: 0.9196 - val_loss: 2.6918 - val_accuracy: 0.8012 - val_f1_score: 0.9196\n",
      "Epoch 808/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9196 - val_loss: 2.6901 - val_accuracy: 0.8017 - val_f1_score: 0.9196\n",
      "Epoch 809/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0607 - accuracy: 0.9840 - f1_score: 0.9196 - val_loss: 2.6940 - val_accuracy: 0.8004 - val_f1_score: 0.9196\n",
      "Epoch 810/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0604 - accuracy: 0.9840 - f1_score: 0.9196 - val_loss: 2.7108 - val_accuracy: 0.7990 - val_f1_score: 0.9196\n",
      "Epoch 811/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9196 - val_loss: 2.6611 - val_accuracy: 0.8047 - val_f1_score: 0.9196\n",
      "Epoch 812/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0566 - accuracy: 0.9839 - f1_score: 0.9196 - val_loss: 2.6880 - val_accuracy: 0.7986 - val_f1_score: 0.9196\n",
      "Epoch 813/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9196 - val_loss: 2.6908 - val_accuracy: 0.7999 - val_f1_score: 0.9196\n",
      "Epoch 814/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0534 - accuracy: 0.9829 - f1_score: 0.9197 - val_loss: 2.6820 - val_accuracy: 0.7999 - val_f1_score: 0.9197\n",
      "Epoch 815/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9848 - f1_score: 0.9197 - val_loss: 2.6709 - val_accuracy: 0.8025 - val_f1_score: 0.9197\n",
      "Epoch 816/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9825 - f1_score: 0.9197 - val_loss: 2.6984 - val_accuracy: 0.8017 - val_f1_score: 0.9197\n",
      "Epoch 817/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9197 - val_loss: 2.7131 - val_accuracy: 0.7999 - val_f1_score: 0.9197\n",
      "Epoch 818/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9197 - val_loss: 2.7155 - val_accuracy: 0.7999 - val_f1_score: 0.9197\n",
      "Epoch 819/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9844 - f1_score: 0.9197 - val_loss: 2.7192 - val_accuracy: 0.7999 - val_f1_score: 0.9197\n",
      "Epoch 820/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0610 - accuracy: 0.9840 - f1_score: 0.9197 - val_loss: 2.7107 - val_accuracy: 0.8030 - val_f1_score: 0.9197\n",
      "Epoch 821/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9854 - f1_score: 0.9197 - val_loss: 2.7329 - val_accuracy: 0.8004 - val_f1_score: 0.9197\n",
      "Epoch 822/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9197 - val_loss: 2.7171 - val_accuracy: 0.7999 - val_f1_score: 0.9197\n",
      "Epoch 823/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9197 - val_loss: 2.7158 - val_accuracy: 0.7986 - val_f1_score: 0.9197\n",
      "Epoch 824/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9197 - val_loss: 2.7212 - val_accuracy: 0.7990 - val_f1_score: 0.9197\n",
      "Epoch 825/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9824 - f1_score: 0.9198 - val_loss: 2.7230 - val_accuracy: 0.7986 - val_f1_score: 0.9198\n",
      "Epoch 826/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9825 - f1_score: 0.9198 - val_loss: 2.7188 - val_accuracy: 0.7964 - val_f1_score: 0.9198\n",
      "Epoch 827/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9198 - val_loss: 2.7278 - val_accuracy: 0.7938 - val_f1_score: 0.9198\n",
      "Epoch 828/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9198 - val_loss: 2.7073 - val_accuracy: 0.7977 - val_f1_score: 0.9198\n",
      "Epoch 829/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9198 - val_loss: 2.7025 - val_accuracy: 0.7964 - val_f1_score: 0.9198\n",
      "Epoch 830/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9837 - f1_score: 0.9198 - val_loss: 2.7077 - val_accuracy: 0.7999 - val_f1_score: 0.9198\n",
      "Epoch 831/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9198 - val_loss: 2.6896 - val_accuracy: 0.8008 - val_f1_score: 0.9198\n",
      "Epoch 832/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9198 - val_loss: 2.6962 - val_accuracy: 0.7999 - val_f1_score: 0.9198\n",
      "Epoch 833/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9198 - val_loss: 2.7206 - val_accuracy: 0.7960 - val_f1_score: 0.9198\n",
      "Epoch 834/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9198 - val_loss: 2.7210 - val_accuracy: 0.7955 - val_f1_score: 0.9198\n",
      "Epoch 835/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9198 - val_loss: 2.7339 - val_accuracy: 0.7960 - val_f1_score: 0.9198\n",
      "Epoch 836/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9829 - f1_score: 0.9198 - val_loss: 2.7338 - val_accuracy: 0.7955 - val_f1_score: 0.9198\n",
      "Epoch 837/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9198 - val_loss: 2.7191 - val_accuracy: 0.7951 - val_f1_score: 0.9198\n",
      "Epoch 838/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0604 - accuracy: 0.9835 - f1_score: 0.9198 - val_loss: 2.7129 - val_accuracy: 0.7982 - val_f1_score: 0.9199\n",
      "Epoch 839/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9199 - val_loss: 2.7096 - val_accuracy: 0.7960 - val_f1_score: 0.9199\n",
      "Epoch 840/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9837 - f1_score: 0.9199 - val_loss: 2.7134 - val_accuracy: 0.7986 - val_f1_score: 0.9199\n",
      "Epoch 841/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9199 - val_loss: 2.7109 - val_accuracy: 0.7999 - val_f1_score: 0.9199\n",
      "Epoch 842/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9199 - val_loss: 2.7128 - val_accuracy: 0.7960 - val_f1_score: 0.9199\n",
      "Epoch 843/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9199 - val_loss: 2.7174 - val_accuracy: 0.7942 - val_f1_score: 0.9199\n",
      "Epoch 844/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9837 - f1_score: 0.9199 - val_loss: 2.7216 - val_accuracy: 0.7933 - val_f1_score: 0.9199\n",
      "Epoch 845/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9199 - val_loss: 2.7402 - val_accuracy: 0.7942 - val_f1_score: 0.9199\n",
      "Epoch 846/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0609 - accuracy: 0.9835 - f1_score: 0.9199 - val_loss: 2.7426 - val_accuracy: 0.7951 - val_f1_score: 0.9199\n",
      "Epoch 847/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9837 - f1_score: 0.9199 - val_loss: 2.7369 - val_accuracy: 0.7942 - val_f1_score: 0.9199\n",
      "Epoch 848/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0576 - accuracy: 0.9846 - f1_score: 0.9199 - val_loss: 2.7366 - val_accuracy: 0.7960 - val_f1_score: 0.9199\n",
      "Epoch 849/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0567 - accuracy: 0.9831 - f1_score: 0.9199 - val_loss: 2.7326 - val_accuracy: 0.7938 - val_f1_score: 0.9199\n",
      "Epoch 850/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9199 - val_loss: 2.7315 - val_accuracy: 0.7925 - val_f1_score: 0.9199\n",
      "Epoch 851/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0576 - accuracy: 0.9842 - f1_score: 0.9199 - val_loss: 2.7454 - val_accuracy: 0.7920 - val_f1_score: 0.9199\n",
      "Epoch 852/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9833 - f1_score: 0.9199 - val_loss: 2.7320 - val_accuracy: 0.7925 - val_f1_score: 0.9199\n",
      "Epoch 853/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9200 - val_loss: 2.7202 - val_accuracy: 0.7947 - val_f1_score: 0.9200\n",
      "Epoch 854/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9200 - val_loss: 2.7167 - val_accuracy: 0.7968 - val_f1_score: 0.9200\n",
      "Epoch 855/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9200 - val_loss: 2.7281 - val_accuracy: 0.7942 - val_f1_score: 0.9200\n",
      "Epoch 856/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9200 - val_loss: 2.7185 - val_accuracy: 0.7973 - val_f1_score: 0.9200\n",
      "Epoch 857/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9855 - f1_score: 0.9200 - val_loss: 2.7268 - val_accuracy: 0.7925 - val_f1_score: 0.9200\n",
      "Epoch 858/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9200 - val_loss: 2.7200 - val_accuracy: 0.7916 - val_f1_score: 0.9200\n",
      "Epoch 859/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9200 - val_loss: 2.7250 - val_accuracy: 0.7933 - val_f1_score: 0.9200\n",
      "Epoch 860/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9200 - val_loss: 2.7264 - val_accuracy: 0.7951 - val_f1_score: 0.9200\n",
      "Epoch 861/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9200 - val_loss: 2.7158 - val_accuracy: 0.8012 - val_f1_score: 0.9200\n",
      "Epoch 862/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9200 - val_loss: 2.7163 - val_accuracy: 0.8012 - val_f1_score: 0.9200\n",
      "Epoch 863/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9200 - val_loss: 2.7252 - val_accuracy: 0.8012 - val_f1_score: 0.9200\n",
      "Epoch 864/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9200 - val_loss: 2.7199 - val_accuracy: 0.7964 - val_f1_score: 0.9200\n",
      "Epoch 865/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9200 - val_loss: 2.7300 - val_accuracy: 0.7964 - val_f1_score: 0.9200\n",
      "Epoch 866/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9200 - val_loss: 2.7347 - val_accuracy: 0.7960 - val_f1_score: 0.9200\n",
      "Epoch 867/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9201 - val_loss: 2.7378 - val_accuracy: 0.7973 - val_f1_score: 0.9201\n",
      "Epoch 868/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9201 - val_loss: 2.7386 - val_accuracy: 0.7960 - val_f1_score: 0.9201\n",
      "Epoch 869/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9201 - val_loss: 2.7403 - val_accuracy: 0.7999 - val_f1_score: 0.9201\n",
      "Epoch 870/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9201 - val_loss: 2.7381 - val_accuracy: 0.7999 - val_f1_score: 0.9201\n",
      "Epoch 871/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9201 - val_loss: 2.7347 - val_accuracy: 0.7977 - val_f1_score: 0.9201\n",
      "Epoch 872/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9201 - val_loss: 2.7343 - val_accuracy: 0.7964 - val_f1_score: 0.9201\n",
      "Epoch 873/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9829 - f1_score: 0.9201 - val_loss: 2.7238 - val_accuracy: 0.7951 - val_f1_score: 0.9201\n",
      "Epoch 874/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9201 - val_loss: 2.7261 - val_accuracy: 0.7951 - val_f1_score: 0.9201\n",
      "Epoch 875/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0640 - accuracy: 0.9846 - f1_score: 0.9201 - val_loss: 2.7278 - val_accuracy: 0.7955 - val_f1_score: 0.9201\n",
      "Epoch 876/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9201 - val_loss: 2.7297 - val_accuracy: 0.7977 - val_f1_score: 0.9201\n",
      "Epoch 877/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9201 - val_loss: 2.7254 - val_accuracy: 0.7977 - val_f1_score: 0.9201\n",
      "Epoch 878/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9201 - val_loss: 2.7208 - val_accuracy: 0.7977 - val_f1_score: 0.9201\n",
      "Epoch 879/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9201 - val_loss: 2.7378 - val_accuracy: 0.7982 - val_f1_score: 0.9201\n",
      "Epoch 880/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9202 - val_loss: 2.7359 - val_accuracy: 0.7986 - val_f1_score: 0.9202\n",
      "Epoch 881/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9202 - val_loss: 2.7442 - val_accuracy: 0.7968 - val_f1_score: 0.9202\n",
      "Epoch 882/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9202 - val_loss: 2.7473 - val_accuracy: 0.7938 - val_f1_score: 0.9202\n",
      "Epoch 883/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9202 - val_loss: 2.7583 - val_accuracy: 0.7933 - val_f1_score: 0.9202\n",
      "Epoch 884/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0573 - accuracy: 0.9842 - f1_score: 0.9202 - val_loss: 2.7595 - val_accuracy: 0.7929 - val_f1_score: 0.9202\n",
      "Epoch 885/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9202 - val_loss: 2.7595 - val_accuracy: 0.7912 - val_f1_score: 0.9202\n",
      "Epoch 886/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9202 - val_loss: 2.7528 - val_accuracy: 0.7933 - val_f1_score: 0.9202\n",
      "Epoch 887/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9202 - val_loss: 2.7362 - val_accuracy: 0.7951 - val_f1_score: 0.9202\n",
      "Epoch 888/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9202 - val_loss: 2.7403 - val_accuracy: 0.7964 - val_f1_score: 0.9202\n",
      "Epoch 889/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9202 - val_loss: 2.7340 - val_accuracy: 0.7955 - val_f1_score: 0.9202\n",
      "Epoch 890/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9202 - val_loss: 2.7556 - val_accuracy: 0.7951 - val_f1_score: 0.9202\n",
      "Epoch 891/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9202 - val_loss: 2.7357 - val_accuracy: 0.7951 - val_f1_score: 0.9202\n",
      "Epoch 892/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9202 - val_loss: 2.7416 - val_accuracy: 0.7955 - val_f1_score: 0.9202\n",
      "Epoch 893/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9202 - val_loss: 2.7466 - val_accuracy: 0.7960 - val_f1_score: 0.9202\n",
      "Epoch 894/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9202 - val_loss: 2.7531 - val_accuracy: 0.7964 - val_f1_score: 0.9202\n",
      "Epoch 895/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9202 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9202\n",
      "Epoch 896/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9839 - f1_score: 0.9203 - val_loss: 2.7495 - val_accuracy: 0.7968 - val_f1_score: 0.9203\n",
      "Epoch 897/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9203 - val_loss: 2.7554 - val_accuracy: 0.7964 - val_f1_score: 0.9203\n",
      "Epoch 898/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9203 - val_loss: 2.7547 - val_accuracy: 0.7942 - val_f1_score: 0.9203\n",
      "Epoch 899/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9848 - f1_score: 0.9203 - val_loss: 2.7392 - val_accuracy: 0.7968 - val_f1_score: 0.9203\n",
      "Epoch 900/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9203 - val_loss: 2.7403 - val_accuracy: 0.7968 - val_f1_score: 0.9203\n",
      "Epoch 901/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9203 - val_loss: 2.7497 - val_accuracy: 0.7960 - val_f1_score: 0.9203\n",
      "Epoch 902/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9203 - val_loss: 2.7415 - val_accuracy: 0.7951 - val_f1_score: 0.9203\n",
      "Epoch 903/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9827 - f1_score: 0.9203 - val_loss: 2.7530 - val_accuracy: 0.7947 - val_f1_score: 0.9203\n",
      "Epoch 904/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9203 - val_loss: 2.7585 - val_accuracy: 0.7951 - val_f1_score: 0.9203\n",
      "Epoch 905/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9203 - val_loss: 2.7435 - val_accuracy: 0.7947 - val_f1_score: 0.9203\n",
      "Epoch 906/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9825 - f1_score: 0.9203 - val_loss: 2.7458 - val_accuracy: 0.7951 - val_f1_score: 0.9203\n",
      "Epoch 907/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9203 - val_loss: 2.7527 - val_accuracy: 0.7968 - val_f1_score: 0.9203\n",
      "Epoch 908/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0605 - accuracy: 0.9833 - f1_score: 0.9203 - val_loss: 2.7593 - val_accuracy: 0.7960 - val_f1_score: 0.9203\n",
      "Epoch 909/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9203 - val_loss: 2.7707 - val_accuracy: 0.7938 - val_f1_score: 0.9203\n",
      "Epoch 910/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9203 - val_loss: 2.7742 - val_accuracy: 0.7925 - val_f1_score: 0.9203\n",
      "Epoch 911/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9203 - val_loss: 2.7675 - val_accuracy: 0.7925 - val_f1_score: 0.9203\n",
      "Epoch 912/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0561 - accuracy: 0.9850 - f1_score: 0.9203 - val_loss: 2.7571 - val_accuracy: 0.7925 - val_f1_score: 0.9203\n",
      "Epoch 913/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9203 - val_loss: 2.7538 - val_accuracy: 0.7925 - val_f1_score: 0.9204\n",
      "Epoch 914/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9204 - val_loss: 2.7554 - val_accuracy: 0.7938 - val_f1_score: 0.9204\n",
      "Epoch 915/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9204 - val_loss: 2.7336 - val_accuracy: 0.7960 - val_f1_score: 0.9204\n",
      "Epoch 916/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9833 - f1_score: 0.9204 - val_loss: 2.7299 - val_accuracy: 0.7968 - val_f1_score: 0.9204\n",
      "Epoch 917/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9204 - val_loss: 2.7109 - val_accuracy: 0.7977 - val_f1_score: 0.9204\n",
      "Epoch 918/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9825 - f1_score: 0.9204 - val_loss: 2.7178 - val_accuracy: 0.7968 - val_f1_score: 0.9204\n",
      "Epoch 919/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9204 - val_loss: 2.7261 - val_accuracy: 0.7964 - val_f1_score: 0.9204\n",
      "Epoch 920/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9204 - val_loss: 2.7421 - val_accuracy: 0.7951 - val_f1_score: 0.9204\n",
      "Epoch 921/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9204 - val_loss: 2.7426 - val_accuracy: 0.7955 - val_f1_score: 0.9204\n",
      "Epoch 922/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9204 - val_loss: 2.7631 - val_accuracy: 0.7942 - val_f1_score: 0.9204\n",
      "Epoch 923/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9204 - val_loss: 2.7353 - val_accuracy: 0.7960 - val_f1_score: 0.9204\n",
      "Epoch 924/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9204 - val_loss: 2.7448 - val_accuracy: 0.7960 - val_f1_score: 0.9204\n",
      "Epoch 925/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9204 - val_loss: 2.7330 - val_accuracy: 0.7968 - val_f1_score: 0.9204\n",
      "Epoch 926/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9852 - f1_score: 0.9204 - val_loss: 2.7311 - val_accuracy: 0.7977 - val_f1_score: 0.9204\n",
      "Epoch 927/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9204 - val_loss: 2.7308 - val_accuracy: 0.7964 - val_f1_score: 0.9204\n",
      "Epoch 928/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9204 - val_loss: 2.7498 - val_accuracy: 0.7960 - val_f1_score: 0.9204\n",
      "Epoch 929/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9857 - f1_score: 0.9204 - val_loss: 2.7516 - val_accuracy: 0.7964 - val_f1_score: 0.9204\n",
      "Epoch 930/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9205 - val_loss: 2.7528 - val_accuracy: 0.7964 - val_f1_score: 0.9205\n",
      "Epoch 931/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9205 - val_loss: 2.7483 - val_accuracy: 0.7964 - val_f1_score: 0.9205\n",
      "Epoch 932/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9205 - val_loss: 2.7488 - val_accuracy: 0.7955 - val_f1_score: 0.9205\n",
      "Epoch 933/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9205 - val_loss: 2.7494 - val_accuracy: 0.7955 - val_f1_score: 0.9205\n",
      "Epoch 934/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9205 - val_loss: 2.7535 - val_accuracy: 0.7955 - val_f1_score: 0.9205\n",
      "Epoch 935/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9205 - val_loss: 2.7449 - val_accuracy: 0.7955 - val_f1_score: 0.9205\n",
      "Epoch 936/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9205 - val_loss: 2.7478 - val_accuracy: 0.7960 - val_f1_score: 0.9205\n",
      "Epoch 937/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9833 - f1_score: 0.9205 - val_loss: 2.7584 - val_accuracy: 0.7951 - val_f1_score: 0.9205\n",
      "Epoch 938/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9205 - val_loss: 2.7649 - val_accuracy: 0.7942 - val_f1_score: 0.9205\n",
      "Epoch 939/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9205 - val_loss: 2.7654 - val_accuracy: 0.7933 - val_f1_score: 0.9205\n",
      "Epoch 940/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9205 - val_loss: 2.7727 - val_accuracy: 0.7942 - val_f1_score: 0.9205\n",
      "Epoch 941/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9205 - val_loss: 2.7695 - val_accuracy: 0.7938 - val_f1_score: 0.9205\n",
      "Epoch 942/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9205 - val_loss: 2.7531 - val_accuracy: 0.7977 - val_f1_score: 0.9205\n",
      "Epoch 943/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9824 - f1_score: 0.9205 - val_loss: 2.7587 - val_accuracy: 0.7964 - val_f1_score: 0.9205\n",
      "Epoch 944/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0608 - accuracy: 0.9855 - f1_score: 0.9205 - val_loss: 2.7494 - val_accuracy: 0.7964 - val_f1_score: 0.9205\n",
      "Epoch 945/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9840 - f1_score: 0.9205 - val_loss: 2.7454 - val_accuracy: 0.7968 - val_f1_score: 0.9205\n",
      "Epoch 946/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9205 - val_loss: 2.7458 - val_accuracy: 0.7960 - val_f1_score: 0.9205\n",
      "Epoch 947/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9205 - val_loss: 2.7414 - val_accuracy: 0.7964 - val_f1_score: 0.9206\n",
      "Epoch 948/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9825 - f1_score: 0.9206 - val_loss: 2.7496 - val_accuracy: 0.7973 - val_f1_score: 0.9206\n",
      "Epoch 949/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9206 - val_loss: 2.7450 - val_accuracy: 0.7960 - val_f1_score: 0.9206\n",
      "Epoch 950/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9206 - val_loss: 2.7365 - val_accuracy: 0.7968 - val_f1_score: 0.9206\n",
      "Epoch 951/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9206 - val_loss: 2.7327 - val_accuracy: 0.7964 - val_f1_score: 0.9206\n",
      "Epoch 952/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9206 - val_loss: 2.7290 - val_accuracy: 0.7977 - val_f1_score: 0.9206\n",
      "Epoch 953/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9206 - val_loss: 2.7322 - val_accuracy: 0.7973 - val_f1_score: 0.9206\n",
      "Epoch 954/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9206 - val_loss: 2.7271 - val_accuracy: 0.7977 - val_f1_score: 0.9206\n",
      "Epoch 955/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9206 - val_loss: 2.7046 - val_accuracy: 0.8004 - val_f1_score: 0.9206\n",
      "Epoch 956/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9206 - val_loss: 2.7066 - val_accuracy: 0.7999 - val_f1_score: 0.9206\n",
      "Epoch 957/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9206 - val_loss: 2.6920 - val_accuracy: 0.7999 - val_f1_score: 0.9206\n",
      "Epoch 958/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9206 - val_loss: 2.6673 - val_accuracy: 0.8012 - val_f1_score: 0.9206\n",
      "Epoch 959/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9822 - f1_score: 0.9206 - val_loss: 2.7091 - val_accuracy: 0.7995 - val_f1_score: 0.9206\n",
      "Epoch 960/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9206 - val_loss: 2.7217 - val_accuracy: 0.7977 - val_f1_score: 0.9206\n",
      "Epoch 961/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9206 - val_loss: 2.7380 - val_accuracy: 0.7982 - val_f1_score: 0.9206\n",
      "Epoch 962/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9206 - val_loss: 2.7358 - val_accuracy: 0.7968 - val_f1_score: 0.9206\n",
      "Epoch 963/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9842 - f1_score: 0.9206 - val_loss: 2.7320 - val_accuracy: 0.7964 - val_f1_score: 0.9207\n",
      "Epoch 964/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0610 - accuracy: 0.9839 - f1_score: 0.9207 - val_loss: 2.7329 - val_accuracy: 0.7964 - val_f1_score: 0.9207\n",
      "Epoch 965/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9207 - val_loss: 2.7341 - val_accuracy: 0.7973 - val_f1_score: 0.9207\n",
      "Epoch 966/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9207 - val_loss: 2.7381 - val_accuracy: 0.7973 - val_f1_score: 0.9207\n",
      "Epoch 967/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9207 - val_loss: 2.7332 - val_accuracy: 0.7977 - val_f1_score: 0.9207\n",
      "Epoch 968/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9846 - f1_score: 0.9207 - val_loss: 2.7287 - val_accuracy: 0.7968 - val_f1_score: 0.9207\n",
      "Epoch 969/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9207 - val_loss: 2.7326 - val_accuracy: 0.7982 - val_f1_score: 0.9207\n",
      "Epoch 970/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9855 - f1_score: 0.9207 - val_loss: 2.7316 - val_accuracy: 0.7990 - val_f1_score: 0.9207\n",
      "Epoch 971/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9207 - val_loss: 2.7241 - val_accuracy: 0.7982 - val_f1_score: 0.9207\n",
      "Epoch 972/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9207 - val_loss: 2.7337 - val_accuracy: 0.7982 - val_f1_score: 0.9207\n",
      "Epoch 973/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9207 - val_loss: 2.7417 - val_accuracy: 0.7973 - val_f1_score: 0.9207\n",
      "Epoch 974/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9207 - val_loss: 2.7470 - val_accuracy: 0.7977 - val_f1_score: 0.9207\n",
      "Epoch 975/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9207 - val_loss: 2.7440 - val_accuracy: 0.7951 - val_f1_score: 0.9207\n",
      "Epoch 976/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9207 - val_loss: 2.7423 - val_accuracy: 0.7964 - val_f1_score: 0.9207\n",
      "Epoch 977/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9207 - val_loss: 2.7424 - val_accuracy: 0.7964 - val_f1_score: 0.9207\n",
      "Epoch 978/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9207 - val_loss: 2.7465 - val_accuracy: 0.7986 - val_f1_score: 0.9207\n",
      "Epoch 979/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9859 - f1_score: 0.9207 - val_loss: 2.7502 - val_accuracy: 0.7977 - val_f1_score: 0.9207\n",
      "Epoch 980/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9833 - f1_score: 0.9207 - val_loss: 2.7448 - val_accuracy: 0.7982 - val_f1_score: 0.9208\n",
      "Epoch 981/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9208 - val_loss: 2.7486 - val_accuracy: 0.7982 - val_f1_score: 0.9208\n",
      "Epoch 982/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9208 - val_loss: 2.7252 - val_accuracy: 0.7986 - val_f1_score: 0.9208\n",
      "Epoch 983/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9208 - val_loss: 2.7184 - val_accuracy: 0.7990 - val_f1_score: 0.9208\n",
      "Epoch 984/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0616 - accuracy: 0.9844 - f1_score: 0.9208 - val_loss: 2.7076 - val_accuracy: 0.7977 - val_f1_score: 0.9208\n",
      "Epoch 985/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9208 - val_loss: 2.7250 - val_accuracy: 0.7990 - val_f1_score: 0.9208\n",
      "Epoch 986/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9208 - val_loss: 2.7251 - val_accuracy: 0.7990 - val_f1_score: 0.9208\n",
      "Epoch 987/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9208 - val_loss: 2.7386 - val_accuracy: 0.7977 - val_f1_score: 0.9208\n",
      "Epoch 988/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9208 - val_loss: 2.7504 - val_accuracy: 0.7990 - val_f1_score: 0.9208\n",
      "Epoch 989/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9208 - val_loss: 2.7498 - val_accuracy: 0.7977 - val_f1_score: 0.9208\n",
      "Epoch 990/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9208 - val_loss: 2.7454 - val_accuracy: 0.7990 - val_f1_score: 0.9208\n",
      "Epoch 991/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9208 - val_loss: 2.7348 - val_accuracy: 0.7986 - val_f1_score: 0.9208\n",
      "Epoch 992/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9208 - val_loss: 2.7429 - val_accuracy: 0.7982 - val_f1_score: 0.9208\n",
      "Epoch 993/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9208 - val_loss: 2.7472 - val_accuracy: 0.8004 - val_f1_score: 0.9208\n",
      "Epoch 994/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9208 - val_loss: 2.7560 - val_accuracy: 0.8004 - val_f1_score: 0.9208\n",
      "Epoch 995/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9208 - val_loss: 2.7537 - val_accuracy: 0.7986 - val_f1_score: 0.9208\n",
      "Epoch 996/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9208 - val_loss: 2.7507 - val_accuracy: 0.7977 - val_f1_score: 0.9208\n",
      "Epoch 997/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9209 - val_loss: 2.7541 - val_accuracy: 0.7977 - val_f1_score: 0.9209\n",
      "Epoch 998/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0630 - accuracy: 0.9840 - f1_score: 0.9209 - val_loss: 2.7515 - val_accuracy: 0.7973 - val_f1_score: 0.9209\n",
      "Epoch 999/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9209 - val_loss: 2.7499 - val_accuracy: 0.7968 - val_f1_score: 0.9209\n",
      "Epoch 1000/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9209 - val_loss: 2.7472 - val_accuracy: 0.7968 - val_f1_score: 0.9209\n",
      "Epoch 1001/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9209 - val_loss: 2.7542 - val_accuracy: 0.7977 - val_f1_score: 0.9209\n",
      "Epoch 1002/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9209 - val_loss: 2.7586 - val_accuracy: 0.7973 - val_f1_score: 0.9209\n",
      "Epoch 1003/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9209 - val_loss: 2.7598 - val_accuracy: 0.7982 - val_f1_score: 0.9209\n",
      "Epoch 1004/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9824 - f1_score: 0.9209 - val_loss: 2.7623 - val_accuracy: 0.7982 - val_f1_score: 0.9209\n",
      "Epoch 1005/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9209 - val_loss: 2.7548 - val_accuracy: 0.7986 - val_f1_score: 0.9209\n",
      "Epoch 1006/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9209 - val_loss: 2.7315 - val_accuracy: 0.7968 - val_f1_score: 0.9209\n",
      "Epoch 1007/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9842 - f1_score: 0.9209 - val_loss: 2.7363 - val_accuracy: 0.7982 - val_f1_score: 0.9209\n",
      "Epoch 1008/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9209 - val_loss: 2.7421 - val_accuracy: 0.7977 - val_f1_score: 0.9209\n",
      "Epoch 1009/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9209 - val_loss: 2.7371 - val_accuracy: 0.7986 - val_f1_score: 0.9209\n",
      "Epoch 1010/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9209 - val_loss: 2.7339 - val_accuracy: 0.7982 - val_f1_score: 0.9209\n",
      "Epoch 1011/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0572 - accuracy: 0.9825 - f1_score: 0.9209 - val_loss: 2.7414 - val_accuracy: 0.7982 - val_f1_score: 0.9209\n",
      "Epoch 1012/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9209 - val_loss: 2.7318 - val_accuracy: 0.7977 - val_f1_score: 0.9209\n",
      "Epoch 1013/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9209 - val_loss: 2.7491 - val_accuracy: 0.7973 - val_f1_score: 0.9209\n",
      "Epoch 1014/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9827 - f1_score: 0.9209 - val_loss: 2.7483 - val_accuracy: 0.7977 - val_f1_score: 0.9209\n",
      "Epoch 1015/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0527 - accuracy: 0.9835 - f1_score: 0.9209 - val_loss: 2.7527 - val_accuracy: 0.7973 - val_f1_score: 0.9210\n",
      "Epoch 1016/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9859 - f1_score: 0.9210 - val_loss: 2.7531 - val_accuracy: 0.7977 - val_f1_score: 0.9210\n",
      "Epoch 1017/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9822 - f1_score: 0.9210 - val_loss: 2.7522 - val_accuracy: 0.7968 - val_f1_score: 0.9210\n",
      "Epoch 1018/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9210 - val_loss: 2.7499 - val_accuracy: 0.7973 - val_f1_score: 0.9210\n",
      "Epoch 1019/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9210 - val_loss: 2.7462 - val_accuracy: 0.7964 - val_f1_score: 0.9210\n",
      "Epoch 1020/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9210 - val_loss: 2.7478 - val_accuracy: 0.7999 - val_f1_score: 0.9210\n",
      "Epoch 1021/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9210 - val_loss: 2.7569 - val_accuracy: 0.7977 - val_f1_score: 0.9210\n",
      "Epoch 1022/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0612 - accuracy: 0.9842 - f1_score: 0.9210 - val_loss: 2.7506 - val_accuracy: 0.7982 - val_f1_score: 0.9210\n",
      "Epoch 1023/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9210 - val_loss: 2.7529 - val_accuracy: 0.7982 - val_f1_score: 0.9210\n",
      "Epoch 1024/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9210 - val_loss: 2.7472 - val_accuracy: 0.7960 - val_f1_score: 0.9210\n",
      "Epoch 1025/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9210 - val_loss: 2.7478 - val_accuracy: 0.7977 - val_f1_score: 0.9210\n",
      "Epoch 1026/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9210 - val_loss: 2.7537 - val_accuracy: 0.7977 - val_f1_score: 0.9210\n",
      "Epoch 1027/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9210 - val_loss: 2.7516 - val_accuracy: 0.7977 - val_f1_score: 0.9210\n",
      "Epoch 1028/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9839 - f1_score: 0.9210 - val_loss: 2.7426 - val_accuracy: 0.7995 - val_f1_score: 0.9210\n",
      "Epoch 1029/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9210 - val_loss: 2.7416 - val_accuracy: 0.8008 - val_f1_score: 0.9210\n",
      "Epoch 1030/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9210 - val_loss: 2.7427 - val_accuracy: 0.7995 - val_f1_score: 0.9210\n",
      "Epoch 1031/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0569 - accuracy: 0.9848 - f1_score: 0.9210 - val_loss: 2.7498 - val_accuracy: 0.8008 - val_f1_score: 0.9210\n",
      "Epoch 1032/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0606 - accuracy: 0.9837 - f1_score: 0.9210 - val_loss: 2.7472 - val_accuracy: 0.8021 - val_f1_score: 0.9210\n",
      "Epoch 1033/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9827 - f1_score: 0.9210 - val_loss: 2.7372 - val_accuracy: 0.8012 - val_f1_score: 0.9211\n",
      "Epoch 1034/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9211 - val_loss: 2.7340 - val_accuracy: 0.7999 - val_f1_score: 0.9211\n",
      "Epoch 1035/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9211 - val_loss: 2.7351 - val_accuracy: 0.8004 - val_f1_score: 0.9211\n",
      "Epoch 1036/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9211 - val_loss: 2.7404 - val_accuracy: 0.8017 - val_f1_score: 0.9211\n",
      "Epoch 1037/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9211 - val_loss: 2.7407 - val_accuracy: 0.7995 - val_f1_score: 0.9211\n",
      "Epoch 1038/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9211 - val_loss: 2.7427 - val_accuracy: 0.7999 - val_f1_score: 0.9211\n",
      "Epoch 1039/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9211 - val_loss: 2.7437 - val_accuracy: 0.7973 - val_f1_score: 0.9211\n",
      "Epoch 1040/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9211 - val_loss: 2.7473 - val_accuracy: 0.7990 - val_f1_score: 0.9211\n",
      "Epoch 1041/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0655 - accuracy: 0.9839 - f1_score: 0.9211 - val_loss: 2.7417 - val_accuracy: 0.7986 - val_f1_score: 0.9211\n",
      "Epoch 1042/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9211 - val_loss: 2.7455 - val_accuracy: 0.7995 - val_f1_score: 0.9211\n",
      "Epoch 1043/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0607 - accuracy: 0.9833 - f1_score: 0.9211 - val_loss: 2.7424 - val_accuracy: 0.8004 - val_f1_score: 0.9211\n",
      "Epoch 1044/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9211 - val_loss: 2.7368 - val_accuracy: 0.7995 - val_f1_score: 0.9211\n",
      "Epoch 1045/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9211 - val_loss: 2.7343 - val_accuracy: 0.7995 - val_f1_score: 0.9211\n",
      "Epoch 1046/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9211 - val_loss: 2.7309 - val_accuracy: 0.7999 - val_f1_score: 0.9211\n",
      "Epoch 1047/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9211 - val_loss: 2.7303 - val_accuracy: 0.7986 - val_f1_score: 0.9211\n",
      "Epoch 1048/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9211 - val_loss: 2.7255 - val_accuracy: 0.8004 - val_f1_score: 0.9211\n",
      "Epoch 1049/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9211 - val_loss: 2.7271 - val_accuracy: 0.7973 - val_f1_score: 0.9211\n",
      "Epoch 1050/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9211 - val_loss: 2.7397 - val_accuracy: 0.7973 - val_f1_score: 0.9211\n",
      "Epoch 1051/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7326 - val_accuracy: 0.7982 - val_f1_score: 0.9212\n",
      "Epoch 1052/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7329 - val_accuracy: 0.7986 - val_f1_score: 0.9212\n",
      "Epoch 1053/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7260 - val_accuracy: 0.7990 - val_f1_score: 0.9212\n",
      "Epoch 1054/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9212 - val_loss: 2.7272 - val_accuracy: 0.7990 - val_f1_score: 0.9212\n",
      "Epoch 1055/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7273 - val_accuracy: 0.7986 - val_f1_score: 0.9212\n",
      "Epoch 1056/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9212 - val_loss: 2.7415 - val_accuracy: 0.7977 - val_f1_score: 0.9212\n",
      "Epoch 1057/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9212 - val_loss: 2.7560 - val_accuracy: 0.7973 - val_f1_score: 0.9212\n",
      "Epoch 1058/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9857 - f1_score: 0.9212 - val_loss: 2.7588 - val_accuracy: 0.7973 - val_f1_score: 0.9212\n",
      "Epoch 1059/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7546 - val_accuracy: 0.7968 - val_f1_score: 0.9212\n",
      "Epoch 1060/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9212 - val_loss: 2.7621 - val_accuracy: 0.7955 - val_f1_score: 0.9212\n",
      "Epoch 1061/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7621 - val_accuracy: 0.7955 - val_f1_score: 0.9212\n",
      "Epoch 1062/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9212 - val_loss: 2.7591 - val_accuracy: 0.7951 - val_f1_score: 0.9212\n",
      "Epoch 1063/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9212 - val_loss: 2.7613 - val_accuracy: 0.7964 - val_f1_score: 0.9212\n",
      "Epoch 1064/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7558 - val_accuracy: 0.7973 - val_f1_score: 0.9212\n",
      "Epoch 1065/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0588 - accuracy: 0.9842 - f1_score: 0.9212 - val_loss: 2.7590 - val_accuracy: 0.7955 - val_f1_score: 0.9212\n",
      "Epoch 1066/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9212 - val_loss: 2.7635 - val_accuracy: 0.7942 - val_f1_score: 0.9212\n",
      "Epoch 1067/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9212 - val_loss: 2.7622 - val_accuracy: 0.7947 - val_f1_score: 0.9212\n",
      "Epoch 1068/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9212 - val_loss: 2.7590 - val_accuracy: 0.7955 - val_f1_score: 0.9212\n",
      "Epoch 1069/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9835 - f1_score: 0.9212 - val_loss: 2.7558 - val_accuracy: 0.7964 - val_f1_score: 0.9212\n",
      "Epoch 1070/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9833 - f1_score: 0.9212 - val_loss: 2.7548 - val_accuracy: 0.7951 - val_f1_score: 0.9212\n",
      "Epoch 1071/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9857 - f1_score: 0.9212 - val_loss: 2.7632 - val_accuracy: 0.7960 - val_f1_score: 0.9212\n",
      "Epoch 1072/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9213 - val_loss: 2.7615 - val_accuracy: 0.7960 - val_f1_score: 0.9213\n",
      "Epoch 1073/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9213 - val_loss: 2.7576 - val_accuracy: 0.7964 - val_f1_score: 0.9213\n",
      "Epoch 1074/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9213 - val_loss: 2.7590 - val_accuracy: 0.7964 - val_f1_score: 0.9213\n",
      "Epoch 1075/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9213 - val_loss: 2.7560 - val_accuracy: 0.7955 - val_f1_score: 0.9213\n",
      "Epoch 1076/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9844 - f1_score: 0.9213 - val_loss: 2.7604 - val_accuracy: 0.7973 - val_f1_score: 0.9213\n",
      "Epoch 1077/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9213 - val_loss: 2.7585 - val_accuracy: 0.7990 - val_f1_score: 0.9213\n",
      "Epoch 1078/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9213 - val_loss: 2.7376 - val_accuracy: 0.7990 - val_f1_score: 0.9213\n",
      "Epoch 1079/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9213 - val_loss: 2.7463 - val_accuracy: 0.7986 - val_f1_score: 0.9213\n",
      "Epoch 1080/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9213 - val_loss: 2.7453 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1081/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9213 - val_loss: 2.7264 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1082/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9829 - f1_score: 0.9213 - val_loss: 2.7312 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1083/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9213 - val_loss: 2.7330 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1084/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9213 - val_loss: 2.7385 - val_accuracy: 0.7977 - val_f1_score: 0.9213\n",
      "Epoch 1085/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9213 - val_loss: 2.7366 - val_accuracy: 0.7977 - val_f1_score: 0.9213\n",
      "Epoch 1086/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9842 - f1_score: 0.9213 - val_loss: 2.7475 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1087/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9213 - val_loss: 2.7491 - val_accuracy: 0.7986 - val_f1_score: 0.9213\n",
      "Epoch 1088/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9213 - val_loss: 2.7358 - val_accuracy: 0.7986 - val_f1_score: 0.9213\n",
      "Epoch 1089/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9213 - val_loss: 2.7526 - val_accuracy: 0.7990 - val_f1_score: 0.9213\n",
      "Epoch 1090/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9857 - f1_score: 0.9213 - val_loss: 2.7405 - val_accuracy: 0.7990 - val_f1_score: 0.9213\n",
      "Epoch 1091/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9213 - val_loss: 2.7554 - val_accuracy: 0.7982 - val_f1_score: 0.9213\n",
      "Epoch 1092/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9213 - val_loss: 2.7615 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1093/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9214 - val_loss: 2.7675 - val_accuracy: 0.7982 - val_f1_score: 0.9214\n",
      "Epoch 1094/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9825 - f1_score: 0.9214 - val_loss: 2.7587 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1095/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9214 - val_loss: 2.7621 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1096/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9214 - val_loss: 2.7545 - val_accuracy: 0.7982 - val_f1_score: 0.9214\n",
      "Epoch 1097/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9214 - val_loss: 2.7498 - val_accuracy: 0.7982 - val_f1_score: 0.9214\n",
      "Epoch 1098/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9214 - val_loss: 2.7503 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1099/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9214 - val_loss: 2.7568 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1100/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9850 - f1_score: 0.9214 - val_loss: 2.7423 - val_accuracy: 0.7982 - val_f1_score: 0.9214\n",
      "Epoch 1101/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0571 - accuracy: 0.9839 - f1_score: 0.9214 - val_loss: 2.7509 - val_accuracy: 0.7990 - val_f1_score: 0.9214\n",
      "Epoch 1102/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9214 - val_loss: 2.7495 - val_accuracy: 0.7986 - val_f1_score: 0.9214\n",
      "Epoch 1103/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9214 - val_loss: 2.7363 - val_accuracy: 0.7990 - val_f1_score: 0.9214\n",
      "Epoch 1104/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9214 - val_loss: 2.7430 - val_accuracy: 0.7990 - val_f1_score: 0.9214\n",
      "Epoch 1105/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9824 - f1_score: 0.9214 - val_loss: 2.7501 - val_accuracy: 0.7990 - val_f1_score: 0.9214\n",
      "Epoch 1106/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9852 - f1_score: 0.9214 - val_loss: 2.7414 - val_accuracy: 0.7986 - val_f1_score: 0.9214\n",
      "Epoch 1107/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9214 - val_loss: 2.7409 - val_accuracy: 0.7986 - val_f1_score: 0.9214\n",
      "Epoch 1108/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9827 - f1_score: 0.9214 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1109/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9214 - val_loss: 2.7572 - val_accuracy: 0.7977 - val_f1_score: 0.9214\n",
      "Epoch 1110/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9214 - val_loss: 2.7448 - val_accuracy: 0.7973 - val_f1_score: 0.9214\n",
      "Epoch 1111/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9214 - val_loss: 2.7506 - val_accuracy: 0.7973 - val_f1_score: 0.9214\n",
      "Epoch 1112/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9214 - val_loss: 2.7522 - val_accuracy: 0.7968 - val_f1_score: 0.9214\n",
      "Epoch 1113/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9214 - val_loss: 2.7500 - val_accuracy: 0.7955 - val_f1_score: 0.9214\n",
      "Epoch 1114/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9215 - val_loss: 2.7505 - val_accuracy: 0.7947 - val_f1_score: 0.9215\n",
      "Epoch 1115/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9857 - f1_score: 0.9215 - val_loss: 2.7533 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1116/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9842 - f1_score: 0.9215 - val_loss: 2.7455 - val_accuracy: 0.7951 - val_f1_score: 0.9215\n",
      "Epoch 1117/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9215 - val_loss: 2.7550 - val_accuracy: 0.7960 - val_f1_score: 0.9215\n",
      "Epoch 1118/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9215 - val_loss: 2.7539 - val_accuracy: 0.7960 - val_f1_score: 0.9215\n",
      "Epoch 1119/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9854 - f1_score: 0.9215 - val_loss: 2.7533 - val_accuracy: 0.7964 - val_f1_score: 0.9215\n",
      "Epoch 1120/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9215 - val_loss: 2.7490 - val_accuracy: 0.7986 - val_f1_score: 0.9215\n",
      "Epoch 1121/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9215 - val_loss: 2.7506 - val_accuracy: 0.7973 - val_f1_score: 0.9215\n",
      "Epoch 1122/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9215 - val_loss: 2.7532 - val_accuracy: 0.7977 - val_f1_score: 0.9215\n",
      "Epoch 1123/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9215 - val_loss: 2.7536 - val_accuracy: 0.7982 - val_f1_score: 0.9215\n",
      "Epoch 1124/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9215 - val_loss: 2.7542 - val_accuracy: 0.7982 - val_f1_score: 0.9215\n",
      "Epoch 1125/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9846 - f1_score: 0.9215 - val_loss: 2.7534 - val_accuracy: 0.7968 - val_f1_score: 0.9215\n",
      "Epoch 1126/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0618 - accuracy: 0.9835 - f1_score: 0.9215 - val_loss: 2.7509 - val_accuracy: 0.7960 - val_f1_score: 0.9215\n",
      "Epoch 1127/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9825 - f1_score: 0.9215 - val_loss: 2.7487 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1128/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9844 - f1_score: 0.9215 - val_loss: 2.7443 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1129/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9215 - val_loss: 2.7443 - val_accuracy: 0.7947 - val_f1_score: 0.9215\n",
      "Epoch 1130/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9833 - f1_score: 0.9215 - val_loss: 2.7450 - val_accuracy: 0.7951 - val_f1_score: 0.9215\n",
      "Epoch 1131/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9215 - val_loss: 2.7472 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1132/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9215 - val_loss: 2.7471 - val_accuracy: 0.7977 - val_f1_score: 0.9215\n",
      "Epoch 1133/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9215 - val_loss: 2.7524 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1134/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9215 - val_loss: 2.7478 - val_accuracy: 0.7955 - val_f1_score: 0.9215\n",
      "Epoch 1135/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9846 - f1_score: 0.9215 - val_loss: 2.7500 - val_accuracy: 0.7960 - val_f1_score: 0.9215\n",
      "Epoch 1136/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9857 - f1_score: 0.9215 - val_loss: 2.7507 - val_accuracy: 0.7968 - val_f1_score: 0.9215\n",
      "Epoch 1137/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9850 - f1_score: 0.9215 - val_loss: 2.7475 - val_accuracy: 0.7960 - val_f1_score: 0.9215\n",
      "Epoch 1138/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9216 - val_loss: 2.7457 - val_accuracy: 0.7947 - val_f1_score: 0.9216\n",
      "Epoch 1139/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9216 - val_loss: 2.7455 - val_accuracy: 0.7955 - val_f1_score: 0.9216\n",
      "Epoch 1140/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9842 - f1_score: 0.9216 - val_loss: 2.7445 - val_accuracy: 0.7951 - val_f1_score: 0.9216\n",
      "Epoch 1141/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9825 - f1_score: 0.9216 - val_loss: 2.7451 - val_accuracy: 0.7960 - val_f1_score: 0.9216\n",
      "Epoch 1142/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9216 - val_loss: 2.7454 - val_accuracy: 0.7947 - val_f1_score: 0.9216\n",
      "Epoch 1143/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9831 - f1_score: 0.9216 - val_loss: 2.7454 - val_accuracy: 0.7955 - val_f1_score: 0.9216\n",
      "Epoch 1144/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9216 - val_loss: 2.7435 - val_accuracy: 0.7942 - val_f1_score: 0.9216\n",
      "Epoch 1145/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9216 - val_loss: 2.7459 - val_accuracy: 0.7947 - val_f1_score: 0.9216\n",
      "Epoch 1146/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9216 - val_loss: 2.7442 - val_accuracy: 0.7942 - val_f1_score: 0.9216\n",
      "Epoch 1147/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9216 - val_loss: 2.7511 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1148/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9859 - f1_score: 0.9216 - val_loss: 2.7485 - val_accuracy: 0.7973 - val_f1_score: 0.9216\n",
      "Epoch 1149/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9216 - val_loss: 2.7460 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1150/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9859 - f1_score: 0.9216 - val_loss: 2.7513 - val_accuracy: 0.7973 - val_f1_score: 0.9216\n",
      "Epoch 1151/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9216 - val_loss: 2.7469 - val_accuracy: 0.7973 - val_f1_score: 0.9216\n",
      "Epoch 1152/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9216 - val_loss: 2.7433 - val_accuracy: 0.7968 - val_f1_score: 0.9216\n",
      "Epoch 1153/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9216 - val_loss: 2.7445 - val_accuracy: 0.7977 - val_f1_score: 0.9216\n",
      "Epoch 1154/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9216 - val_loss: 2.7460 - val_accuracy: 0.7977 - val_f1_score: 0.9216\n",
      "Epoch 1155/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9855 - f1_score: 0.9216 - val_loss: 2.7443 - val_accuracy: 0.7977 - val_f1_score: 0.9216\n",
      "Epoch 1156/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9216 - val_loss: 2.7502 - val_accuracy: 0.7973 - val_f1_score: 0.9216\n",
      "Epoch 1157/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9216 - val_loss: 2.7494 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1158/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9216 - val_loss: 2.7482 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1159/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9216 - val_loss: 2.7472 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1160/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9216 - val_loss: 2.7473 - val_accuracy: 0.7968 - val_f1_score: 0.9216\n",
      "Epoch 1161/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9854 - f1_score: 0.9216 - val_loss: 2.7456 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1162/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9216 - val_loss: 2.7454 - val_accuracy: 0.7964 - val_f1_score: 0.9216\n",
      "Epoch 1163/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9216 - val_loss: 2.7436 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1164/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9217 - val_loss: 2.7524 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1165/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9217 - val_loss: 2.7542 - val_accuracy: 0.7977 - val_f1_score: 0.9217\n",
      "Epoch 1166/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9861 - f1_score: 0.9217 - val_loss: 2.7527 - val_accuracy: 0.7973 - val_f1_score: 0.9217\n",
      "Epoch 1167/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9217 - val_loss: 2.7594 - val_accuracy: 0.7977 - val_f1_score: 0.9217\n",
      "Epoch 1168/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9822 - f1_score: 0.9217 - val_loss: 2.7506 - val_accuracy: 0.7973 - val_f1_score: 0.9217\n",
      "Epoch 1169/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9217 - val_loss: 2.7563 - val_accuracy: 0.7964 - val_f1_score: 0.9217\n",
      "Epoch 1170/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9833 - f1_score: 0.9217 - val_loss: 2.7528 - val_accuracy: 0.7964 - val_f1_score: 0.9217\n",
      "Epoch 1171/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0597 - accuracy: 0.9831 - f1_score: 0.9217 - val_loss: 2.7589 - val_accuracy: 0.7973 - val_f1_score: 0.9217\n",
      "Epoch 1172/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9825 - f1_score: 0.9217 - val_loss: 2.7591 - val_accuracy: 0.7960 - val_f1_score: 0.9217\n",
      "Epoch 1173/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9217 - val_loss: 2.7589 - val_accuracy: 0.7964 - val_f1_score: 0.9217\n",
      "Epoch 1174/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9217 - val_loss: 2.7538 - val_accuracy: 0.7964 - val_f1_score: 0.9217\n",
      "Epoch 1175/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9217 - val_loss: 2.7547 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1176/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9217 - val_loss: 2.7543 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1177/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9217 - val_loss: 2.7546 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1178/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9217 - val_loss: 2.7527 - val_accuracy: 0.7964 - val_f1_score: 0.9217\n",
      "Epoch 1179/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0577 - accuracy: 0.9865 - f1_score: 0.9217 - val_loss: 2.7556 - val_accuracy: 0.7973 - val_f1_score: 0.9217\n",
      "Epoch 1180/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9839 - f1_score: 0.9217 - val_loss: 2.7519 - val_accuracy: 0.7982 - val_f1_score: 0.9217\n",
      "Epoch 1181/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9854 - f1_score: 0.9217 - val_loss: 2.7507 - val_accuracy: 0.7973 - val_f1_score: 0.9217\n",
      "Epoch 1182/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9217 - val_loss: 2.7491 - val_accuracy: 0.7977 - val_f1_score: 0.9217\n",
      "Epoch 1183/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9217 - val_loss: 2.7507 - val_accuracy: 0.7977 - val_f1_score: 0.9217\n",
      "Epoch 1184/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9217 - val_loss: 2.7515 - val_accuracy: 0.7977 - val_f1_score: 0.9217\n",
      "Epoch 1185/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9217 - val_loss: 2.7524 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1186/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9217 - val_loss: 2.7524 - val_accuracy: 0.7968 - val_f1_score: 0.9217\n",
      "Epoch 1187/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9217 - val_loss: 2.7509 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1188/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9218 - val_loss: 2.7501 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1189/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9218 - val_loss: 2.7482 - val_accuracy: 0.7960 - val_f1_score: 0.9218\n",
      "Epoch 1190/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9218 - val_loss: 2.7505 - val_accuracy: 0.7960 - val_f1_score: 0.9218\n",
      "Epoch 1191/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0593 - accuracy: 0.9837 - f1_score: 0.9218 - val_loss: 2.7582 - val_accuracy: 0.7960 - val_f1_score: 0.9218\n",
      "Epoch 1192/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9218 - val_loss: 2.7553 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1193/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9218 - val_loss: 2.7546 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1194/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9218 - val_loss: 2.7523 - val_accuracy: 0.7977 - val_f1_score: 0.9218\n",
      "Epoch 1195/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9822 - f1_score: 0.9218 - val_loss: 2.7589 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1196/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9218 - val_loss: 2.7589 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1197/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9218 - val_loss: 2.7554 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1198/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9833 - f1_score: 0.9218 - val_loss: 2.7587 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1199/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9218 - val_loss: 2.7582 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1200/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9846 - f1_score: 0.9218 - val_loss: 2.7586 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1201/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9218 - val_loss: 2.7583 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1202/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9218 - val_loss: 2.7583 - val_accuracy: 0.7973 - val_f1_score: 0.9218\n",
      "Epoch 1203/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9218 - val_loss: 2.7587 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1204/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9859 - f1_score: 0.9218 - val_loss: 2.7569 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1205/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9859 - f1_score: 0.9218 - val_loss: 2.7588 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1206/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9852 - f1_score: 0.9218 - val_loss: 2.7530 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1207/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9824 - f1_score: 0.9218 - val_loss: 2.7524 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1208/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9218 - val_loss: 2.7524 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1209/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9854 - f1_score: 0.9218 - val_loss: 2.7521 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1210/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9218 - val_loss: 2.7583 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1211/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9218 - val_loss: 2.7564 - val_accuracy: 0.7968 - val_f1_score: 0.9218\n",
      "Epoch 1212/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9218 - val_loss: 2.7566 - val_accuracy: 0.7964 - val_f1_score: 0.9218\n",
      "Epoch 1213/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9218 - val_loss: 2.7587 - val_accuracy: 0.7982 - val_f1_score: 0.9218\n",
      "Epoch 1214/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9218 - val_loss: 2.7586 - val_accuracy: 0.7964 - val_f1_score: 0.9219\n",
      "Epoch 1215/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9219 - val_loss: 2.7618 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1216/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9219 - val_loss: 2.7636 - val_accuracy: 0.7973 - val_f1_score: 0.9219\n",
      "Epoch 1217/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9219 - val_loss: 2.7643 - val_accuracy: 0.7968 - val_f1_score: 0.9219\n",
      "Epoch 1218/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9219 - val_loss: 2.7596 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1219/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9219 - val_loss: 2.7520 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1220/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9219 - val_loss: 2.7519 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1221/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9219 - val_loss: 2.7512 - val_accuracy: 0.7968 - val_f1_score: 0.9219\n",
      "Epoch 1222/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9219 - val_loss: 2.7511 - val_accuracy: 0.7968 - val_f1_score: 0.9219\n",
      "Epoch 1223/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9219 - val_loss: 2.7514 - val_accuracy: 0.7964 - val_f1_score: 0.9219\n",
      "Epoch 1224/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9219 - val_loss: 2.7496 - val_accuracy: 0.7964 - val_f1_score: 0.9219\n",
      "Epoch 1225/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0565 - accuracy: 0.9840 - f1_score: 0.9219 - val_loss: 2.7544 - val_accuracy: 0.7951 - val_f1_score: 0.9219\n",
      "Epoch 1226/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9219 - val_loss: 2.7525 - val_accuracy: 0.7951 - val_f1_score: 0.9219\n",
      "Epoch 1227/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9219 - val_loss: 2.7525 - val_accuracy: 0.7951 - val_f1_score: 0.9219\n",
      "Epoch 1228/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9219 - val_loss: 2.7524 - val_accuracy: 0.7947 - val_f1_score: 0.9219\n",
      "Epoch 1229/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9219 - val_loss: 2.7382 - val_accuracy: 0.7947 - val_f1_score: 0.9219\n",
      "Epoch 1230/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9219 - val_loss: 2.7407 - val_accuracy: 0.7947 - val_f1_score: 0.9219\n",
      "Epoch 1231/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9219 - val_loss: 2.7475 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1232/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9219 - val_loss: 2.7495 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1233/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9219 - val_loss: 2.7470 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1234/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9219 - val_loss: 2.7476 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1235/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9219 - val_loss: 2.7437 - val_accuracy: 0.7951 - val_f1_score: 0.9219\n",
      "Epoch 1236/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9219 - val_loss: 2.7407 - val_accuracy: 0.7951 - val_f1_score: 0.9219\n",
      "Epoch 1237/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9831 - f1_score: 0.9219 - val_loss: 2.7472 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1238/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9219 - val_loss: 2.7455 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1239/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9219 - val_loss: 2.7521 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1240/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9219 - val_loss: 2.7504 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1241/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9219 - val_loss: 2.7474 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1242/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9219 - val_loss: 2.7497 - val_accuracy: 0.7960 - val_f1_score: 0.9219\n",
      "Epoch 1243/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9859 - f1_score: 0.9219 - val_loss: 2.7481 - val_accuracy: 0.7955 - val_f1_score: 0.9219\n",
      "Epoch 1244/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9844 - f1_score: 0.9220 - val_loss: 2.7478 - val_accuracy: 0.7942 - val_f1_score: 0.9220\n",
      "Epoch 1245/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9220 - val_loss: 2.7460 - val_accuracy: 0.7947 - val_f1_score: 0.9220\n",
      "Epoch 1246/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9220 - val_loss: 2.7482 - val_accuracy: 0.7955 - val_f1_score: 0.9220\n",
      "Epoch 1247/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0563 - accuracy: 0.9837 - f1_score: 0.9220 - val_loss: 2.7498 - val_accuracy: 0.7947 - val_f1_score: 0.9220\n",
      "Epoch 1248/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9220 - val_loss: 2.7544 - val_accuracy: 0.7955 - val_f1_score: 0.9220\n",
      "Epoch 1249/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9220 - val_loss: 2.7543 - val_accuracy: 0.7960 - val_f1_score: 0.9220\n",
      "Epoch 1250/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9220 - val_loss: 2.7542 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1251/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9863 - f1_score: 0.9220 - val_loss: 2.7544 - val_accuracy: 0.7964 - val_f1_score: 0.9220\n",
      "Epoch 1252/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9855 - f1_score: 0.9220 - val_loss: 2.7546 - val_accuracy: 0.7973 - val_f1_score: 0.9220\n",
      "Epoch 1253/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0591 - accuracy: 0.9840 - f1_score: 0.9220 - val_loss: 2.7547 - val_accuracy: 0.7964 - val_f1_score: 0.9220\n",
      "Epoch 1254/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9220 - val_loss: 2.7544 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1255/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9220 - val_loss: 2.7543 - val_accuracy: 0.7964 - val_f1_score: 0.9220\n",
      "Epoch 1256/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9220 - val_loss: 2.7541 - val_accuracy: 0.7964 - val_f1_score: 0.9220\n",
      "Epoch 1257/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9220 - val_loss: 2.7504 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1258/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9220 - val_loss: 2.7524 - val_accuracy: 0.7973 - val_f1_score: 0.9220\n",
      "Epoch 1259/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9220 - val_loss: 2.7496 - val_accuracy: 0.7960 - val_f1_score: 0.9220\n",
      "Epoch 1260/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9220 - val_loss: 2.7529 - val_accuracy: 0.7973 - val_f1_score: 0.9220\n",
      "Epoch 1261/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9220 - val_loss: 2.7544 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1262/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0531 - accuracy: 0.9850 - f1_score: 0.9220 - val_loss: 2.7547 - val_accuracy: 0.7960 - val_f1_score: 0.9220\n",
      "Epoch 1263/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9220 - val_loss: 2.7612 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1264/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9220 - val_loss: 2.7617 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1265/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9220 - val_loss: 2.7588 - val_accuracy: 0.7973 - val_f1_score: 0.9220\n",
      "Epoch 1266/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9220 - val_loss: 2.7555 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1267/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9833 - f1_score: 0.9220 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9220\n",
      "Epoch 1268/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9220 - val_loss: 2.7566 - val_accuracy: 0.7977 - val_f1_score: 0.9220\n",
      "Epoch 1269/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0566 - accuracy: 0.9844 - f1_score: 0.9220 - val_loss: 2.7541 - val_accuracy: 0.7977 - val_f1_score: 0.9220\n",
      "Epoch 1270/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9825 - f1_score: 0.9220 - val_loss: 2.7555 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1271/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9220 - val_loss: 2.7559 - val_accuracy: 0.7968 - val_f1_score: 0.9220\n",
      "Epoch 1272/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9220 - val_loss: 2.7518 - val_accuracy: 0.7973 - val_f1_score: 0.9220\n",
      "Epoch 1273/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9220 - val_loss: 2.7537 - val_accuracy: 0.7968 - val_f1_score: 0.9221\n",
      "Epoch 1274/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9221 - val_loss: 2.7591 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1275/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9221 - val_loss: 2.7590 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1276/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9848 - f1_score: 0.9221 - val_loss: 2.7562 - val_accuracy: 0.7968 - val_f1_score: 0.9221\n",
      "Epoch 1277/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9221 - val_loss: 2.7509 - val_accuracy: 0.7960 - val_f1_score: 0.9221\n",
      "Epoch 1278/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9221 - val_loss: 2.7548 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1279/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9221 - val_loss: 2.7558 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1280/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9221 - val_loss: 2.7536 - val_accuracy: 0.7968 - val_f1_score: 0.9221\n",
      "Epoch 1281/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9221 - val_loss: 2.7520 - val_accuracy: 0.7968 - val_f1_score: 0.9221\n",
      "Epoch 1282/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9221 - val_loss: 2.7548 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1283/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9857 - f1_score: 0.9221 - val_loss: 2.7577 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1284/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9221 - val_loss: 2.7576 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1285/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9221 - val_loss: 2.7540 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1286/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9221 - val_loss: 2.7554 - val_accuracy: 0.7968 - val_f1_score: 0.9221\n",
      "Epoch 1287/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9221 - val_loss: 2.7569 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1288/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9221 - val_loss: 2.7547 - val_accuracy: 0.7960 - val_f1_score: 0.9221\n",
      "Epoch 1289/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9221 - val_loss: 2.7480 - val_accuracy: 0.7960 - val_f1_score: 0.9221\n",
      "Epoch 1290/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9221 - val_loss: 2.7474 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1291/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9842 - f1_score: 0.9221 - val_loss: 2.7387 - val_accuracy: 0.7960 - val_f1_score: 0.9221\n",
      "Epoch 1292/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0569 - accuracy: 0.9850 - f1_score: 0.9221 - val_loss: 2.7393 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1293/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9221 - val_loss: 2.7444 - val_accuracy: 0.7964 - val_f1_score: 0.9221\n",
      "Epoch 1294/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0614 - accuracy: 0.9852 - f1_score: 0.9221 - val_loss: 2.7470 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1295/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9221 - val_loss: 2.7545 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1296/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9221 - val_loss: 2.7502 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1297/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9221 - val_loss: 2.7334 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1298/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9221 - val_loss: 2.7326 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1299/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9221 - val_loss: 2.7359 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1300/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9221 - val_loss: 2.7341 - val_accuracy: 0.7973 - val_f1_score: 0.9221\n",
      "Epoch 1301/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9221 - val_loss: 2.7361 - val_accuracy: 0.7977 - val_f1_score: 0.9221\n",
      "Epoch 1302/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9221 - val_loss: 2.7322 - val_accuracy: 0.7977 - val_f1_score: 0.9221\n",
      "Epoch 1303/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9221 - val_loss: 2.7305 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1304/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9222 - val_loss: 2.7352 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1305/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9222 - val_loss: 2.7282 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1306/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9222 - val_loss: 2.7295 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1307/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9222 - val_loss: 2.7282 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1308/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9222 - val_loss: 2.7284 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1309/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9222 - val_loss: 2.7290 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1310/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9852 - f1_score: 0.9222 - val_loss: 2.7331 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1311/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9222 - val_loss: 2.7381 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1312/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0567 - accuracy: 0.9831 - f1_score: 0.9222 - val_loss: 2.7383 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1313/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9222 - val_loss: 2.7477 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1314/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9869 - f1_score: 0.9222 - val_loss: 2.7465 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1315/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9222 - val_loss: 2.7441 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1316/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9835 - f1_score: 0.9222 - val_loss: 2.7454 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1317/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9222 - val_loss: 2.7443 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1318/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9222 - val_loss: 2.7567 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1319/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9222 - val_loss: 2.7548 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1320/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9835 - f1_score: 0.9222 - val_loss: 2.7480 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1321/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9222 - val_loss: 2.7478 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1322/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9827 - f1_score: 0.9222 - val_loss: 2.7518 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1323/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9222 - val_loss: 2.7500 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1324/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9222 - val_loss: 2.7491 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1325/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9855 - f1_score: 0.9222 - val_loss: 2.7495 - val_accuracy: 0.7977 - val_f1_score: 0.9222\n",
      "Epoch 1326/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9222 - val_loss: 2.7470 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1327/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9222 - val_loss: 2.7447 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1328/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9222 - val_loss: 2.7500 - val_accuracy: 0.7973 - val_f1_score: 0.9222\n",
      "Epoch 1329/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9222 - val_loss: 2.7449 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1330/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9222 - val_loss: 2.7525 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1331/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9222 - val_loss: 2.7527 - val_accuracy: 0.7964 - val_f1_score: 0.9222\n",
      "Epoch 1332/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9222 - val_loss: 2.7504 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1333/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9222 - val_loss: 2.7503 - val_accuracy: 0.7968 - val_f1_score: 0.9222\n",
      "Epoch 1334/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9222 - val_loss: 2.7462 - val_accuracy: 0.7964 - val_f1_score: 0.9222\n",
      "Epoch 1335/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9223 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1336/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9223 - val_loss: 2.7465 - val_accuracy: 0.7977 - val_f1_score: 0.9223\n",
      "Epoch 1337/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9223 - val_loss: 2.7507 - val_accuracy: 0.7977 - val_f1_score: 0.9223\n",
      "Epoch 1338/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0586 - accuracy: 0.9850 - f1_score: 0.9223 - val_loss: 2.7559 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1339/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9223 - val_loss: 2.7508 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1340/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9223 - val_loss: 2.7457 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1341/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7534 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1342/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9223 - val_loss: 2.7534 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1343/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9223 - val_loss: 2.7519 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1344/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9223 - val_loss: 2.7459 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1345/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0565 - accuracy: 0.9852 - f1_score: 0.9223 - val_loss: 2.7531 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1346/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9223 - val_loss: 2.7530 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1347/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9855 - f1_score: 0.9223 - val_loss: 2.7512 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1348/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7493 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1349/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9223 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1350/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0535 - accuracy: 0.9852 - f1_score: 0.9223 - val_loss: 2.7453 - val_accuracy: 0.7964 - val_f1_score: 0.9223\n",
      "Epoch 1351/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7452 - val_accuracy: 0.7964 - val_f1_score: 0.9223\n",
      "Epoch 1352/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9223 - val_loss: 2.7451 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1353/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7488 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1354/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7483 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1355/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9223 - val_loss: 2.7491 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1356/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9827 - f1_score: 0.9223 - val_loss: 2.7495 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1357/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9223 - val_loss: 2.7460 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1358/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9863 - f1_score: 0.9223 - val_loss: 2.7442 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1359/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9223 - val_loss: 2.7488 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1360/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9223 - val_loss: 2.7439 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1361/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9223 - val_loss: 2.7454 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1362/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9223 - val_loss: 2.7465 - val_accuracy: 0.7968 - val_f1_score: 0.9223\n",
      "Epoch 1363/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9223 - val_loss: 2.7494 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1364/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9223 - val_loss: 2.7445 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1365/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9223 - val_loss: 2.7477 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1366/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9223 - val_loss: 2.7448 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1367/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9824 - f1_score: 0.9223 - val_loss: 2.7443 - val_accuracy: 0.7973 - val_f1_score: 0.9223\n",
      "Epoch 1368/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7417 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1369/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9859 - f1_score: 0.9224 - val_loss: 2.7396 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1370/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9224 - val_loss: 2.7443 - val_accuracy: 0.7973 - val_f1_score: 0.9224\n",
      "Epoch 1371/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9224 - val_loss: 2.7338 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1372/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7402 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1373/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9224 - val_loss: 2.7422 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1374/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9224 - val_loss: 2.7378 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1375/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9224 - val_loss: 2.7330 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1376/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9224 - val_loss: 2.7361 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1377/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7366 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1378/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7331 - val_accuracy: 0.7977 - val_f1_score: 0.9224\n",
      "Epoch 1379/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7290 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1380/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9224 - val_loss: 2.7290 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1381/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9224 - val_loss: 2.7267 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1382/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7265 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1383/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9224 - val_loss: 2.7257 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1384/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9224 - val_loss: 2.7256 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1385/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9224 - val_loss: 2.7265 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1386/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9224 - val_loss: 2.7252 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1387/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9844 - f1_score: 0.9224 - val_loss: 2.7254 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1388/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7235 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1389/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7234 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1390/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7260 - val_accuracy: 0.7990 - val_f1_score: 0.9224\n",
      "Epoch 1391/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9846 - f1_score: 0.9224 - val_loss: 2.7236 - val_accuracy: 0.7990 - val_f1_score: 0.9224\n",
      "Epoch 1392/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0606 - accuracy: 0.9846 - f1_score: 0.9224 - val_loss: 2.7242 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1393/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0606 - accuracy: 0.9837 - f1_score: 0.9224 - val_loss: 2.7242 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1394/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7244 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1395/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9224 - val_loss: 2.7247 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1396/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9224 - val_loss: 2.7262 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1397/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9224 - val_loss: 2.7254 - val_accuracy: 0.7986 - val_f1_score: 0.9224\n",
      "Epoch 1398/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9224 - val_loss: 2.7285 - val_accuracy: 0.7982 - val_f1_score: 0.9224\n",
      "Epoch 1399/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9224 - val_loss: 2.7283 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1400/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9225 - val_loss: 2.7283 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1401/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9225 - val_loss: 2.7296 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1402/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9225 - val_loss: 2.7364 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1403/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9225 - val_loss: 2.7297 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1404/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9225 - val_loss: 2.7386 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1405/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9225 - val_loss: 2.7363 - val_accuracy: 0.7986 - val_f1_score: 0.9225\n",
      "Epoch 1406/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9225 - val_loss: 2.7366 - val_accuracy: 0.7986 - val_f1_score: 0.9225\n",
      "Epoch 1407/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9225 - val_loss: 2.7416 - val_accuracy: 0.7986 - val_f1_score: 0.9225\n",
      "Epoch 1408/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9225 - val_loss: 2.7414 - val_accuracy: 0.7977 - val_f1_score: 0.9225\n",
      "Epoch 1409/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0549 - accuracy: 0.9839 - f1_score: 0.9225 - val_loss: 2.7449 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1410/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9225 - val_loss: 2.7471 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1411/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9225 - val_loss: 2.7453 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1412/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9225 - val_loss: 2.7460 - val_accuracy: 0.7982 - val_f1_score: 0.9225\n",
      "Epoch 1413/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9225 - val_loss: 2.7460 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1414/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9225 - val_loss: 2.7483 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1415/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9225 - val_loss: 2.7457 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1416/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9854 - f1_score: 0.9225 - val_loss: 2.7476 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1417/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9825 - f1_score: 0.9225 - val_loss: 2.7476 - val_accuracy: 0.7968 - val_f1_score: 0.9225\n",
      "Epoch 1418/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9225 - val_loss: 2.7477 - val_accuracy: 0.7968 - val_f1_score: 0.9225\n",
      "Epoch 1419/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9225 - val_loss: 2.7474 - val_accuracy: 0.7968 - val_f1_score: 0.9225\n",
      "Epoch 1420/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9225 - val_loss: 2.7433 - val_accuracy: 0.7973 - val_f1_score: 0.9225\n",
      "Epoch 1421/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9831 - f1_score: 0.9225 - val_loss: 2.7438 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1422/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9225 - val_loss: 2.7457 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1423/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9225 - val_loss: 2.7477 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1424/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9848 - f1_score: 0.9225 - val_loss: 2.7458 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1425/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9225 - val_loss: 2.7476 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1426/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9225 - val_loss: 2.7443 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1427/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9225 - val_loss: 2.7436 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1428/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9225 - val_loss: 2.7437 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1429/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9225 - val_loss: 2.7435 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1430/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9225 - val_loss: 2.7449 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1431/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9225 - val_loss: 2.7433 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1432/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9225 - val_loss: 2.7457 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1433/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9824 - f1_score: 0.9225 - val_loss: 2.7474 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1434/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9225 - val_loss: 2.7477 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1435/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9225 - val_loss: 2.7476 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1436/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9225 - val_loss: 2.7453 - val_accuracy: 0.7960 - val_f1_score: 0.9225\n",
      "Epoch 1437/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9225 - val_loss: 2.7478 - val_accuracy: 0.7964 - val_f1_score: 0.9225\n",
      "Epoch 1438/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7546 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1439/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9226 - val_loss: 2.7547 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1440/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9226 - val_loss: 2.7549 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1441/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9854 - f1_score: 0.9226 - val_loss: 2.7548 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1442/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7550 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1443/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9226 - val_loss: 2.7527 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1444/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7548 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1445/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7456 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1446/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9854 - f1_score: 0.9226 - val_loss: 2.7454 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1447/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7456 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1448/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9226 - val_loss: 2.7478 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1449/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9848 - f1_score: 0.9226 - val_loss: 2.7481 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1450/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7485 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1451/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7480 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1452/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9226 - val_loss: 2.7502 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1453/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7489 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1454/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9226 - val_loss: 2.7515 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1455/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0617 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7509 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1456/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7510 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1457/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0564 - accuracy: 0.9829 - f1_score: 0.9226 - val_loss: 2.7478 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1458/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7488 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1459/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9226 - val_loss: 2.7484 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1460/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7485 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1461/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7508 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1462/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7488 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1463/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9226 - val_loss: 2.7484 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1464/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7509 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1465/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9226 - val_loss: 2.7512 - val_accuracy: 0.7960 - val_f1_score: 0.9226\n",
      "Epoch 1466/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9226 - val_loss: 2.7516 - val_accuracy: 0.7964 - val_f1_score: 0.9226\n",
      "Epoch 1467/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7526 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1468/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7527 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1469/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9226 - val_loss: 2.7509 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1470/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7511 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1471/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9226 - val_loss: 2.7508 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1472/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9226 - val_loss: 2.7507 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1473/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0525 - accuracy: 0.9835 - f1_score: 0.9226 - val_loss: 2.7507 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1474/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9226 - val_loss: 2.7509 - val_accuracy: 0.7968 - val_f1_score: 0.9226\n",
      "Epoch 1475/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9226 - val_loss: 2.7484 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1476/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9226 - val_loss: 2.7487 - val_accuracy: 0.7977 - val_f1_score: 0.9226\n",
      "Epoch 1477/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9226 - val_loss: 2.7464 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1478/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9227 - val_loss: 2.7481 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1479/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9227 - val_loss: 2.7461 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1480/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9227 - val_loss: 2.7461 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1481/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0607 - accuracy: 0.9848 - f1_score: 0.9227 - val_loss: 2.7482 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1482/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9227 - val_loss: 2.7483 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1483/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9227 - val_loss: 2.7462 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1484/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9227 - val_loss: 2.7503 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1485/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9227 - val_loss: 2.7465 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1486/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9227 - val_loss: 2.7475 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1487/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9227 - val_loss: 2.7469 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1488/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9227 - val_loss: 2.7469 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1489/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9227 - val_loss: 2.7489 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1490/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9850 - f1_score: 0.9227 - val_loss: 2.7530 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1491/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9227 - val_loss: 2.7553 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1492/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9852 - f1_score: 0.9227 - val_loss: 2.7556 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1493/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9227 - val_loss: 2.7457 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1494/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9227 - val_loss: 2.7453 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1495/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9227 - val_loss: 2.7475 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1496/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9227 - val_loss: 2.7479 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1497/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9227 - val_loss: 2.7532 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1498/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9857 - f1_score: 0.9227 - val_loss: 2.7533 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1499/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9227 - val_loss: 2.7535 - val_accuracy: 0.7968 - val_f1_score: 0.9227\n",
      "Epoch 1500/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9227 - val_loss: 2.7513 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1501/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9227 - val_loss: 2.7472 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1502/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9227 - val_loss: 2.7470 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1503/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9227 - val_loss: 2.7473 - val_accuracy: 0.7973 - val_f1_score: 0.9227\n",
      "Epoch 1504/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9227 - val_loss: 2.7466 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1505/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9227 - val_loss: 2.7471 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1506/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9227 - val_loss: 2.7465 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1507/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0582 - accuracy: 0.9852 - f1_score: 0.9227 - val_loss: 2.7471 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1508/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9227 - val_loss: 2.7470 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1509/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9227 - val_loss: 2.7463 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1510/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9827 - f1_score: 0.9227 - val_loss: 2.7469 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1511/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0609 - accuracy: 0.9837 - f1_score: 0.9227 - val_loss: 2.7467 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1512/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9227 - val_loss: 2.7470 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1513/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9227 - val_loss: 2.7468 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1514/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9227 - val_loss: 2.7480 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1515/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9855 - f1_score: 0.9227 - val_loss: 2.7474 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1516/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9227 - val_loss: 2.7474 - val_accuracy: 0.7964 - val_f1_score: 0.9227\n",
      "Epoch 1517/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9824 - f1_score: 0.9227 - val_loss: 2.7500 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1518/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9831 - f1_score: 0.9228 - val_loss: 2.7467 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1519/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9228 - val_loss: 2.7467 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1520/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9228 - val_loss: 2.7467 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1521/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9228 - val_loss: 2.7466 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1522/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9228 - val_loss: 2.7462 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1523/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9228 - val_loss: 2.7464 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1524/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9228 - val_loss: 2.7464 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1525/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9228 - val_loss: 2.7468 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1526/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9228 - val_loss: 2.7464 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1527/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9228 - val_loss: 2.7466 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1528/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9228 - val_loss: 2.7473 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1529/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9855 - f1_score: 0.9228 - val_loss: 2.7467 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1530/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9228 - val_loss: 2.7468 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1531/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7466 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1532/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0549 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7473 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1533/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9228 - val_loss: 2.7498 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1534/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7499 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1535/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0549 - accuracy: 0.9855 - f1_score: 0.9228 - val_loss: 2.7503 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1536/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9228 - val_loss: 2.7474 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1537/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9228 - val_loss: 2.7500 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1538/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9228 - val_loss: 2.7503 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1539/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9228 - val_loss: 2.7475 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1540/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9833 - f1_score: 0.9228 - val_loss: 2.7473 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1541/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7481 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1542/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7505 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1543/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9228 - val_loss: 2.7481 - val_accuracy: 0.7964 - val_f1_score: 0.9228\n",
      "Epoch 1544/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9842 - f1_score: 0.9228 - val_loss: 2.7503 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1545/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9228 - val_loss: 2.7524 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1546/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0587 - accuracy: 0.9844 - f1_score: 0.9228 - val_loss: 2.7524 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1547/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9228 - val_loss: 2.7526 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1548/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9228 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1549/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9228 - val_loss: 2.7528 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1550/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9228 - val_loss: 2.7510 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1551/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9839 - f1_score: 0.9228 - val_loss: 2.7492 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1552/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9837 - f1_score: 0.9228 - val_loss: 2.7496 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1553/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9228 - val_loss: 2.7447 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1554/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9228 - val_loss: 2.7431 - val_accuracy: 0.7977 - val_f1_score: 0.9228\n",
      "Epoch 1555/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9228 - val_loss: 2.7446 - val_accuracy: 0.7973 - val_f1_score: 0.9228\n",
      "Epoch 1556/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9228 - val_loss: 2.7445 - val_accuracy: 0.7982 - val_f1_score: 0.9228\n",
      "Epoch 1557/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9228 - val_loss: 2.7449 - val_accuracy: 0.7982 - val_f1_score: 0.9228\n",
      "Epoch 1558/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0527 - accuracy: 0.9857 - f1_score: 0.9228 - val_loss: 2.7512 - val_accuracy: 0.7977 - val_f1_score: 0.9228\n",
      "Epoch 1559/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9228 - val_loss: 2.7512 - val_accuracy: 0.7977 - val_f1_score: 0.9228\n",
      "Epoch 1560/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9228 - val_loss: 2.7512 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1561/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0576 - accuracy: 0.9844 - f1_score: 0.9229 - val_loss: 2.7513 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1562/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9229 - val_loss: 2.7514 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1563/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9229 - val_loss: 2.7499 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1564/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9229 - val_loss: 2.7472 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1565/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9229 - val_loss: 2.7513 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1566/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0611 - accuracy: 0.9840 - f1_score: 0.9229 - val_loss: 2.7516 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1567/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0588 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7534 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1568/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0564 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7533 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1569/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9846 - f1_score: 0.9229 - val_loss: 2.7531 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1570/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7530 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1571/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9229 - val_loss: 2.7529 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1572/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9229 - val_loss: 2.7529 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1573/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9229 - val_loss: 2.7531 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1574/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9229 - val_loss: 2.7531 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1575/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9229 - val_loss: 2.7534 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1576/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9229 - val_loss: 2.7538 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1577/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0591 - accuracy: 0.9829 - f1_score: 0.9229 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1578/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9229 - val_loss: 2.7537 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1579/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9229 - val_loss: 2.7536 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1580/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9229 - val_loss: 2.7534 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1581/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9229 - val_loss: 2.7533 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1582/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9825 - f1_score: 0.9229 - val_loss: 2.7534 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1583/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9229 - val_loss: 2.7534 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1584/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9229 - val_loss: 2.7535 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1585/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9229 - val_loss: 2.7510 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1586/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9229 - val_loss: 2.7510 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1587/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9229 - val_loss: 2.7487 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1588/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9820 - f1_score: 0.9229 - val_loss: 2.7488 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1589/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7451 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1590/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0532 - accuracy: 0.9850 - f1_score: 0.9229 - val_loss: 2.7488 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1591/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9229 - val_loss: 2.7488 - val_accuracy: 0.7968 - val_f1_score: 0.9229\n",
      "Epoch 1592/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9848 - f1_score: 0.9229 - val_loss: 2.7488 - val_accuracy: 0.7977 - val_f1_score: 0.9229\n",
      "Epoch 1593/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9229 - val_loss: 2.7450 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1594/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9844 - f1_score: 0.9229 - val_loss: 2.7437 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1595/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9229 - val_loss: 2.7491 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1596/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9229 - val_loss: 2.7449 - val_accuracy: 0.7973 - val_f1_score: 0.9229\n",
      "Epoch 1597/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9229 - val_loss: 2.7447 - val_accuracy: 0.7973 - val_f1_score: 0.9229\n",
      "Epoch 1598/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9229 - val_loss: 2.7444 - val_accuracy: 0.7973 - val_f1_score: 0.9229\n",
      "Epoch 1599/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7444 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1600/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9229 - val_loss: 2.7444 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1601/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9229 - val_loss: 2.7448 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1602/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9229 - val_loss: 2.7446 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1603/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9833 - f1_score: 0.9229 - val_loss: 2.7489 - val_accuracy: 0.7982 - val_f1_score: 0.9229\n",
      "Epoch 1604/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9230 - val_loss: 2.7466 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1605/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9230 - val_loss: 2.7466 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1606/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9230 - val_loss: 2.7481 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1607/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9825 - f1_score: 0.9230 - val_loss: 2.7488 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1608/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9230 - val_loss: 2.7494 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1609/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9230 - val_loss: 2.7493 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1610/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9839 - f1_score: 0.9230 - val_loss: 2.7510 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1611/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9230 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1612/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9230 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1613/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9230 - val_loss: 2.7511 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1614/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0628 - accuracy: 0.9854 - f1_score: 0.9230 - val_loss: 2.7533 - val_accuracy: 0.7968 - val_f1_score: 0.9230\n",
      "Epoch 1615/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0614 - accuracy: 0.9839 - f1_score: 0.9230 - val_loss: 2.7533 - val_accuracy: 0.7968 - val_f1_score: 0.9230\n",
      "Epoch 1616/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9852 - f1_score: 0.9230 - val_loss: 2.7534 - val_accuracy: 0.7968 - val_f1_score: 0.9230\n",
      "Epoch 1617/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0613 - accuracy: 0.9835 - f1_score: 0.9230 - val_loss: 2.7512 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1618/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9824 - f1_score: 0.9230 - val_loss: 2.7514 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1619/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9854 - f1_score: 0.9230 - val_loss: 2.7515 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1620/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9230 - val_loss: 2.7514 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1621/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9857 - f1_score: 0.9230 - val_loss: 2.7514 - val_accuracy: 0.7968 - val_f1_score: 0.9230\n",
      "Epoch 1622/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9230 - val_loss: 2.7512 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1623/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9230 - val_loss: 2.7513 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1624/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9230 - val_loss: 2.7494 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1625/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9230 - val_loss: 2.7473 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1626/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9230 - val_loss: 2.7537 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1627/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9230 - val_loss: 2.7514 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1628/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9827 - f1_score: 0.9230 - val_loss: 2.7494 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1629/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0618 - accuracy: 0.9831 - f1_score: 0.9230 - val_loss: 2.7474 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1630/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9230 - val_loss: 2.7473 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1631/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9230 - val_loss: 2.7471 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1632/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0608 - accuracy: 0.9829 - f1_score: 0.9230 - val_loss: 2.7475 - val_accuracy: 0.7977 - val_f1_score: 0.9230\n",
      "Epoch 1633/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9230 - val_loss: 2.7475 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1634/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9230 - val_loss: 2.7471 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1635/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9230 - val_loss: 2.7450 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1636/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9230 - val_loss: 2.7450 - val_accuracy: 0.7982 - val_f1_score: 0.9230\n",
      "Epoch 1637/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9230 - val_loss: 2.7452 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1638/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9230 - val_loss: 2.7478 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1639/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9230 - val_loss: 2.7490 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1640/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9230 - val_loss: 2.7453 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1641/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9230 - val_loss: 2.7452 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1642/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9230 - val_loss: 2.7431 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1643/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9230 - val_loss: 2.7429 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1644/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9230 - val_loss: 2.7448 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1645/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9230 - val_loss: 2.7470 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1646/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9230 - val_loss: 2.7490 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1647/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9852 - f1_score: 0.9230 - val_loss: 2.7491 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1648/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9230 - val_loss: 2.7491 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1649/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9854 - f1_score: 0.9230 - val_loss: 2.7491 - val_accuracy: 0.7973 - val_f1_score: 0.9230\n",
      "Epoch 1650/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9230 - val_loss: 2.7490 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1651/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9824 - f1_score: 0.9231 - val_loss: 2.7454 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1652/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9231 - val_loss: 2.7435 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1653/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0610 - accuracy: 0.9848 - f1_score: 0.9231 - val_loss: 2.7432 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1654/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7451 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1655/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9231 - val_loss: 2.7452 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1656/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7453 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1657/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9231 - val_loss: 2.7452 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1658/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7453 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1659/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7457 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1660/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7460 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1661/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7459 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1662/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9231 - val_loss: 2.7495 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1663/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7495 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1664/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7495 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1665/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7514 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1666/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9231 - val_loss: 2.7514 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1667/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7513 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1668/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7512 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1669/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7511 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1670/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9231 - val_loss: 2.7510 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1671/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9854 - f1_score: 0.9231 - val_loss: 2.7510 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1672/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9231 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1673/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7509 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1674/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7510 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1675/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7513 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1676/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7512 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1677/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7513 - val_accuracy: 0.7977 - val_f1_score: 0.9231\n",
      "Epoch 1678/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0576 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7515 - val_accuracy: 0.7977 - val_f1_score: 0.9231\n",
      "Epoch 1679/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9231 - val_loss: 2.7511 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1680/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7512 - val_accuracy: 0.7973 - val_f1_score: 0.9231\n",
      "Epoch 1681/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9822 - f1_score: 0.9231 - val_loss: 2.7514 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1682/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7520 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1683/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7515 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1684/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9231 - val_loss: 2.7515 - val_accuracy: 0.7977 - val_f1_score: 0.9231\n",
      "Epoch 1685/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9822 - f1_score: 0.9231 - val_loss: 2.7512 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1686/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9231 - val_loss: 2.7518 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1687/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7517 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1688/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9231 - val_loss: 2.7514 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1689/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7509 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1690/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0592 - accuracy: 0.9833 - f1_score: 0.9231 - val_loss: 2.7512 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1691/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9231 - val_loss: 2.7508 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1692/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1693/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0577 - accuracy: 0.9842 - f1_score: 0.9231 - val_loss: 2.7506 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1694/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9231 - val_loss: 2.7507 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1695/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7982 - val_f1_score: 0.9231\n",
      "Epoch 1696/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1697/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1698/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1699/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9231 - val_loss: 2.7505 - val_accuracy: 0.7986 - val_f1_score: 0.9231\n",
      "Epoch 1700/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7505 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1701/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0564 - accuracy: 0.9846 - f1_score: 0.9232 - val_loss: 2.7505 - val_accuracy: 0.7982 - val_f1_score: 0.9232\n",
      "Epoch 1702/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0572 - accuracy: 0.9844 - f1_score: 0.9232 - val_loss: 2.7508 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1703/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9825 - f1_score: 0.9232 - val_loss: 2.7508 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1704/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9232 - val_loss: 2.7511 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1705/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9232 - val_loss: 2.7512 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1706/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9232 - val_loss: 2.7512 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1707/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9232 - val_loss: 2.7515 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1708/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9232 - val_loss: 2.7513 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1709/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9232 - val_loss: 2.7533 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1710/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7512 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1711/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9232 - val_loss: 2.7535 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1712/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9232 - val_loss: 2.7537 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1713/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9232 - val_loss: 2.7538 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1714/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9232 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1715/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1716/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9232 - val_loss: 2.7563 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1717/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1718/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9232 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1719/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1720/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1721/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1722/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1723/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1724/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9232 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1725/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1726/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1727/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1728/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1729/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1730/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1731/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1732/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9865 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1733/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1734/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9840 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1735/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1736/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9854 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1737/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1738/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9850 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1739/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1740/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9232 - val_loss: 2.7557 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1741/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9232 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9232\n",
      "Epoch 1742/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9232 - val_loss: 2.7557 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1743/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1744/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0615 - accuracy: 0.9833 - f1_score: 0.9232 - val_loss: 2.7558 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1745/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9837 - f1_score: 0.9232 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1746/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9232 - val_loss: 2.7560 - val_accuracy: 0.7977 - val_f1_score: 0.9232\n",
      "Epoch 1747/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7560 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1748/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7559 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1749/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9824 - f1_score: 0.9233 - val_loss: 2.7559 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1750/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9233 - val_loss: 2.7558 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1751/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9233 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1752/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1753/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9852 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1754/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9233 - val_loss: 2.7541 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1755/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1756/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0568 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7535 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1757/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1758/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1759/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7537 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1760/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7535 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1761/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7533 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1762/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7535 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1763/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7535 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1764/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9233 - val_loss: 2.7534 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1765/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7536 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1766/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9233 - val_loss: 2.7537 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1767/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0608 - accuracy: 0.9833 - f1_score: 0.9233 - val_loss: 2.7538 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1768/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1769/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0568 - accuracy: 0.9854 - f1_score: 0.9233 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1770/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1771/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7540 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1772/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0589 - accuracy: 0.9825 - f1_score: 0.9233 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1773/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7539 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1774/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7549 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1775/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9827 - f1_score: 0.9233 - val_loss: 2.7538 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1776/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9233 - val_loss: 2.7584 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1777/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7567 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1778/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9233 - val_loss: 2.7549 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1779/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0575 - accuracy: 0.9840 - f1_score: 0.9233 - val_loss: 2.7547 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1780/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7547 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1781/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7551 - val_accuracy: 0.7977 - val_f1_score: 0.9233\n",
      "Epoch 1782/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9831 - f1_score: 0.9233 - val_loss: 2.7566 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1783/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1784/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1785/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9233 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1786/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1787/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9233 - val_loss: 2.7543 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1788/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9233 - val_loss: 2.7540 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1789/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9233 - val_loss: 2.7537 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1790/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7536 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1791/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7538 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1792/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9827 - f1_score: 0.9233 - val_loss: 2.7552 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1793/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9233 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1794/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0609 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1795/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9233 - val_loss: 2.7548 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1796/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9233 - val_loss: 2.7548 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1797/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9233 - val_loss: 2.7549 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1798/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9233 - val_loss: 2.7549 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1799/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9233 - val_loss: 2.7544 - val_accuracy: 0.7986 - val_f1_score: 0.9233\n",
      "Epoch 1800/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7543 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1801/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1802/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9234 - val_loss: 2.7568 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1803/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9234 - val_loss: 2.7548 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1804/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7566 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1805/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9825 - f1_score: 0.9234 - val_loss: 2.7568 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1806/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9234 - val_loss: 2.7567 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1807/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9234 - val_loss: 2.7567 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1808/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1809/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1810/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9234 - val_loss: 2.7568 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1811/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7569 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1812/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0524 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7567 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1813/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7567 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1814/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7566 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1815/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1816/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9816 - f1_score: 0.9234 - val_loss: 2.7564 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1817/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7563 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1818/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1819/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1820/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7543 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1821/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9833 - f1_score: 0.9234 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1822/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9234 - val_loss: 2.7537 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1823/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9831 - f1_score: 0.9234 - val_loss: 2.7538 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1824/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9234 - val_loss: 2.7536 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1825/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9829 - f1_score: 0.9234 - val_loss: 2.7537 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1826/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9234 - val_loss: 2.7538 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1827/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9859 - f1_score: 0.9234 - val_loss: 2.7540 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1828/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9234 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1829/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9848 - f1_score: 0.9234 - val_loss: 2.7544 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1830/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9234 - val_loss: 2.7542 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1831/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7542 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1832/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9812 - f1_score: 0.9234 - val_loss: 2.7547 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1833/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7545 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1834/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7547 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1835/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0605 - accuracy: 0.9844 - f1_score: 0.9234 - val_loss: 2.7563 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1836/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1837/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9863 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1838/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9846 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1839/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9825 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1840/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0608 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1841/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1842/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9234 - val_loss: 2.7541 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1843/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7986 - val_f1_score: 0.9234\n",
      "Epoch 1844/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1845/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1846/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1847/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1848/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1849/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1850/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7561 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1851/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1852/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9234 - val_loss: 2.7563 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1853/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9234 - val_loss: 2.7562 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1854/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9234 - val_loss: 2.7565 - val_accuracy: 0.7977 - val_f1_score: 0.9234\n",
      "Epoch 1855/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9234 - val_loss: 2.7564 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1856/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9829 - f1_score: 0.9235 - val_loss: 2.7566 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1857/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7565 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1858/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1859/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1860/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9831 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1861/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1862/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1863/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9235 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1864/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1865/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1866/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7610 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1867/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9833 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1868/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1869/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1870/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1871/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1872/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9235 - val_loss: 2.7609 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1873/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0601 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1874/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1875/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1876/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9852 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1877/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1878/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9855 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1879/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1880/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1881/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1882/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1883/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9833 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1884/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1885/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1886/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1887/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1888/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1889/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1890/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1891/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9855 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1892/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1893/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1894/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1895/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9833 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1896/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1897/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7590 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1898/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7590 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1899/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1900/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1901/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7589 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1902/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1903/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9235 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9235\n",
      "Epoch 1904/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7587 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1905/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7982 - val_f1_score: 0.9235\n",
      "Epoch 1906/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7990 - val_f1_score: 0.9235\n",
      "Epoch 1907/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9835 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1908/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9842 - f1_score: 0.9235 - val_loss: 2.7585 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1909/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1910/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0637 - accuracy: 0.9827 - f1_score: 0.9235 - val_loss: 2.7586 - val_accuracy: 0.7977 - val_f1_score: 0.9235\n",
      "Epoch 1911/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1912/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1913/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1914/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9827 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1915/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1916/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1917/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7572 - val_accuracy: 0.7982 - val_f1_score: 0.9236\n",
      "Epoch 1918/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9236 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1919/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9236 - val_loss: 2.7574 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1920/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9236 - val_loss: 2.7587 - val_accuracy: 0.7973 - val_f1_score: 0.9236\n",
      "Epoch 1921/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0571 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1922/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9827 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1923/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1924/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7577 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1925/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7982 - val_f1_score: 0.9236\n",
      "Epoch 1926/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7573 - val_accuracy: 0.7982 - val_f1_score: 0.9236\n",
      "Epoch 1927/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1928/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9835 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1929/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1930/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7586 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1931/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9236 - val_loss: 2.7587 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1932/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7977 - val_f1_score: 0.9236\n",
      "Epoch 1933/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1934/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1935/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1936/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1937/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1938/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9236 - val_loss: 2.7573 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1939/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7573 - val_accuracy: 0.7990 - val_f1_score: 0.9236\n",
      "Epoch 1940/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9236 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1941/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1942/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1943/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9859 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1944/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9859 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1945/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1946/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1947/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0638 - accuracy: 0.9835 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1948/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1949/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9236 - val_loss: 2.7556 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1950/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1951/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9236 - val_loss: 2.7588 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1952/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9236 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1953/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9839 - f1_score: 0.9236 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1954/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9831 - f1_score: 0.9236 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1955/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9831 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1956/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7575 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1957/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9236 - val_loss: 2.7571 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1958/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9236 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1959/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0587 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7569 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1960/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9854 - f1_score: 0.9236 - val_loss: 2.7570 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1961/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1962/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9236 - val_loss: 2.7611 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1963/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9236 - val_loss: 2.7610 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1964/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9236 - val_loss: 2.7611 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1965/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9236 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1966/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9236 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1967/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9859 - f1_score: 0.9236 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1968/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9236 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1969/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9840 - f1_score: 0.9236 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9236\n",
      "Epoch 1970/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1971/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1972/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7598 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1973/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9237 - val_loss: 2.7610 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1974/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7610 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1975/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9857 - f1_score: 0.9237 - val_loss: 2.7611 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1976/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1977/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0592 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1978/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9850 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1979/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1980/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1981/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1982/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1983/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1984/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7982 - val_f1_score: 0.9237\n",
      "Epoch 1985/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7982 - val_f1_score: 0.9237\n",
      "Epoch 1986/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1987/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9857 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1988/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7982 - val_f1_score: 0.9237\n",
      "Epoch 1989/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1990/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1991/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9237 - val_loss: 2.7574 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1992/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9237 - val_loss: 2.7574 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1993/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9237 - val_loss: 2.7574 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1994/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1995/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9237 - val_loss: 2.7575 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1996/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9825 - f1_score: 0.9237 - val_loss: 2.7584 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1997/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9237 - val_loss: 2.7578 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1998/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 1999/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2000/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2001/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2002/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7593 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2003/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9831 - f1_score: 0.9237 - val_loss: 2.7594 - val_accuracy: 0.7990 - val_f1_score: 0.9237\n",
      "Epoch 2004/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9237 - val_loss: 2.7594 - val_accuracy: 0.7990 - val_f1_score: 0.9237\n",
      "Epoch 2005/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9844 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2006/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2007/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9833 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2008/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7594 - val_accuracy: 0.7990 - val_f1_score: 0.9237\n",
      "Epoch 2009/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2010/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9848 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2011/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2012/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2013/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2014/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2015/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2016/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2017/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2018/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9816 - f1_score: 0.9237 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2019/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2020/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2021/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2022/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2023/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2024/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2025/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9237 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2026/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9827 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2027/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9237 - val_loss: 2.7612 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2028/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7595 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2029/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2030/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0577 - accuracy: 0.9844 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2031/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2032/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9237 - val_loss: 2.7592 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2033/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9237 - val_loss: 2.7593 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2034/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9237 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9237\n",
      "Epoch 2035/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9237 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2036/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2037/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2038/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2039/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2040/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9837 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2041/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2042/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2043/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2044/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7589 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2045/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2046/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2047/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2048/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2049/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2050/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9852 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2051/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2052/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2053/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2054/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2055/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2056/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2057/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2058/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9844 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2059/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2060/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2061/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2062/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2063/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2064/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2065/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2066/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2067/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2068/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2069/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9831 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2070/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2071/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7590 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2072/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2073/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2074/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9238 - val_loss: 2.7592 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2075/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2076/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7591 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2077/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9831 - f1_score: 0.9238 - val_loss: 2.7595 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2078/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9238 - val_loss: 2.7604 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2079/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7597 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2080/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0567 - accuracy: 0.9854 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2081/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2082/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2083/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9833 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2084/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2085/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7612 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2086/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0523 - accuracy: 0.9844 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2087/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2088/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9857 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2089/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2090/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2091/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9238 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2092/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0572 - accuracy: 0.9852 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2093/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2094/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2095/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2096/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2097/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7977 - val_f1_score: 0.9238\n",
      "Epoch 2098/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2099/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2100/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2101/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2102/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9238\n",
      "Epoch 2103/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0576 - accuracy: 0.9839 - f1_score: 0.9238 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2104/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2105/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2106/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2107/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2108/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2109/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2110/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9833 - f1_score: 0.9239 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2111/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2112/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2113/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2114/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0610 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2115/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2116/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2117/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2118/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2119/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2120/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2121/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2122/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9239 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2123/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2124/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2125/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9239 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2126/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9822 - f1_score: 0.9239 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2127/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9239 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2128/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2129/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2130/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2131/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9859 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2132/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0575 - accuracy: 0.9833 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2133/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2134/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2135/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2136/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9825 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2137/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2138/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2139/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2140/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9825 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2141/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2142/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2143/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2144/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2145/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9854 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2146/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2147/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2148/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2149/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2150/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2151/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2152/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2153/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2154/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2155/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2156/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2157/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2158/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2159/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2160/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2161/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2162/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2163/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2164/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2165/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2166/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2167/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2168/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2169/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2170/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2171/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2172/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2173/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2174/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9239\n",
      "Epoch 2175/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9239 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2176/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2177/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2178/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9240 - val_loss: 2.7599 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2179/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7605 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2180/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2181/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2182/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2183/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2184/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2185/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2186/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2187/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2188/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2189/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0607 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2190/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2191/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2192/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2193/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2194/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2195/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2196/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2197/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9824 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2198/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9833 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2199/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2200/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2201/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2202/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2203/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2204/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9240 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2205/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9857 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2206/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2207/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2208/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2209/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7602 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2210/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7602 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2211/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2212/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2213/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9240 - val_loss: 2.7602 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2214/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2215/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7604 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2216/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2217/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2218/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9240 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2219/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2220/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2221/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9854 - f1_score: 0.9240 - val_loss: 2.7604 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2222/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9240 - val_loss: 2.7605 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2223/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2224/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2225/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2226/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2227/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2228/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7603 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2229/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2230/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9852 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2231/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2232/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7602 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2233/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2234/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0593 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2235/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7599 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2236/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2237/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2238/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2239/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2240/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2241/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0575 - accuracy: 0.9829 - f1_score: 0.9240 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2242/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9240 - val_loss: 2.7599 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2243/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9855 - f1_score: 0.9240 - val_loss: 2.7605 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2244/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2245/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9240 - val_loss: 2.7601 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2246/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9240 - val_loss: 2.7599 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2247/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9240 - val_loss: 2.7597 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2248/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9240 - val_loss: 2.7596 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2249/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9240 - val_loss: 2.7597 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2250/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7596 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2251/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9240 - val_loss: 2.7597 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2252/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9240 - val_loss: 2.7597 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2253/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9240 - val_loss: 2.7597 - val_accuracy: 0.7986 - val_f1_score: 0.9240\n",
      "Epoch 2254/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2255/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2256/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9859 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2257/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2258/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2259/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2260/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2261/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7603 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2262/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7600 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2263/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2264/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2265/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2266/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2267/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2268/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2269/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2270/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0548 - accuracy: 0.9850 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2271/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7599 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2272/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2273/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2274/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2275/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2276/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2277/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2278/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2279/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2280/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2281/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2282/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2283/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2284/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2285/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0549 - accuracy: 0.9852 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2286/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9825 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2287/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2288/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2289/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2290/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9824 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2291/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2292/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2293/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2294/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2295/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9842 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2296/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0564 - accuracy: 0.9831 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2297/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2298/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2299/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2300/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9825 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2301/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2302/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2303/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9844 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2304/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2305/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2306/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2307/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9850 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2308/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2309/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2310/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2311/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2312/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2313/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2314/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2315/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2316/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2317/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2318/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2319/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2320/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2321/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2322/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2323/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2324/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2325/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2326/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2327/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2328/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2329/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2330/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2331/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2332/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9241 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2333/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9846 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2334/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2335/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9241\n",
      "Epoch 2336/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9241 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2337/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2338/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2339/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2340/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2341/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9852 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2342/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2343/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2344/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2345/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2346/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2347/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2348/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9242 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2349/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2350/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9242 - val_loss: 2.7635 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2351/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7622 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2352/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2353/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2354/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2355/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2356/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2357/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2358/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2359/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7636 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2360/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9242 - val_loss: 2.7623 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2361/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9854 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2362/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2363/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2364/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9859 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2365/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2366/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9859 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2367/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2368/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9829 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2369/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2370/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2371/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0549 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2372/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9825 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2373/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2374/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9825 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2375/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2376/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7622 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2377/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2378/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2379/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9242 - val_loss: 2.7624 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2380/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7622 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2381/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2382/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2383/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0564 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2384/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2385/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0568 - accuracy: 0.9852 - f1_score: 0.9242 - val_loss: 2.7623 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2386/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2387/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2388/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2389/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2390/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2391/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2392/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9831 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2393/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2394/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2395/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2396/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2397/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0608 - accuracy: 0.9844 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2398/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0531 - accuracy: 0.9844 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2399/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2400/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2401/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2402/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2403/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7622 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2404/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7635 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2405/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9831 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2406/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2407/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9854 - f1_score: 0.9242 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2408/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2409/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0574 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7620 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2410/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9859 - f1_score: 0.9242 - val_loss: 2.7621 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2411/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7635 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2412/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9242 - val_loss: 2.7624 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2413/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7622 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2414/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7619 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2415/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9242 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2416/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2417/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0589 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2418/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2419/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9242 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2420/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9242 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2421/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2422/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9829 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2423/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0608 - accuracy: 0.9852 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2424/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2425/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0567 - accuracy: 0.9829 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9242\n",
      "Epoch 2426/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9242 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2427/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9859 - f1_score: 0.9243 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2428/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2429/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9859 - f1_score: 0.9243 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2430/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2431/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2432/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2433/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9857 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2434/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2435/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2436/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2437/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2438/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2439/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2440/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2441/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9825 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2442/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2443/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2444/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9825 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2445/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2446/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2447/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2448/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2449/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2450/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2451/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9831 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2452/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9822 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2453/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2454/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2455/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9831 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2456/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2457/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2458/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2459/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9822 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2460/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2461/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2462/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0604 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2463/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2464/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2465/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9855 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2466/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0570 - accuracy: 0.9835 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2467/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0614 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2468/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2469/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2470/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0565 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2471/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2472/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2473/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2474/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0564 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2475/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0601 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2476/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2477/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2478/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2479/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9852 - f1_score: 0.9243 - val_loss: 2.7613 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2480/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2481/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2482/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9833 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2483/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2484/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2485/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9859 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2486/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2487/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9833 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2488/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2489/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2490/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2491/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0587 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2492/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2493/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2494/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2495/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2496/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2497/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2498/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2499/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2500/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2501/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2502/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9850 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2503/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2504/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2505/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2506/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2507/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2508/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9859 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2509/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2510/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2511/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9827 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2512/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2513/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2514/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9835 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2515/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2516/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2517/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9243 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2518/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2519/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9854 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9243\n",
      "Epoch 2520/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9846 - f1_score: 0.9243 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2521/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2522/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2523/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2524/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9857 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2525/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2526/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2527/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2528/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2529/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9854 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2530/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2531/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2532/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2533/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2534/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2535/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2536/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2537/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2538/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2539/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2540/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9825 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2541/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2542/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2543/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2544/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2545/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2546/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2547/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2548/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2549/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2550/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2551/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2552/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2553/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2554/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2555/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2556/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2557/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2558/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2559/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2560/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0547 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2561/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2562/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2563/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2564/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2565/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2566/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0571 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2567/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2568/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2569/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9825 - f1_score: 0.9244 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2570/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2571/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2572/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2573/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2574/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2575/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0616 - accuracy: 0.9827 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2576/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2577/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0530 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2578/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2579/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2580/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2581/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2582/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2583/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2584/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2585/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2586/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2587/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2588/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2589/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2590/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2591/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2592/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2593/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2594/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2595/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2596/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9244 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2597/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2598/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0606 - accuracy: 0.9839 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2599/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2600/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0591 - accuracy: 0.9829 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2601/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2602/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2603/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0541 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2604/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0525 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2605/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2606/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2607/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2608/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9850 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2609/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2610/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2611/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2612/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2613/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2614/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2615/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2616/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2617/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0575 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2618/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2619/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9852 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2620/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2621/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9244 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9244\n",
      "Epoch 2622/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9244 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2623/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2624/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2625/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2626/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9855 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2627/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2628/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2629/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2630/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9855 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2631/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2632/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2633/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9827 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2634/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2635/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2636/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2637/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2638/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2639/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2640/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2641/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0557 - accuracy: 0.9855 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2642/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2643/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2644/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2645/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7618 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2646/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2647/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2648/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2649/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2650/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2651/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9852 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2652/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2653/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2654/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2655/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7617 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2656/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2657/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2658/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2659/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2660/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2661/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2662/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2663/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2664/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2665/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2666/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2667/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2668/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2669/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2670/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9854 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2671/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9824 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2672/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2673/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2674/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2675/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2676/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2677/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2678/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2679/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2680/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2681/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2682/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2683/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0568 - accuracy: 0.9854 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2684/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2685/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2686/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0575 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2687/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2688/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2689/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2690/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2691/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2692/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9827 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2693/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9825 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2694/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2695/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2696/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2697/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2698/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2699/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2700/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2701/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2702/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2703/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9829 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2704/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2705/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2706/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2707/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9857 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2708/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2709/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2710/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2711/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2712/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2713/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2714/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9855 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2715/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2716/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0564 - accuracy: 0.9829 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2717/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0607 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2718/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2719/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0629 - accuracy: 0.9844 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2720/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2721/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2722/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2723/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2724/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9850 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2725/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2726/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2727/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9855 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2728/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2729/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2730/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2731/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2732/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2733/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9831 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2734/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2735/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0606 - accuracy: 0.9833 - f1_score: 0.9245 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9245\n",
      "Epoch 2736/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2737/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2738/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2739/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2740/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2741/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0549 - accuracy: 0.9835 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2742/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2743/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2744/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2745/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2746/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2747/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2748/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9831 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2749/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2750/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2751/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9822 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2752/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2753/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2754/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0529 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2755/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2756/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2757/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2758/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2759/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2760/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2761/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2762/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2763/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2764/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7616 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2765/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0566 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2766/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2767/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2768/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2769/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2770/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2771/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2772/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2773/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2774/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0562 - accuracy: 0.9825 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2775/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2776/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2777/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2778/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9827 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2779/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2780/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2781/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2782/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2783/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2784/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9825 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2785/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9822 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2786/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2787/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0611 - accuracy: 0.9829 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2788/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0597 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2789/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2790/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2791/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2792/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2793/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2794/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2795/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2796/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2797/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2798/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9857 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2799/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2800/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2801/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2802/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2803/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2804/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2805/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0543 - accuracy: 0.9835 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2806/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2807/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9859 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2808/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2809/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0575 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2810/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2811/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2812/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0532 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2813/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2814/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9831 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2815/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2816/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2817/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2818/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0577 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2819/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7615 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2820/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0571 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2821/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2822/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2823/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2824/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2825/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9824 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2826/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2827/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2828/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2829/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0572 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2830/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0589 - accuracy: 0.9829 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2831/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2832/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9848 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2833/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2834/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9827 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2835/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2836/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2837/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2838/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2839/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2840/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0598 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2841/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2842/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9827 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2843/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2844/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0574 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2845/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0568 - accuracy: 0.9816 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2846/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2847/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9861 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2848/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0607 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2849/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2850/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2851/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0579 - accuracy: 0.9854 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2852/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2853/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2854/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2855/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2856/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2857/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2858/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0562 - accuracy: 0.9833 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2859/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9842 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9246\n",
      "Epoch 2860/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9246 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2861/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2862/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2863/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2864/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2865/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2866/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0577 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2867/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2868/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2869/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2870/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2871/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2872/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9859 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2873/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9857 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2874/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0593 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2875/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2876/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2877/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2878/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2879/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2880/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0606 - accuracy: 0.9855 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2881/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2882/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2883/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2884/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2885/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2886/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2887/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2888/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9854 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2889/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9855 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2890/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2891/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2892/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2893/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2894/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2895/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2896/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2897/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2898/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2899/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2900/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2901/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2902/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2903/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2904/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9859 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2905/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2906/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2907/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2908/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2909/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2910/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2911/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2912/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2913/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2914/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2915/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2916/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2917/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2918/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2919/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2920/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2921/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2922/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2923/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2924/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2925/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2926/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2927/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2928/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2929/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2930/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2931/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2932/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2933/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9857 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2934/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2935/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2936/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2937/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2938/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2939/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2940/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2941/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2942/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2943/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2944/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2945/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0601 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2946/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2947/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2948/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2949/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2950/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2951/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9829 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2952/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9857 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2953/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2954/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2955/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0523 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2956/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2957/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2958/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0586 - accuracy: 0.9833 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2959/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2960/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2961/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9825 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2962/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2963/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2964/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2965/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2966/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0576 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2967/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9850 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2968/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2969/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2970/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2971/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2972/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0609 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2973/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2974/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9829 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2975/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2976/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2977/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2978/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0582 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2979/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2980/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2981/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2982/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2983/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2984/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2985/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2986/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2987/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2988/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2989/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2990/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2991/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9846 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2992/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9848 - f1_score: 0.9247 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9247\n",
      "Epoch 2993/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2994/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2995/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2996/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2997/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2998/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 2999/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3000/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0548 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3001/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3002/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3003/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3004/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9852 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3005/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3006/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3007/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9827 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3008/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3009/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3010/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3011/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3012/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0567 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3013/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3014/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9855 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3015/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0526 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3016/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9827 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3017/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3018/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3019/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3020/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9852 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3021/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3022/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3023/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9824 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3024/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3025/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3026/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3027/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3028/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3029/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3030/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3031/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3032/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3033/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3034/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3035/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3036/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9824 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3037/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9820 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3038/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3039/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3040/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9833 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3041/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9855 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3042/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3043/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3044/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3045/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3046/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3047/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3048/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3049/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3050/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3051/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9859 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3052/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3053/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3054/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3055/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0572 - accuracy: 0.9855 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3056/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3057/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3058/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3059/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3060/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3061/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3062/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3063/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0587 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3064/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3065/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3066/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3067/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3068/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3069/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3070/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3071/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3072/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3073/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0616 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3074/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3075/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3076/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3077/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3078/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3079/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3080/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3081/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3082/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3083/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3084/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3085/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0612 - accuracy: 0.9833 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3086/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3087/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3088/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3089/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3090/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3091/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3092/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3093/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0589 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3094/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3095/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3096/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3097/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3098/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9857 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3099/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3100/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3101/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3102/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3103/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3104/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3105/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3106/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3107/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3108/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3109/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3110/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3111/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3112/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3113/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3114/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3115/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3116/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3117/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0565 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3118/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3119/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3120/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9857 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3121/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3122/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3123/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3124/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3125/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3126/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3127/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3128/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3129/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3130/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3131/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3132/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3133/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3134/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9248 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9248\n",
      "Epoch 3135/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3136/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3137/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0570 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3138/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3139/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3140/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3141/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0566 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3142/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0609 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3143/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3144/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3145/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3146/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9831 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3147/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3148/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0538 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3149/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3150/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3151/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3152/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3153/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3154/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3155/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3156/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3157/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3158/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3159/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3160/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3161/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0529 - accuracy: 0.9824 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3162/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9824 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3163/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3164/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3165/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3166/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9816 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3167/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3168/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3169/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3170/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3171/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3172/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3173/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3174/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3175/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3176/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3177/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3178/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3179/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3180/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3181/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0609 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3182/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3183/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3184/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3185/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3186/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0572 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3187/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3188/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3189/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3190/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3191/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3192/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3193/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3194/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3195/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3196/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3197/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3198/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3199/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3200/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0525 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3201/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3202/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3203/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3204/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3205/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3206/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3207/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3208/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3209/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9827 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3210/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0593 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3211/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3212/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3213/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3214/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3215/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3216/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3217/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3218/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3219/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0606 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3220/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3221/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3222/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3223/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3224/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0614 - accuracy: 0.9825 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3225/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3226/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3227/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3228/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3229/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3230/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0551 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3231/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3232/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3233/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0571 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3234/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9861 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3235/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3236/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3237/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9854 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3238/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3239/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9818 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3240/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9827 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3241/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3242/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9859 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3243/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3244/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3245/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9857 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3246/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3247/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3248/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3249/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3250/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3251/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9820 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3252/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3253/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3254/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3255/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3256/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3257/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9825 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3258/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3259/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9861 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3260/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3261/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3262/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3263/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3264/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3265/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3266/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3267/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3268/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3269/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3270/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3271/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3272/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3273/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3274/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3275/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3276/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3277/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9857 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3278/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3279/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3280/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3281/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3282/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3283/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3284/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3285/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0608 - accuracy: 0.9852 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3286/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3287/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3288/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3289/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0613 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3290/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3291/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3292/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3293/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3294/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9827 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3295/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9249 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9249\n",
      "Epoch 3296/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9855 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3297/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3298/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3299/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3300/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0568 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3301/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3302/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3303/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3304/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3305/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3306/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3307/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3308/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3309/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3310/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3311/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3312/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3313/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3314/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3315/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3316/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3317/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3318/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3319/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3320/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3321/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3322/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3323/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0576 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3324/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3325/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3326/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3327/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3328/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3329/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3330/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9827 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3331/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3332/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3333/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3334/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3335/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3336/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3337/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0570 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3338/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3339/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3340/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3341/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3342/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3343/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3344/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0597 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3345/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3346/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3347/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3348/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3349/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3350/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3351/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3352/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0547 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3353/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3354/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0564 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3355/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3356/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3357/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3358/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3359/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3360/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3361/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3362/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3363/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3364/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3365/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3366/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9863 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3367/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3368/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3369/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3370/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3371/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3372/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0575 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3373/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3374/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0531 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3375/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3376/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3377/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3378/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3379/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3380/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3381/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3382/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9865 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3383/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3384/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3385/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3386/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3387/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3388/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3389/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3390/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3391/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3392/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9824 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3393/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3394/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3395/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3396/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3397/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3398/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3399/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3400/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3401/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3402/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3403/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3404/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3405/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3406/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3407/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3408/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3409/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3410/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3411/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3412/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3413/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3414/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9859 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3415/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3416/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3417/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3418/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3419/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3420/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3421/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0563 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3422/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9829 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3423/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3424/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3425/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3426/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3427/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3428/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3429/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3430/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3431/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3432/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3433/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3434/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3435/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3436/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3437/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3438/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3439/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3440/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3441/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3442/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3443/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3444/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3445/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3446/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3447/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3448/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3449/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3450/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3451/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3452/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3453/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3454/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3455/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3456/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0610 - accuracy: 0.9837 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3457/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3458/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3459/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3460/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3461/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3462/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3463/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3464/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0561 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3465/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9835 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3466/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3467/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3468/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3469/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3470/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3471/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3472/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3473/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9250 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9250\n",
      "Epoch 3474/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3475/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3476/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3477/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3478/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3479/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0594 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3480/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3481/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3482/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9861 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3483/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9863 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3484/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3485/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3486/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3487/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3488/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3489/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3490/5000\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3491/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3492/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3493/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3494/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3495/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3496/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3497/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0545 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3498/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3499/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3500/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3501/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3502/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0575 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3503/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3504/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3505/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9827 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3506/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3507/5000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3508/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3509/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3510/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3511/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3512/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3513/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3514/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3515/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3516/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3517/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3518/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3519/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3520/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3521/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3522/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9859 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3523/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9824 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3524/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3525/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3526/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3527/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0615 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3528/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9824 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3529/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3530/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3531/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9861 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3532/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3533/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3534/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3535/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3536/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3537/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3538/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3539/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3540/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3541/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3542/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3543/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3544/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3545/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3546/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3547/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3548/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3549/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3550/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3551/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3552/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3553/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9857 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3554/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3555/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0532 - accuracy: 0.9850 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3556/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3557/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3558/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3559/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3560/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3561/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3562/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3563/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3564/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3565/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3566/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3567/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3568/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3569/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3570/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3571/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3572/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3573/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3574/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3575/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3576/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3577/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3578/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3579/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3580/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3581/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3582/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3583/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3584/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0627 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3585/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3586/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3587/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3588/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3589/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3590/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3591/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3592/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3593/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0605 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3594/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3595/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3596/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0569 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3597/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3598/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3599/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3600/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3601/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0589 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3602/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3603/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3604/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0566 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3605/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3606/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3607/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3608/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3609/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9855 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3610/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3611/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3612/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3613/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3614/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3615/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3616/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0602 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3617/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3618/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3619/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3620/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3621/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3622/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0589 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3623/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0565 - accuracy: 0.9855 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3624/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9850 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3625/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3626/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3627/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3628/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3629/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3630/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3631/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0619 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3632/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3633/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3634/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3635/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9859 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3636/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3637/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9859 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3638/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3639/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3640/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3641/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3642/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9863 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3643/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9859 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3644/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3645/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3646/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3647/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9827 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3648/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3649/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3650/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3651/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9827 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3652/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9822 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3653/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9861 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3654/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9833 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3655/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3656/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3657/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3658/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3659/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9829 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3660/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3661/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3662/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0590 - accuracy: 0.9829 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3663/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3664/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3665/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3666/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0568 - accuracy: 0.9855 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3667/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3668/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3669/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3670/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9251\n",
      "Epoch 3671/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9251 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3672/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3673/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0615 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3674/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3675/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3676/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3677/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3678/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0611 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3679/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3680/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9827 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3681/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3682/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3683/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3684/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3685/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3686/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3687/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3688/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3689/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3690/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3691/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3692/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0547 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3693/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3694/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9827 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3695/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3696/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3697/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3698/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9854 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3699/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3700/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3701/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3702/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3703/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3704/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3705/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3706/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3707/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3708/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3709/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3710/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3711/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3712/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3713/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3714/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3715/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3716/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3717/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3718/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9825 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3719/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3720/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3721/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3722/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3723/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3724/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3725/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3726/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3727/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3728/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3729/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3730/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3731/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3732/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0591 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3733/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3734/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3735/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3736/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3737/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3738/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3739/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3740/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3741/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3742/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3743/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0575 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3744/5000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0590 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3745/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3746/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0563 - accuracy: 0.9824 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3747/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3748/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3749/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3750/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3751/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3752/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3753/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3754/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3755/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3756/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3757/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3758/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3759/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3760/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9825 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3761/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3762/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3763/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3764/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3765/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3766/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3767/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3768/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3769/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3770/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3771/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3772/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3773/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3774/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3775/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3776/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3777/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3778/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3779/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3780/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3781/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3782/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3783/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3784/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3785/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3786/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3787/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3788/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3789/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3790/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3791/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3792/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3793/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0610 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3794/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3795/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3796/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3797/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3798/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3799/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3800/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3801/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9855 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3802/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3803/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3804/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3805/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3806/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3807/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9824 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3808/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3809/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3810/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3811/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3812/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3813/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3814/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3815/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3816/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3817/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3818/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3819/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3820/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3821/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3822/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3823/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3824/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3825/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3826/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3827/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3828/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3829/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3830/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3831/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3832/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3833/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0608 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3834/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3835/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3836/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3837/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9827 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3838/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3839/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3840/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3841/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3842/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3843/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3844/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3845/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3846/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3847/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3848/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3849/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3850/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3851/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9820 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3852/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3853/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3854/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3855/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3856/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3857/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3858/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3859/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3860/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3861/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3862/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3863/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3864/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3865/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3866/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3867/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3868/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3869/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9833 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3870/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3871/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9855 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3872/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9824 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3873/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3874/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3875/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9827 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3876/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3877/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0614 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3878/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3879/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3880/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3881/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0614 - accuracy: 0.9829 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3882/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3883/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3884/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0529 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3885/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3886/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3887/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0570 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3888/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3889/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0550 - accuracy: 0.9861 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3890/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3891/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3892/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3893/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9850 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3894/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3895/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3896/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3897/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3898/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9252 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9252\n",
      "Epoch 3899/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0588 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3900/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3901/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3902/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3903/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3904/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0632 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3905/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3906/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3907/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3908/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3909/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3910/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3911/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3912/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3913/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3914/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3915/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3916/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3917/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3918/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3919/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3920/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0596 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3921/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3922/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3923/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3924/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3925/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3926/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3927/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3928/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3929/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3930/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3931/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3932/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0579 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3933/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0587 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3934/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3935/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3936/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0588 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3937/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3938/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0597 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3939/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3940/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3941/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9822 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3942/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3943/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3944/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3945/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3946/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3947/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3948/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3949/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3950/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3951/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3952/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3953/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3954/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3955/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3956/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3957/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3958/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3959/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3960/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3961/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3962/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0566 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3963/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3964/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3965/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3966/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9855 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3967/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0614 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3968/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3969/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3970/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3971/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3972/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3973/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0566 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3974/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3975/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3976/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3977/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3978/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3979/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3980/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3981/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3982/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3983/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0610 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3984/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3985/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3986/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3987/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3988/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3989/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3990/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9827 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3991/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9825 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3992/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3993/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3994/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0531 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3995/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3996/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3997/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3998/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 3999/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4000/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4001/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4002/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4003/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9825 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4004/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4005/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4006/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4007/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9825 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4008/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4009/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4010/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4011/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0600 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4012/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4013/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4014/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0605 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4015/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4016/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4017/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4018/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4019/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4020/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4021/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4022/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4023/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0574 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4024/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4025/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9827 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4026/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4027/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4028/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4029/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4030/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4031/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4032/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4033/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4034/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4035/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0615 - accuracy: 0.9824 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4036/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4037/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4038/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4039/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4040/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4041/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4042/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4043/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4044/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4045/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4046/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4047/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0575 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4048/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4049/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4050/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4051/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4052/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4053/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9822 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4054/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4055/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4056/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9863 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4057/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9854 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4058/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4059/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4060/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4061/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4062/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4063/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4064/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4065/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4066/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4067/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4068/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4069/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4070/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0609 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4071/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4072/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4073/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4074/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4075/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4076/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4077/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4078/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4079/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4080/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9827 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4081/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4082/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4083/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4084/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9824 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4085/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4086/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4087/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4088/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4089/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4090/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4091/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4092/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4093/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4094/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4095/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4096/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0571 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4097/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4098/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4099/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0576 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4100/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4101/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4102/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4103/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4104/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4105/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4106/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4107/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4108/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9827 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4109/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4110/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4111/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4112/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4113/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4114/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4115/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0557 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4116/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4117/5000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4118/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0583 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4119/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0548 - accuracy: 0.9857 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4120/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0587 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4121/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4122/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4123/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4124/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4125/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4126/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4127/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4128/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4129/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4130/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4131/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4132/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4133/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9863 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4134/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9825 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4135/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4136/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4137/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4138/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4139/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4140/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9863 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4141/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4142/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0588 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4143/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4144/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4145/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4146/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4147/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4148/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4149/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4150/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4151/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4152/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4153/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4154/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4155/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0569 - accuracy: 0.9846 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4156/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4157/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0607 - accuracy: 0.9842 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4158/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9827 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4159/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4160/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4161/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4162/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0578 - accuracy: 0.9831 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4163/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4164/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4165/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0565 - accuracy: 0.9840 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4166/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9253 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9253\n",
      "Epoch 4167/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4168/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4169/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4170/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9865 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4171/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4172/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4173/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4174/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4175/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4176/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4177/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4178/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4179/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4180/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4181/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4182/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4183/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4184/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4185/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4186/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4187/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4188/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4189/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4190/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4191/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9855 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4192/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4193/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4194/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4195/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4196/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4197/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9816 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4198/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4199/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4200/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4201/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0603 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4202/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4203/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4204/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4205/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4206/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4207/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4208/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0562 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4209/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0579 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4210/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4211/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4212/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4213/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4214/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0617 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4215/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9857 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4216/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4217/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4218/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4219/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4220/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4221/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0565 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4222/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4223/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4224/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0609 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4225/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4226/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0590 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4227/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4228/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4229/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4230/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0609 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4231/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4232/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4233/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4234/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4235/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4236/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4237/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0591 - accuracy: 0.9827 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4238/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4239/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4240/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4241/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4242/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4243/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4244/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9824 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4245/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4246/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4247/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4248/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4249/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4250/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4251/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4252/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0549 - accuracy: 0.9857 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4253/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4254/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4255/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4256/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0570 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4257/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0607 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4258/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4259/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4260/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0609 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4261/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0586 - accuracy: 0.9818 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4262/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4263/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4264/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4265/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4266/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0587 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4267/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4268/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4269/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4270/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4271/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4272/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4273/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4274/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4275/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4276/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9827 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4277/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0612 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4278/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4279/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4280/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4281/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4282/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4283/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4284/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4285/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4286/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4287/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0611 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4288/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0568 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4289/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4290/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4291/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4292/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4293/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4294/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4295/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4296/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4297/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4298/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4299/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4300/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0618 - accuracy: 0.9825 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4301/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0551 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4302/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4303/5000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4304/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4305/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4306/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4307/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0577 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4308/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0600 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4309/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4310/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4311/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4312/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9857 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4313/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4314/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9827 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4315/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4316/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4317/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4318/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4319/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4320/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4321/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4322/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0572 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4323/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4324/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9865 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4325/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4326/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4327/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4328/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4329/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4330/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4331/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4332/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4333/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4334/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4335/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4336/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4337/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4338/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4339/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4340/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4341/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4342/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0638 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4343/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4344/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0561 - accuracy: 0.9822 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4345/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4346/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4347/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4348/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4349/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4350/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4351/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4352/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4353/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0582 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4354/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0597 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4355/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4356/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4357/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4358/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4359/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4360/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4361/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4362/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4363/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9827 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4364/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4365/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9865 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4366/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4367/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4368/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4369/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4370/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4371/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4372/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4373/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4374/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4375/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0602 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4376/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9859 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4377/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4378/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4379/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4380/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4381/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4382/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4383/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4384/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4385/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9859 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4386/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4387/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4388/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4389/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4390/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4391/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4392/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4393/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4394/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4395/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4396/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4397/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4398/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4399/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0599 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4400/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4401/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9824 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4402/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4403/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4404/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4405/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4406/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0560 - accuracy: 0.9822 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4407/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4408/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4409/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4410/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4411/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4412/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4413/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4414/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4415/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4416/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4417/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4418/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0565 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4419/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0526 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4420/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4421/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4422/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4423/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4424/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4425/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4426/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0640 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4427/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4428/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4429/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4430/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4431/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4432/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0556 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4433/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4434/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4435/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0587 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4436/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0609 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4437/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4438/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4439/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4440/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4441/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4442/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4443/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0581 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4444/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9855 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4445/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9822 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4446/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4447/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4448/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4449/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9850 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4450/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4451/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4452/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4453/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4454/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4455/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4456/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4457/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4458/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4459/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4460/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4461/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4462/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4463/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4464/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4465/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9254\n",
      "Epoch 4466/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9254 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4467/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0590 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4468/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0575 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4469/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4470/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4471/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4472/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4473/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4474/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4475/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4476/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4477/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4478/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4479/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4480/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4481/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4482/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4483/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0566 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4484/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4485/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4486/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0609 - accuracy: 0.9825 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4487/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4488/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4489/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4490/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0630 - accuracy: 0.9818 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4491/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4492/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4493/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4494/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0583 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4495/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4496/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0549 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4497/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4498/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4499/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4500/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0598 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4501/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9822 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4502/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4503/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4504/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0573 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4505/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4506/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4507/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4508/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4509/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4510/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4511/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0585 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4512/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4513/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4514/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4515/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4516/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4517/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4518/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4519/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4520/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4521/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4522/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4523/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4524/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4525/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4526/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4527/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4528/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9857 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4529/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4530/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0570 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4531/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4532/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4533/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4534/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4535/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4536/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4537/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4538/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0610 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4539/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4540/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4541/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4542/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4543/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4544/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4545/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0605 - accuracy: 0.9857 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4546/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4547/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0577 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4548/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4549/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9859 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4550/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4551/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4552/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4553/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4554/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4555/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4556/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4557/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4558/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4559/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4560/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4561/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9824 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4562/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4563/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4564/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4565/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4566/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4567/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0562 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4568/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4569/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4570/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0592 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4571/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9825 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4572/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0588 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4573/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4574/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4575/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4576/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4577/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9825 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4578/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4579/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4580/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4581/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4582/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4583/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4584/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0589 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4585/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4586/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4587/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4588/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4589/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4590/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4591/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4592/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4593/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4594/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9863 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4595/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4596/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4597/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4598/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4599/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4600/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4601/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4602/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4603/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4604/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4605/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4606/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4607/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4608/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4609/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4610/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4611/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4612/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0606 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4613/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4614/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0606 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4615/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4616/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4617/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4618/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4619/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4620/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4621/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4622/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4623/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4624/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4625/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4626/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4627/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4628/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4629/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9857 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4630/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0570 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4631/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4632/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4633/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0564 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4634/5000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0584 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4635/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4636/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0576 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4637/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0577 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4638/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4639/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4640/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0611 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4641/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4642/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4643/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9820 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4644/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4645/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4646/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4647/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4648/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4649/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4650/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4651/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4652/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4653/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4654/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4655/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4656/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4657/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4658/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0563 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4659/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4660/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9857 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4661/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0588 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4662/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4663/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0586 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4664/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9865 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4665/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4666/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4667/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4668/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0608 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4669/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4670/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4671/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4672/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4673/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4674/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4675/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0583 - accuracy: 0.9859 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4676/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4677/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0579 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4678/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9827 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4679/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0577 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4680/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4681/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4682/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4683/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4684/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4685/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0581 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4686/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4687/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4688/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0583 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4689/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4690/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4691/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0590 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4692/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0615 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4693/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0570 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4694/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4695/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4696/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4697/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4698/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4699/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4700/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4701/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4702/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4703/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4704/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0564 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4705/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0561 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4706/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4707/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4708/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4709/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9824 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4710/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4711/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4712/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0614 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4713/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0576 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4714/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0577 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4715/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4716/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9831 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4717/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4718/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4719/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4720/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4721/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4722/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4723/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4724/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4725/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4726/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4727/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0560 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4728/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0577 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4729/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0577 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4730/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0567 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4731/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4732/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4733/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4734/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4735/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4736/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0586 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4737/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4738/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4739/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4740/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0581 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4741/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4742/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4743/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0603 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4744/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4745/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4746/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0572 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4747/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4748/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4749/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4750/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0549 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4751/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9825 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4752/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4753/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4754/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4755/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4756/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4757/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0584 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4758/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4759/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4760/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4761/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4762/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4763/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4764/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0550 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4765/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4766/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4767/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4768/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4769/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4770/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4771/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4772/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9820 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4773/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4774/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4775/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4776/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4777/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4778/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0561 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4779/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4780/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4781/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4782/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4783/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4784/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0610 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4785/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4786/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0574 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4787/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4788/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4789/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4790/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4791/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4792/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4793/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0559 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4794/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9852 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4795/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0562 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4796/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4797/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0579 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4798/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9854 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4799/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9855 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4800/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4801/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4802/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4803/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4804/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4805/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0645 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4806/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0610 - accuracy: 0.9846 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4807/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4808/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0583 - accuracy: 0.9835 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4809/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4810/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4811/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0577 - accuracy: 0.9850 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4812/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4813/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4814/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9833 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4815/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9255\n",
      "Epoch 4816/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4817/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9255 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4818/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9827 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4819/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4820/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4821/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4822/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0587 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4823/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4824/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4825/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0592 - accuracy: 0.9820 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4826/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4827/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0556 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4828/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0552 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4829/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4830/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0608 - accuracy: 0.9824 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4831/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0582 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4832/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4833/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4834/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4835/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0563 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4836/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4837/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0568 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4838/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0560 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4839/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0554 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4840/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4841/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0581 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4842/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0525 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4843/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4844/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4845/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0586 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4846/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9855 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4847/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4848/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4849/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4850/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4851/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4852/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0558 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4853/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0584 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4854/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0581 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4855/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4856/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4857/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4858/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4859/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9857 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4860/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0585 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4861/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4862/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4863/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4864/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4865/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4866/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4867/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0666 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4868/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4869/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0587 - accuracy: 0.9825 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4870/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4871/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4872/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4873/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0549 - accuracy: 0.9859 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4874/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4875/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4876/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4877/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4878/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4879/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4880/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0564 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4881/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0586 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4882/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4883/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4884/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4885/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4886/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4887/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4888/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0583 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4889/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4890/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4891/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0578 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4892/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0575 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4893/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4894/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4895/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4896/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4897/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0552 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4898/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0559 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4899/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4900/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4901/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4902/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4903/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4904/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0558 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4905/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4906/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4907/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0563 - accuracy: 0.9825 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4908/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0548 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4909/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4910/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4911/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0556 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4912/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4913/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4914/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0551 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4915/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0550 - accuracy: 0.9859 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4916/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0549 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4917/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4918/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0552 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4919/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0597 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4920/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4921/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4922/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0582 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4923/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0551 - accuracy: 0.9831 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4924/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4925/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4926/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0559 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4927/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0554 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4928/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4929/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4930/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4931/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4932/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0580 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4933/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0580 - accuracy: 0.9831 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4934/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4935/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4936/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0553 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4937/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0609 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4938/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4939/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0557 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4940/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9831 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4941/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0555 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4942/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4943/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0551 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4944/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0589 - accuracy: 0.9827 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4945/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0552 - accuracy: 0.9831 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4946/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0554 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4947/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0558 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4948/5000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0561 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4949/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9831 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4950/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4951/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0552 - accuracy: 0.9857 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4952/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4953/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4954/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0555 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4955/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4956/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0580 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4957/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0560 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4958/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0586 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4959/5000\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0557 - accuracy: 0.9827 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4960/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0579 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4961/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0558 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4962/5000\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0584 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4963/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0554 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4964/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0584 - accuracy: 0.9825 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4965/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0555 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4966/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0551 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4967/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0580 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4968/5000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0582 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4969/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0581 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4970/5000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0552 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4971/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0549 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4972/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4973/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0548 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4974/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0549 - accuracy: 0.9857 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4975/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0601 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4976/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0554 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4977/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0578 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4978/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4979/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0553 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4980/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9850 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4981/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4982/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0558 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4983/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0557 - accuracy: 0.9833 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4984/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0556 - accuracy: 0.9835 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4985/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4986/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9844 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4987/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0554 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4988/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0553 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4989/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0555 - accuracy: 0.9842 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4990/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9839 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4991/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0593 - accuracy: 0.9859 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4992/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0580 - accuracy: 0.9848 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4993/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0550 - accuracy: 0.9859 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4994/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0576 - accuracy: 0.9829 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4995/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9840 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4996/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0576 - accuracy: 0.9854 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4997/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0564 - accuracy: 0.9837 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4998/5000\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0586 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 4999/5000\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0552 - accuracy: 0.9846 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "Epoch 5000/5000\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0553 - accuracy: 0.9852 - f1_score: 0.9256 - val_loss: 2.7614 - val_accuracy: 0.7986 - val_f1_score: 0.9256\n",
      "CPU times: user 3h 13min 37s, sys: 20min 18s, total: 3h 33min 56s\n",
      "Wall time: 2h 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed()\n",
    "model0.compile(optimizer=optimizer0,loss='binary_crossentropy',metrics=['accuracy',keras.metrics.F1Score()])\n",
    "with tf.device('/gpu:0'):\n",
    "    history0=model0.fit(X_train,y_train,epochs=5000,batch_size=64,validation_data=(X_val,y_val),callbacks=[my_callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17c170",
   "metadata": {
    "papermill": {
     "duration": 10.440627,
     "end_time": "2024-06-02T12:37:16.149275",
     "exception": false,
     "start_time": "2024-06-02T12:37:05.708648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### See the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3cdab41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:37:36.998280Z",
     "iopub.status.busy": "2024-06-02T12:37:36.997409Z",
     "iopub.status.idle": "2024-06-02T12:37:37.017058Z",
     "shell.execute_reply": "2024-06-02T12:37:37.016094Z"
    },
    "papermill": {
     "duration": 10.392527,
     "end_time": "2024-06-02T12:37:37.019300",
     "exception": false,
     "start_time": "2024-06-02T12:37:26.626773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.444224</td>\n",
       "      <td>0.469782</td>\n",
       "      <td>0.362037</td>\n",
       "      <td>0.661574</td>\n",
       "      <td>0.741681</td>\n",
       "      <td>0.492279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.675686</td>\n",
       "      <td>0.735736</td>\n",
       "      <td>0.590438</td>\n",
       "      <td>0.535720</td>\n",
       "      <td>0.781086</td>\n",
       "      <td>0.636991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.612586</td>\n",
       "      <td>0.771209</td>\n",
       "      <td>0.663347</td>\n",
       "      <td>0.900483</td>\n",
       "      <td>0.801226</td>\n",
       "      <td>0.684547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.565443</td>\n",
       "      <td>0.771396</td>\n",
       "      <td>0.696221</td>\n",
       "      <td>0.510361</td>\n",
       "      <td>0.786340</td>\n",
       "      <td>0.706275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.531672</td>\n",
       "      <td>0.789977</td>\n",
       "      <td>0.713950</td>\n",
       "      <td>1.030704</td>\n",
       "      <td>0.812609</td>\n",
       "      <td>0.722161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.057641</td>\n",
       "      <td>0.985360</td>\n",
       "      <td>0.925596</td>\n",
       "      <td>2.761369</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.925597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.056439</td>\n",
       "      <td>0.983671</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>2.761369</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.925597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.058554</td>\n",
       "      <td>0.984610</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>2.761369</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.925597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.984610</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>2.761369</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.925597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.055350</td>\n",
       "      <td>0.985173</td>\n",
       "      <td>0.925597</td>\n",
       "      <td>2.761369</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.925598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  f1_score  val_loss  val_accuracy  val_f1_score\n",
       "0     4.444224  0.469782  0.362037  0.661574      0.741681      0.492279\n",
       "1     0.675686  0.735736  0.590438  0.535720      0.781086      0.636991\n",
       "2     0.612586  0.771209  0.663347  0.900483      0.801226      0.684547\n",
       "3     0.565443  0.771396  0.696221  0.510361      0.786340      0.706275\n",
       "4     0.531672  0.789977  0.713950  1.030704      0.812609      0.722161\n",
       "...        ...       ...       ...       ...           ...           ...\n",
       "4995  0.057641  0.985360  0.925596  2.761369      0.798599      0.925597\n",
       "4996  0.056439  0.983671  0.925597  2.761369      0.798599      0.925597\n",
       "4997  0.058554  0.984610  0.925597  2.761369      0.798599      0.925597\n",
       "4998  0.055183  0.984610  0.925597  2.761369      0.798599      0.925597\n",
       "4999  0.055350  0.985173  0.925597  2.761369      0.798599      0.925598\n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0=pd.DataFrame(history0.history)\n",
    "\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ddf0021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:37:58.001672Z",
     "iopub.status.busy": "2024-06-02T12:37:58.001290Z",
     "iopub.status.idle": "2024-06-02T12:37:58.362368Z",
     "shell.execute_reply": "2024-06-02T12:37:58.361368Z"
    },
    "papermill": {
     "duration": 10.846079,
     "end_time": "2024-06-02T12:37:58.365190",
     "exception": false,
     "start_time": "2024-06-02T12:37:47.519111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Model Comparison')\n",
    "plt.plot(history0.history['loss'],label='loss')\n",
    "plt.plot(history0.history['accuracy'],label='accuracy')\n",
    "plt.plot(history0.history['f1_score'],label='f1_score')\n",
    "plt.plot(history0.history['val_loss'],label='val_loss')\n",
    "plt.plot(history0.history['val_accuracy'],label='val_accuracy')\n",
    "plt.plot(history0.history['val_f1_score'],label='val_f1_score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True,linestyle='--',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67d8d2",
   "metadata": {
    "papermill": {
     "duration": 10.413598,
     "end_time": "2024-06-02T12:38:19.294398",
     "exception": false,
     "start_time": "2024-06-02T12:38:08.880800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### The loss, accuracy, f1_score, val_loss, val_accuracy and val_f1_score are stable at around 0.07, 0.98, 0.93, 2.64, 0.8 and 0.93 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96d3c7",
   "metadata": {
    "papermill": {
     "duration": 10.381262,
     "end_time": "2024-06-02T12:38:40.077348",
     "exception": false,
     "start_time": "2024-06-02T12:38:29.696086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see that all metrics are stable after 2000 epochs, so we just simply use the last trained model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "879e0f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:39:00.949695Z",
     "iopub.status.busy": "2024-06-02T12:39:00.949326Z",
     "iopub.status.idle": "2024-06-02T12:39:05.802668Z",
     "shell.execute_reply": "2024-06-02T12:39:05.801640Z"
    },
    "papermill": {
     "duration": 15.29541,
     "end_time": "2024-06-02T12:39:05.805181",
     "exception": false,
     "start_time": "2024-06-02T12:38:50.509771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 5s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model0.predict(test_data,batch_size=64)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8619aea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:39:26.471318Z",
     "iopub.status.busy": "2024-06-02T12:39:26.470957Z",
     "iopub.status.idle": "2024-06-02T12:39:26.497452Z",
     "shell.execute_reply": "2024-06-02T12:39:26.496576Z"
    },
    "papermill": {
     "duration": 10.347018,
     "end_time": "2024-06-02T12:39:26.499529",
     "exception": false,
     "start_time": "2024-06-02T12:39:16.152511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv',index_col=None)\n",
    "df1['target']=y_pred\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a44cdc31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:39:47.189611Z",
     "iopub.status.busy": "2024-06-02T12:39:47.188723Z",
     "iopub.status.idle": "2024-06-02T12:39:47.387653Z",
     "shell.execute_reply": "2024-06-02T12:39:47.386625Z"
    },
    "papermill": {
     "duration": 10.568983,
     "end_time": "2024-06-02T12:39:47.390233",
     "exception": false,
     "start_time": "2024-06-02T12:39:36.821250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean the working file\n",
    "try:\n",
    "    shutil.rmtree('/kaggle/working/')\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac1175e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T12:40:08.066117Z",
     "iopub.status.busy": "2024-06-02T12:40:08.065276Z",
     "iopub.status.idle": "2024-06-02T12:40:08.140297Z",
     "shell.execute_reply": "2024-06-02T12:40:08.139399Z"
    },
    "papermill": {
     "duration": 10.48645,
     "end_time": "2024-06-02T12:40:08.142326",
     "exception": false,
     "start_time": "2024-06-02T12:39:57.655876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save the outputs\n",
    "df0.to_csv('/kaggle/working/df_history.csv')\n",
    "df1.to_csv('/kaggle/working/df_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6b693",
   "metadata": {
    "papermill": {
     "duration": 10.312975,
     "end_time": "2024-06-02T12:40:28.805418",
     "exception": false,
     "start_time": "2024-06-02T12:40:18.492443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612674e",
   "metadata": {
    "papermill": {
     "duration": 10.674433,
     "end_time": "2024-06-02T12:40:49.901713",
     "exception": false,
     "start_time": "2024-06-02T12:40:39.227280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### We can see that BiLSTM is an efficient model to do such twitter classification mission. The validation accuracy can be over 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9d881",
   "metadata": {
    "papermill": {
     "duration": 10.396092,
     "end_time": "2024-06-02T12:41:10.826527",
     "exception": false,
     "start_time": "2024-06-02T12:41:00.430435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Moreover, we have some methods to improve the performances. We can make the BiLSTM model more complicated, like add more complex layers, add more nodes or try different activation function. We can use some other models such as Bert model and enhance the epoch times, like train the model for over hundreds of thousand epochs. However, it needs much more time to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f4575",
   "metadata": {
    "papermill": {
     "duration": 10.374572,
     "end_time": "2024-06-02T12:41:31.735133",
     "exception": false,
     "start_time": "2024-06-02T12:41:21.360561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Ref:\n",
    "#### https://github.com/keras-team/keras/issues/19282\n",
    "#### https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "#### https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#### https://keras.io/api/callbacks/\n",
    "#### https://keras.io/api/optimizers/\n",
    "#### https://keras.io/2.15/api/optimizers/learning_rate_schedules/exponential_decay/\n",
    "#### https://keras.io/examples/keras_recipes/reproducibility_recipes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cdeff",
   "metadata": {
    "papermill": {
     "duration": 10.40947,
     "end_time": "2024-06-02T12:41:52.502712",
     "exception": false,
     "start_time": "2024-06-02T12:41:42.093242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8412.867782,
   "end_time": "2024-06-02T12:42:05.889126",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-02T10:21:53.021344",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
